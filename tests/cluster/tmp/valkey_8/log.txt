15717:C 06 May 2024 19:13:44.984 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
15717:C 06 May 2024 19:13:44.985 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15717:C 06 May 2024 19:13:44.985 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15717:C 06 May 2024 19:13:44.985 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15717, just started
15717:C 06 May 2024 19:13:44.985 * Configuration loaded
15717:M 06 May 2024 19:13:44.985 * monotonic clock: POSIX clock_gettime
15717:M 06 May 2024 19:13:44.986 * Running mode=cluster, port=30008.
15717:M 06 May 2024 19:13:44.987 * No cluster configuration found, I'm 540669d90126bb96e70cd3d229f1c2a2734ad900
15717:M 06 May 2024 19:13:45.007 * Server initialized
15717:M 06 May 2024 19:13:45.008 * Creating AOF base file appendonly.aof.1.base.rdb on server start
15717:M 06 May 2024 19:13:45.009 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
15717:M 06 May 2024 19:13:45.009 * Ready to accept connections tcp
15717:M 06 May 2024 19:13:45.605 * configEpoch set to 0 via CLUSTER RESET HARD
15717:M 06 May 2024 19:13:45.605 * Node hard reset, now I'm 40ca5cb690f7583f3e82756c02c6a6e845ac6672
15717:M 06 May 2024 19:13:45.605 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
15717:M 06 May 2024 19:13:45.610 * CONFIG REWRITE executed with success.
15717:M 06 May 2024 19:13:45.773 * IP address for this node updated to 127.0.0.1
15717:M 06 May 2024 19:13:48.679 # Missing implement of connection type tls
15717:M 06 May 2024 19:13:52.563 * Cluster state changed: ok
15717:M 06 May 2024 19:13:52.833 * configEpoch set to 0 via CLUSTER RESET HARD
15717:M 06 May 2024 19:13:52.833 * Node hard reset, now I'm 14870674fd1761a0b545ac9b6860bd07cc3c82a1
15717:M 06 May 2024 19:13:52.833 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
15717:M 06 May 2024 19:13:52.833 # Cluster state changed: fail
15717:M 06 May 2024 19:13:52.837 * CONFIG REWRITE executed with success.
15717:S 06 May 2024 19:13:56.926 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15717:S 06 May 2024 19:13:56.926 * Connecting to MASTER 127.0.0.1:30003
15717:S 06 May 2024 19:13:56.926 * MASTER <-> REPLICA sync started
15717:S 06 May 2024 19:13:56.926 * Non blocking connect for SYNC fired the event.
15717:S 06 May 2024 19:13:56.927 * Master replied to PING, replication can continue...
15717:S 06 May 2024 19:13:56.927 * Trying a partial resynchronization (request 1c71924ea551d59df788e7c07d60acf1e0366595:3).
15717:S 06 May 2024 19:13:56.927 * Full resync from master: 2b8e94ba20102a7fd57992108d7e7489756e955f:19
15717:S 06 May 2024 19:13:56.929 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15717:S 06 May 2024 19:13:56.929 * Discarding previously cached master state.
15717:S 06 May 2024 19:13:56.929 * MASTER <-> REPLICA sync: Flushing old data
15717:S 06 May 2024 19:13:56.929 * MASTER <-> REPLICA sync: Loading DB in memory
15717:S 06 May 2024 19:13:56.931 * Loading RDB produced by valkey version 255.255.255
15717:S 06 May 2024 19:13:56.931 * RDB age 0 seconds
15717:S 06 May 2024 19:13:56.931 * RDB memory usage when created 2.66 Mb
15717:S 06 May 2024 19:13:56.932 * Done loading RDB, keys loaded: 0, keys expired: 0.
15717:S 06 May 2024 19:13:56.932 * MASTER <-> REPLICA sync: Finished with success
15717:S 06 May 2024 19:13:56.933 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15717:S 06 May 2024 19:13:56.934 * Background append only file rewriting started by pid 15862
15862:C 06 May 2024 19:13:56.935 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15862.aof
15862:C 06 May 2024 19:13:56.935 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15717:S 06 May 2024 19:13:56.997 * Background AOF rewrite terminated with success
15717:S 06 May 2024 19:13:56.997 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15862.aof into appendonly.aof.2.base.rdb
15717:S 06 May 2024 19:13:56.997 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.2.incr.aof
15717:S 06 May 2024 19:13:57.000 * Removing the history file appendonly.aof.1.incr.aof in the background
15717:S 06 May 2024 19:13:57.000 * Removing the history file appendonly.aof.1.base.rdb in the background
15717:S 06 May 2024 19:13:57.001 * Background AOF rewrite finished successfully
15717:S 06 May 2024 19:13:57.504 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is no longer master of shard 93a2a8fbb3ebee1719ea78e416af4f62a280e72f; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:13:57.504 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is now part of shard 0a3baa510e8cc3ec6c1dc6c6aff3c8c121a171ee
15717:S 06 May 2024 19:13:57.506 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is no longer master of shard 8d23c540c28858cc5ee10f86f93cd31765bd7b8b; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:13:57.506 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is now part of shard 5f75c91ff19b41d8200527f5fc06215514a6c230
15717:S 06 May 2024 19:13:57.508 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is no longer master of shard 67426f1e38c225d7050cb5d4244d7014ede80e1a; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:13:57.508 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is now part of shard a2427455940b828a38a3ffb09c2b4cdfd4ccf6f5
15717:S 06 May 2024 19:13:57.509 * Cluster state changed: ok
15717:S 06 May 2024 19:13:57.570 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is no longer master of shard 45a425a508f35dfb3598266dbd995af0daf15e78; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:13:57.570 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is now part of shard e89af52affaeadfe3f4c492254ae30531e68445a
15717:S 06 May 2024 19:14:04.019 * FAIL message received from 9fcb85d514e112d10016edd497098e3fbe9e110e () about 1e5c8c4a07f43bf52fd4450600942b910335d228 ()
15717:S 06 May 2024 19:14:04.019 * FAIL message received from 85ed6fd7ba097e6b2c80626b17e130cb93740a3c () about 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 ()
15717:S 06 May 2024 19:14:04.553 * FAIL message received from 7ce15aada6af76e304df222f463282e865e22db0 () about 125263e49707331bd8e7a9d6681fe581aca463f2 ()
15717:S 06 May 2024 19:14:04.553 # Cluster state changed: fail
15717:S 06 May 2024 19:14:10.707 * Clear FAIL state for node 125263e49707331bd8e7a9d6681fe581aca463f2 (): is reachable again and nobody is serving its slots after some time.
15717:S 06 May 2024 19:14:10.707 * Cluster state changed: ok
15717:S 06 May 2024 19:14:12.654 * Clear FAIL state for node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 ():replica is reachable again.
15717:S 06 May 2024 19:14:12.764 * Clear FAIL state for node 1e5c8c4a07f43bf52fd4450600942b910335d228 ():replica is reachable again.
15717:M 06 May 2024 19:14:18.500 * Connection with master lost.
15717:M 06 May 2024 19:14:18.500 * Caching the disconnected master state.
15717:M 06 May 2024 19:14:18.500 * Discarding previously cached master state.
15717:M 06 May 2024 19:14:18.500 * Setting secondary replication ID to 2b8e94ba20102a7fd57992108d7e7489756e955f, valid up to offset: 89. New replication ID is caa58a67c3684aec8d5d94a4eacc3ec53205e09f
15717:M 06 May 2024 19:14:18.502 * configEpoch set to 0 via CLUSTER RESET HARD
15717:M 06 May 2024 19:14:18.502 * Node hard reset, now I'm ed209190989ecd1b2fc83edcb92a31a33e7f40c0
15717:M 06 May 2024 19:14:18.502 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
15717:M 06 May 2024 19:14:18.502 # Cluster state changed: fail
15717:M 06 May 2024 19:14:18.507 * CONFIG REWRITE executed with success.
15717:S 06 May 2024 19:14:21.908 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15717:S 06 May 2024 19:14:21.908 * Connecting to MASTER 127.0.0.1:30003
15717:S 06 May 2024 19:14:21.909 * MASTER <-> REPLICA sync started
15717:S 06 May 2024 19:14:21.909 * Non blocking connect for SYNC fired the event.
15717:S 06 May 2024 19:14:21.909 * Master replied to PING, replication can continue...
15717:S 06 May 2024 19:14:21.909 * Trying a partial resynchronization (request caa58a67c3684aec8d5d94a4eacc3ec53205e09f:89).
15717:S 06 May 2024 19:14:21.910 * Full resync from master: 2b8e94ba20102a7fd57992108d7e7489756e955f:88
15717:S 06 May 2024 19:14:21.911 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15717:S 06 May 2024 19:14:21.911 * Discarding previously cached master state.
15717:S 06 May 2024 19:14:21.911 * MASTER <-> REPLICA sync: Flushing old data
15717:S 06 May 2024 19:14:21.912 * MASTER <-> REPLICA sync: Loading DB in memory
15717:S 06 May 2024 19:14:21.913 * Loading RDB produced by valkey version 255.255.255
15717:S 06 May 2024 19:14:21.913 * RDB age 0 seconds
15717:S 06 May 2024 19:14:21.913 * RDB memory usage when created 2.70 Mb
15717:S 06 May 2024 19:14:21.913 * Done loading RDB, keys loaded: 0, keys expired: 0.
15717:S 06 May 2024 19:14:21.913 * MASTER <-> REPLICA sync: Finished with success
15717:S 06 May 2024 19:14:21.913 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15717:S 06 May 2024 19:14:21.914 * Background append only file rewriting started by pid 16006
16006:C 06 May 2024 19:14:21.915 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16006.aof
16006:C 06 May 2024 19:14:21.916 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15717:S 06 May 2024 19:14:22.004 * Background AOF rewrite terminated with success
15717:S 06 May 2024 19:14:22.004 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16006.aof into appendonly.aof.3.base.rdb
15717:S 06 May 2024 19:14:22.004 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.3.incr.aof
15717:S 06 May 2024 19:14:22.007 * Removing the history file appendonly.aof.2.incr.aof in the background
15717:S 06 May 2024 19:14:22.007 * Removing the history file appendonly.aof.2.base.rdb in the background
15717:S 06 May 2024 19:14:22.008 * Background AOF rewrite finished successfully
15717:S 06 May 2024 19:14:22.045 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is no longer master of shard 10c2f585fc5fd868287fafbf5212b6227ab7017d; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:22.045 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is now part of shard bccca0c1e0e1dccd3b58d5709156154f9f4c9957
15717:S 06 May 2024 19:14:22.059 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is no longer master of shard f07b8fb6b6cbfb692ab1b87f30279a0b975efbbe; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:22.059 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is now part of shard a3bfec7d6b49cc6990a2b3560914e0bb1a7bda0c
15717:S 06 May 2024 19:14:22.256 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is no longer master of shard afe0861af93a47ac71d717dd7dd1f03d3dbe6564; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:22.256 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is now part of shard 6bb248ce1980dda8969ac6f65c18efe9f1d758d2
15717:S 06 May 2024 19:14:23.523 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is no longer master of shard f4049c4ab89f7706b4043fb6168626bdb0c6d5fc; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:23.523 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is now part of shard 1109f6e41ce862f21e8f082deeae031962ef83ac
15717:S 06 May 2024 19:14:23.523 * Cluster state changed: ok
15717:S 06 May 2024 19:14:30.031 * FAIL message received from 220970bd9ddf17fd39dbd121039031b6350dc7bd () about 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ()
15717:S 06 May 2024 19:14:30.031 # Cluster state changed: fail
15717:S 06 May 2024 19:14:31.063 * Cluster state changed: ok
15717:M 06 May 2024 19:14:31.340 * Connection with master lost.
15717:M 06 May 2024 19:14:31.340 * Caching the disconnected master state.
15717:M 06 May 2024 19:14:31.340 * Discarding previously cached master state.
15717:M 06 May 2024 19:14:31.340 * Setting secondary replication ID to 2b8e94ba20102a7fd57992108d7e7489756e955f, valid up to offset: 2010. New replication ID is 04ce7f64cb3cc4867ae2b0088eeaaaaddf31fcd5
15717:M 06 May 2024 19:14:31.342 * configEpoch set to 0 via CLUSTER RESET HARD
15717:M 06 May 2024 19:14:31.343 * Node hard reset, now I'm 9dad64de16025f02b271510ee2d51a6ca079dc7b
15717:M 06 May 2024 19:14:31.343 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
15717:M 06 May 2024 19:14:31.343 # Cluster state changed: fail
15717:M 06 May 2024 19:14:31.352 * CONFIG REWRITE executed with success.
15717:S 06 May 2024 19:14:34.845 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15717:S 06 May 2024 19:14:34.845 * Connecting to MASTER 127.0.0.1:30003
15717:S 06 May 2024 19:14:34.845 * MASTER <-> REPLICA sync started
15717:S 06 May 2024 19:14:34.846 * Non blocking connect for SYNC fired the event.
15717:S 06 May 2024 19:14:34.847 * Master replied to PING, replication can continue...
15717:S 06 May 2024 19:14:34.847 * Trying a partial resynchronization (request 04ce7f64cb3cc4867ae2b0088eeaaaaddf31fcd5:2010).
15717:S 06 May 2024 19:14:34.848 * Full resync from master: 2b8e94ba20102a7fd57992108d7e7489756e955f:2009
15717:S 06 May 2024 19:14:34.849 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15717:S 06 May 2024 19:14:34.849 * Discarding previously cached master state.
15717:S 06 May 2024 19:14:34.849 * MASTER <-> REPLICA sync: Flushing old data
15717:S 06 May 2024 19:14:34.850 * MASTER <-> REPLICA sync: Loading DB in memory
15717:S 06 May 2024 19:14:34.852 * Loading RDB produced by valkey version 255.255.255
15717:S 06 May 2024 19:14:34.852 * RDB age 0 seconds
15717:S 06 May 2024 19:14:34.852 * RDB memory usage when created 2.70 Mb
15717:S 06 May 2024 19:14:34.852 * Done loading RDB, keys loaded: 0, keys expired: 0.
15717:S 06 May 2024 19:14:34.852 * MASTER <-> REPLICA sync: Finished with success
15717:S 06 May 2024 19:14:34.852 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15717:S 06 May 2024 19:14:34.853 * Background append only file rewriting started by pid 16115
16115:C 06 May 2024 19:14:34.854 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16115.aof
16115:C 06 May 2024 19:14:34.854 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15717:S 06 May 2024 19:14:34.906 * Background AOF rewrite terminated with success
15717:S 06 May 2024 19:14:34.906 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16115.aof into appendonly.aof.4.base.rdb
15717:S 06 May 2024 19:14:34.906 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.4.incr.aof
15717:S 06 May 2024 19:14:34.908 * Removing the history file appendonly.aof.3.incr.aof in the background
15717:S 06 May 2024 19:14:34.908 * Removing the history file appendonly.aof.3.base.rdb in the background
15717:S 06 May 2024 19:14:34.909 * Background AOF rewrite finished successfully
15717:S 06 May 2024 19:14:35.529 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is no longer master of shard a577fdbb713c832cf9850e989f6b72796108c745; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:35.529 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is now part of shard e2220f97589610879561e699a57b8481b7124689
15717:S 06 May 2024 19:14:35.555 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is no longer master of shard 98ec8d4aaf47b8dc3863b2d7c51af4e948cd12a6; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:35.555 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is now part of shard 04e24d93f8b27f67291965144dd39f0ab3b35d25
15717:S 06 May 2024 19:14:35.562 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is no longer master of shard 76f000c5383c5d2746bf6c7b9d86a92aed4820c2; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:35.562 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is now part of shard 45bde1a3488556534c5594a0f6ace0dba22be32b
15717:S 06 May 2024 19:14:35.567 * Node de3054474d708953456bc221c893ce4305ba5136 () is no longer master of shard 77b4d2bf3a14a66eb7635d415bbf20a86e150b57; removed all 0 slot(s) it used to own
15717:S 06 May 2024 19:14:35.567 * Node de3054474d708953456bc221c893ce4305ba5136 () is now part of shard 236cc077a81351c19b896662f24236e227403702
15717:S 06 May 2024 19:14:35.583 * Cluster state changed: ok
15717:S 06 May 2024 19:14:42.797 * FAIL message received from ffdb16089f859482065bad0e5c9a094d823a1410 () about c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ()
15717:S 06 May 2024 19:14:42.797 # Cluster state changed: fail
15717:S 06 May 2024 19:14:43.895 * Cluster state changed: ok
15717:S 06 May 2024 19:14:44.076 * Clear FAIL state for node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ():master without slots is reachable again.
15717:S 06 May 2024 19:14:44.076 * A failover occurred in shard 45bde1a3488556534c5594a0f6ace0dba22be32b; node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff () lost 0 slot(s) to node 671088db4acca8792f424ee7ea50b8153c7529a0 () with a config epoch of 21
15717:signal-handler (1715022884) Received SIGTERM scheduling shutdown...
15717:S 06 May 2024 19:14:45.082 * User requested shutdown...
15717:S 06 May 2024 19:14:45.083 # Valkey is now ready to exit, bye bye...
16220:C 06 May 2024 19:14:45.144 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16220:C 06 May 2024 19:14:45.144 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16220:C 06 May 2024 19:14:45.144 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16220, just started
16220:C 06 May 2024 19:14:45.144 * Configuration loaded
16220:M 06 May 2024 19:14:45.145 * monotonic clock: POSIX clock_gettime
16220:M 06 May 2024 19:14:45.146 * Running mode=cluster, port=30008.
16220:M 06 May 2024 19:14:45.152 * Node configuration loaded, I'm 9dad64de16025f02b271510ee2d51a6ca079dc7b
16220:M 06 May 2024 19:14:45.152 * Server initialized
16220:M 06 May 2024 19:14:45.152 * Reading RDB base file on AOF loading...
16220:M 06 May 2024 19:14:45.152 * Loading RDB produced by valkey version 255.255.255
16220:M 06 May 2024 19:14:45.153 * RDB age 11 seconds
16220:M 06 May 2024 19:14:45.153 * RDB memory usage when created 2.62 Mb
16220:M 06 May 2024 19:14:45.153 * RDB is base AOF
16220:M 06 May 2024 19:14:45.153 * Done loading RDB, keys loaded: 0, keys expired: 0.
16220:M 06 May 2024 19:14:45.153 * DB loaded from base file appendonly.aof.4.base.rdb: 0.001 seconds
16220:M 06 May 2024 19:14:45.154 * DB loaded from incr file appendonly.aof.4.incr.aof: 0.001 seconds
16220:M 06 May 2024 19:14:45.154 * DB loaded from append only file: 0.002 seconds
16220:M 06 May 2024 19:14:45.154 * Opening AOF incr file appendonly.aof.4.incr.aof on server start
16220:M 06 May 2024 19:14:45.154 * Ready to accept connections tcp
16220:S 06 May 2024 19:14:45.156 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16220:S 06 May 2024 19:14:45.156 * Connecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:45.156 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:45.156 * Cluster state changed: ok
16220:S 06 May 2024 19:14:45.157 * Non blocking connect for SYNC fired the event.
16220:S 06 May 2024 19:14:45.160 * Master replied to PING, replication can continue...
16220:S 06 May 2024 19:14:45.160 * Trying a partial resynchronization (request b70cedd95f2664937f49c34a5666d35fc84852d3:1).
16220:S 06 May 2024 19:14:45.161 * Full resync from master: 2b8e94ba20102a7fd57992108d7e7489756e955f:17073
16220:S 06 May 2024 19:14:45.164 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16220:S 06 May 2024 19:14:45.164 * Discarding previously cached master state.
16220:S 06 May 2024 19:14:45.164 * MASTER <-> REPLICA sync: Flushing old data
16220:S 06 May 2024 19:14:45.164 * MASTER <-> REPLICA sync: Loading DB in memory
16220:S 06 May 2024 19:14:45.166 * Loading RDB produced by valkey version 255.255.255
16220:S 06 May 2024 19:14:45.166 * RDB age 0 seconds
16220:S 06 May 2024 19:14:45.166 * RDB memory usage when created 2.76 Mb
16220:S 06 May 2024 19:14:45.166 * Done loading RDB, keys loaded: 201, keys expired: 0.
16220:S 06 May 2024 19:14:45.166 * MASTER <-> REPLICA sync: Finished with success
16220:S 06 May 2024 19:14:45.166 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16220:S 06 May 2024 19:14:45.167 * Background append only file rewriting started by pid 16227
16227:C 06 May 2024 19:14:45.169 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16227.aof
16227:C 06 May 2024 19:14:45.170 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16220:S 06 May 2024 19:14:45.257 * Background AOF rewrite terminated with success
16220:S 06 May 2024 19:14:45.257 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16227.aof into appendonly.aof.5.base.rdb
16220:S 06 May 2024 19:14:45.257 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.5.incr.aof
16220:S 06 May 2024 19:14:45.259 * Removing the history file appendonly.aof.4.incr.aof in the background
16220:S 06 May 2024 19:14:45.259 * Removing the history file appendonly.aof.4.base.rdb in the background
16220:S 06 May 2024 19:14:45.261 * Background AOF rewrite finished successfully
16220:S 06 May 2024 19:14:45.338 # Missing implement of connection type tls
16220:S 06 May 2024 19:14:46.149 * Connection with master lost.
16220:S 06 May 2024 19:14:46.149 * Caching the disconnected master state.
16220:S 06 May 2024 19:14:46.149 * Reconnecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:46.149 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:46.149 # Error condition on socket for SYNC: Connection refused
16220:S 06 May 2024 19:14:47.080 * Connecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:47.080 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:47.080 # Error condition on socket for SYNC: Connection refused
16220:S 06 May 2024 19:14:48.091 * Connecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:48.092 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:48.092 # Error condition on socket for SYNC: Connection refused
16220:S 06 May 2024 19:14:49.101 * Connecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:49.101 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:49.101 # Error condition on socket for SYNC: Connection refused
16220:S 06 May 2024 19:14:49.724 * Marking node d979b3705974c02442b170e8dacc0ba5e04d56bb () as failing (quorum reached).
16220:S 06 May 2024 19:14:49.724 # Cluster state changed: fail
16220:S 06 May 2024 19:14:49.808 * Start of election delayed for 913 milliseconds (rank #0, offset 18382).
16220:S 06 May 2024 19:14:50.111 * Connecting to MASTER 127.0.0.1:30003
16220:S 06 May 2024 19:14:50.112 * MASTER <-> REPLICA sync started
16220:S 06 May 2024 19:14:50.112 # Error condition on socket for SYNC: Connection refused
16220:S 06 May 2024 19:14:50.819 * Starting a failover election for epoch 22.
16220:S 06 May 2024 19:14:50.829 * Failover election won: I'm the new master.
16220:S 06 May 2024 19:14:50.829 * configEpoch set to 22 after successful failover
16220:M 06 May 2024 19:14:50.829 * Discarding previously cached master state.
16220:M 06 May 2024 19:14:50.829 * Setting secondary replication ID to 2b8e94ba20102a7fd57992108d7e7489756e955f, valid up to offset: 18383. New replication ID is d8cd85d398c673ed281852cbb89615f0ae002b94
16220:M 06 May 2024 19:14:50.829 * Cluster state changed: ok
16220:M 06 May 2024 19:14:51.024 * Replica 127.0.0.1:30003 asks for synchronization
16220:M 06 May 2024 19:14:51.024 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '765f5cf85d7dd344406853b582930de22e16ed9e', my replication IDs are 'd8cd85d398c673ed281852cbb89615f0ae002b94' and '2b8e94ba20102a7fd57992108d7e7489756e955f')
16220:M 06 May 2024 19:14:51.024 * Starting BGSAVE for SYNC with target: replicas sockets
16220:M 06 May 2024 19:14:51.024 * Background RDB transfer started by pid 16298
16220:M 06 May 2024 19:14:51.025 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16220:M 06 May 2024 19:14:51.025 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 22
16298:C 06 May 2024 19:14:51.029 * Fork CoW for RDB: current 1 MB, peak 1 MB, average 1 MB
16220:M 06 May 2024 19:14:51.029 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16220:M 06 May 2024 19:14:51.033 * Background RDB transfer terminated with success
16220:M 06 May 2024 19:14:51.033 * Streamed RDB transfer with replica 127.0.0.1:30003 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16220:M 06 May 2024 19:14:51.033 * Synchronization with replica 127.0.0.1:30003 succeeded
16220:M 06 May 2024 19:14:55.628 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16220:M 06 May 2024 19:14:55.628 # Cluster state changed: fail
16220:M 06 May 2024 19:14:56.724 * Failover auth granted to d612f4e24864679d4cb0b4d862c48e2df4373065 () for epoch 23
16220:M 06 May 2024 19:14:57.020 * Cluster state changed: ok
16220:M 06 May 2024 19:14:57.346 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16220:M 06 May 2024 19:14:57.346 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 23
16220:M 06 May 2024 19:15:04.029 * Marking node d612f4e24864679d4cb0b4d862c48e2df4373065 () as failing (quorum reached).
16220:M 06 May 2024 19:15:04.029 # Cluster state changed: fail
16220:M 06 May 2024 19:15:04.844 * Failover auth granted to c26ea12fd513a731ced8b60c63390b5dbe84235d () for epoch 24
16220:M 06 May 2024 19:15:04.891 * Cluster state changed: ok
16220:M 06 May 2024 19:15:05.048 * Clear FAIL state for node d612f4e24864679d4cb0b4d862c48e2df4373065 ():master without slots is reachable again.
16220:M 06 May 2024 19:15:05.048 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node d612f4e24864679d4cb0b4d862c48e2df4373065 () lost 0 slot(s) to node c26ea12fd513a731ced8b60c63390b5dbe84235d () with a config epoch of 24
16220:M 06 May 2024 19:15:10.100 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16220:M 06 May 2024 19:15:10.100 # Cluster state changed: fail
16220:M 06 May 2024 19:15:10.914 * Failover auth granted to d612f4e24864679d4cb0b4d862c48e2df4373065 () for epoch 25
16220:M 06 May 2024 19:15:10.959 * Cluster state changed: ok
16220:M 06 May 2024 19:15:11.116 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16220:M 06 May 2024 19:15:11.117 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 25
16220:M 06 May 2024 19:15:11.205 * Connection with replica 127.0.0.1:30003 lost.
16220:M 06 May 2024 19:15:11.285 * Replica 127.0.0.1:30003 asks for synchronization
16220:M 06 May 2024 19:15:11.285 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'c507c01debb81e1df6f471c7ff30f1a904f453a2', my replication IDs are 'd8cd85d398c673ed281852cbb89615f0ae002b94' and '2b8e94ba20102a7fd57992108d7e7489756e955f')
16220:M 06 May 2024 19:15:11.285 * Starting BGSAVE for SYNC with target: replicas sockets
16220:M 06 May 2024 19:15:11.286 * Background RDB transfer started by pid 16521
16521:C 06 May 2024 19:15:11.290 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16220:M 06 May 2024 19:15:11.290 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16220:M 06 May 2024 19:15:11.294 * Background RDB transfer terminated with success
16220:M 06 May 2024 19:15:11.294 * Streamed RDB transfer with replica 127.0.0.1:30003 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16220:M 06 May 2024 19:15:11.294 * Synchronization with replica 127.0.0.1:30003 succeeded
16220:signal-handler (1715022911) Received SIGTERM scheduling shutdown...
16220:M 06 May 2024 19:15:11.609 * User requested shutdown...
16220:M 06 May 2024 19:15:11.609 * Waiting for replicas before shutting down.
16220:M 06 May 2024 19:15:12.212 * 1 of 1 replicas are in sync when shutting down.
16220:M 06 May 2024 19:15:12.212 # Valkey is now ready to exit, bye bye...
16582:C 06 May 2024 19:15:16.737 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16582:C 06 May 2024 19:15:16.738 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16582:C 06 May 2024 19:15:16.738 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16582, just started
16582:C 06 May 2024 19:15:16.738 * Configuration loaded
16582:M 06 May 2024 19:15:16.738 * monotonic clock: POSIX clock_gettime
16582:M 06 May 2024 19:15:16.739 * Running mode=cluster, port=30008.
16582:M 06 May 2024 19:15:16.746 * Node configuration loaded, I'm 9dad64de16025f02b271510ee2d51a6ca079dc7b
16582:M 06 May 2024 19:15:16.766 * Server initialized
16582:M 06 May 2024 19:15:16.777 * Reading RDB base file on AOF loading...
16582:M 06 May 2024 19:15:16.777 * Loading RDB produced by valkey version 255.255.255
16582:M 06 May 2024 19:15:16.777 * RDB age 31 seconds
16582:M 06 May 2024 19:15:16.778 * RDB memory usage when created 2.30 Mb
16582:M 06 May 2024 19:15:16.778 * RDB is base AOF
16582:M 06 May 2024 19:15:16.779 * Done loading RDB, keys loaded: 201, keys expired: 0.
16582:M 06 May 2024 19:15:16.779 * DB loaded from base file appendonly.aof.5.base.rdb: 0.013 seconds
16582:M 06 May 2024 19:15:16.780 * DB loaded from incr file appendonly.aof.5.incr.aof: 0.001 seconds
16582:M 06 May 2024 19:15:16.780 * DB loaded from append only file: 0.014 seconds
16582:M 06 May 2024 19:15:16.780 * Opening AOF incr file appendonly.aof.5.incr.aof on server start
16582:M 06 May 2024 19:15:16.780 * Ready to accept connections tcp
16582:M 06 May 2024 19:15:16.793 * Configuration change detected. Reconfiguring myself as a replica of d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16582:S 06 May 2024 19:15:16.793 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16582:S 06 May 2024 19:15:16.793 * Connecting to MASTER 127.0.0.1:30003
16582:S 06 May 2024 19:15:16.794 * MASTER <-> REPLICA sync started
16582:S 06 May 2024 19:15:16.794 * Cluster state changed: ok
16582:S 06 May 2024 19:15:16.797 * Non blocking connect for SYNC fired the event.
16582:S 06 May 2024 19:15:16.797 * Master replied to PING, replication can continue...
16582:S 06 May 2024 19:15:16.797 * Trying a partial resynchronization (request a773bd20079251bf326fca0f53c3bcf3db26d95b:1).
16582:S 06 May 2024 19:15:16.797 * Full resync from master: f750808358d7a1060858c27fdc5d21f1723ef38d:34238
16582:S 06 May 2024 19:15:16.801 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16582:S 06 May 2024 19:15:16.802 * Discarding previously cached master state.
16582:S 06 May 2024 19:15:16.802 * MASTER <-> REPLICA sync: Flushing old data
16582:S 06 May 2024 19:15:16.802 * MASTER <-> REPLICA sync: Loading DB in memory
16582:S 06 May 2024 19:15:16.805 * Loading RDB produced by valkey version 255.255.255
16582:S 06 May 2024 19:15:16.805 * RDB age 0 seconds
16582:S 06 May 2024 19:15:16.805 * RDB memory usage when created 2.58 Mb
16582:S 06 May 2024 19:15:16.806 * Done loading RDB, keys loaded: 427, keys expired: 0.
16582:S 06 May 2024 19:15:16.806 * MASTER <-> REPLICA sync: Finished with success
16582:S 06 May 2024 19:15:16.806 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16582:S 06 May 2024 19:15:16.807 * Background append only file rewriting started by pid 16589
16589:C 06 May 2024 19:15:16.813 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16589.aof
16589:C 06 May 2024 19:15:16.813 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16582:S 06 May 2024 19:15:16.883 * Background AOF rewrite terminated with success
16582:S 06 May 2024 19:15:16.883 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16589.aof into appendonly.aof.6.base.rdb
16582:S 06 May 2024 19:15:16.883 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.6.incr.aof
16582:S 06 May 2024 19:15:16.886 * Removing the history file appendonly.aof.5.incr.aof in the background
16582:S 06 May 2024 19:15:16.886 * Removing the history file appendonly.aof.5.base.rdb in the background
16582:S 06 May 2024 19:15:16.887 * Background AOF rewrite finished successfully
16582:signal-handler (1715022917) Received SIGTERM scheduling shutdown...
16582:S 06 May 2024 19:15:17.190 * User requested shutdown...
16582:S 06 May 2024 19:15:17.190 # Valkey is now ready to exit, bye bye...
16611:C 06 May 2024 19:15:17.252 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16611:C 06 May 2024 19:15:17.252 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16611:C 06 May 2024 19:15:17.252 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16611, just started
16611:C 06 May 2024 19:15:17.252 * Configuration loaded
16611:M 06 May 2024 19:15:17.253 * monotonic clock: POSIX clock_gettime
16611:M 06 May 2024 19:15:17.254 * Running mode=cluster, port=30008.
16611:M 06 May 2024 19:15:17.264 * Node configuration loaded, I'm 9dad64de16025f02b271510ee2d51a6ca079dc7b
16611:M 06 May 2024 19:15:17.265 * Server initialized
16611:M 06 May 2024 19:15:17.266 * Reading RDB base file on AOF loading...
16611:M 06 May 2024 19:15:17.266 * Loading RDB produced by valkey version 255.255.255
16611:M 06 May 2024 19:15:17.266 * RDB age 1 seconds
16611:M 06 May 2024 19:15:17.266 * RDB memory usage when created 2.38 Mb
16611:M 06 May 2024 19:15:17.266 * RDB is base AOF
16611:M 06 May 2024 19:15:17.267 * Done loading RDB, keys loaded: 427, keys expired: 0.
16611:M 06 May 2024 19:15:17.267 * DB loaded from base file appendonly.aof.6.base.rdb: 0.002 seconds
16611:M 06 May 2024 19:15:17.267 * DB loaded from incr file appendonly.aof.6.incr.aof: 0.000 seconds
16611:M 06 May 2024 19:15:17.267 * DB loaded from append only file: 0.002 seconds
16611:M 06 May 2024 19:15:17.267 * Opening AOF incr file appendonly.aof.6.incr.aof on server start
16611:M 06 May 2024 19:15:17.267 * Ready to accept connections tcp
16611:S 06 May 2024 19:15:17.269 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16611:S 06 May 2024 19:15:17.269 * Connecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:17.269 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:17.269 * Cluster state changed: ok
16611:S 06 May 2024 19:15:17.270 * Non blocking connect for SYNC fired the event.
16611:S 06 May 2024 19:15:17.273 * Master replied to PING, replication can continue...
16611:S 06 May 2024 19:15:17.276 * Trying a partial resynchronization (request 310cf7f3e88b64c57f33b69a44093e65df666de0:1).
16611:S 06 May 2024 19:15:17.276 * Full resync from master: f750808358d7a1060858c27fdc5d21f1723ef38d:39947
16611:S 06 May 2024 19:15:17.280 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16611:S 06 May 2024 19:15:17.281 * Discarding previously cached master state.
16611:S 06 May 2024 19:15:17.281 * MASTER <-> REPLICA sync: Flushing old data
16611:S 06 May 2024 19:15:17.281 * MASTER <-> REPLICA sync: Loading DB in memory
16611:S 06 May 2024 19:15:17.283 * Loading RDB produced by valkey version 255.255.255
16611:S 06 May 2024 19:15:17.283 * RDB age 0 seconds
16611:S 06 May 2024 19:15:17.283 * RDB memory usage when created 2.63 Mb
16611:S 06 May 2024 19:15:17.283 * Done loading RDB, keys loaded: 503, keys expired: 0.
16611:S 06 May 2024 19:15:17.284 * MASTER <-> REPLICA sync: Finished with success
16611:S 06 May 2024 19:15:17.284 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16611:S 06 May 2024 19:15:17.284 * Background append only file rewriting started by pid 16618
16618:C 06 May 2024 19:15:17.289 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16618.aof
16618:C 06 May 2024 19:15:17.289 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16611:S 06 May 2024 19:15:17.370 * Background AOF rewrite terminated with success
16611:S 06 May 2024 19:15:17.370 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16618.aof into appendonly.aof.7.base.rdb
16611:S 06 May 2024 19:15:17.370 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.7.incr.aof
16611:S 06 May 2024 19:15:17.371 * Removing the history file appendonly.aof.6.incr.aof in the background
16611:S 06 May 2024 19:15:17.371 * Removing the history file appendonly.aof.6.base.rdb in the background
16611:S 06 May 2024 19:15:17.373 * Background AOF rewrite finished successfully
16611:S 06 May 2024 19:15:17.387 # Missing implement of connection type tls
16611:S 06 May 2024 19:15:18.257 * Connection with master lost.
16611:S 06 May 2024 19:15:18.257 * Caching the disconnected master state.
16611:S 06 May 2024 19:15:18.257 * Reconnecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:18.258 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:18.258 # Error condition on socket for SYNC: Connection refused
16611:S 06 May 2024 19:15:19.188 * Connecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:19.188 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:19.188 # Error condition on socket for SYNC: Connection refused
16611:S 06 May 2024 19:15:20.199 * Connecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:20.199 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:20.199 # Error condition on socket for SYNC: Connection refused
16611:S 06 May 2024 19:15:21.210 * Connecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:21.211 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:21.211 # Error condition on socket for SYNC: Connection refused
16611:S 06 May 2024 19:15:21.819 * Marking node d979b3705974c02442b170e8dacc0ba5e04d56bb () as failing (quorum reached).
16611:S 06 May 2024 19:15:21.819 # Cluster state changed: fail
16611:S 06 May 2024 19:15:21.919 * Start of election delayed for 718 milliseconds (rank #0, offset 41897).
16611:S 06 May 2024 19:15:22.223 * Connecting to MASTER 127.0.0.1:30003
16611:S 06 May 2024 19:15:22.223 * MASTER <-> REPLICA sync started
16611:S 06 May 2024 19:15:22.223 # Error condition on socket for SYNC: Connection refused
16611:S 06 May 2024 19:15:22.728 * Starting a failover election for epoch 27.
16611:S 06 May 2024 19:15:22.738 * Failover election won: I'm the new master.
16611:S 06 May 2024 19:15:22.738 * configEpoch set to 27 after successful failover
16611:M 06 May 2024 19:15:22.738 * Discarding previously cached master state.
16611:M 06 May 2024 19:15:22.738 * Setting secondary replication ID to f750808358d7a1060858c27fdc5d21f1723ef38d, valid up to offset: 41898. New replication ID is b78d83567e01a86dfee7a97452f720fe423faf17
16611:M 06 May 2024 19:15:22.738 * Cluster state changed: ok
16611:M 06 May 2024 19:15:22.917 * Replica 127.0.0.1:30003 asks for synchronization
16611:M 06 May 2024 19:15:22.917 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'bffbf1a06f3d804e1b2660eeae93220b43785110', my replication IDs are 'b78d83567e01a86dfee7a97452f720fe423faf17' and 'f750808358d7a1060858c27fdc5d21f1723ef38d')
16611:M 06 May 2024 19:15:22.917 * Starting BGSAVE for SYNC with target: replicas sockets
16611:M 06 May 2024 19:15:22.918 * Background RDB transfer started by pid 16691
16691:C 06 May 2024 19:15:22.922 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16611:M 06 May 2024 19:15:22.922 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16611:M 06 May 2024 19:15:22.926 * Background RDB transfer terminated with success
16611:M 06 May 2024 19:15:22.926 * Streamed RDB transfer with replica 127.0.0.1:30003 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16611:M 06 May 2024 19:15:22.926 * Synchronization with replica 127.0.0.1:30003 succeeded
16611:M 06 May 2024 19:15:22.931 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16611:M 06 May 2024 19:15:22.931 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 27
16611:signal-handler (1715022923) Received SIGTERM scheduling shutdown...
16611:M 06 May 2024 19:15:23.737 * User requested shutdown...
16611:M 06 May 2024 19:15:23.737 * Waiting for replicas before shutting down.
16611:M 06 May 2024 19:15:23.838 * 1 of 1 replicas are in sync when shutting down.
16611:M 06 May 2024 19:15:23.838 # Valkey is now ready to exit, bye bye...
16756:C 06 May 2024 19:15:28.847 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16756:C 06 May 2024 19:15:28.847 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16756:C 06 May 2024 19:15:28.847 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16756, just started
16756:C 06 May 2024 19:15:28.847 * Configuration loaded
16756:M 06 May 2024 19:15:28.849 * monotonic clock: POSIX clock_gettime
16756:M 06 May 2024 19:15:28.851 * Running mode=cluster, port=30008.
16756:M 06 May 2024 19:15:28.857 * Node configuration loaded, I'm 9dad64de16025f02b271510ee2d51a6ca079dc7b
16756:M 06 May 2024 19:15:28.858 * Server initialized
16756:M 06 May 2024 19:15:28.859 * Reading RDB base file on AOF loading...
16756:M 06 May 2024 19:15:28.859 * Loading RDB produced by valkey version 255.255.255
16756:M 06 May 2024 19:15:28.859 * RDB age 11 seconds
16756:M 06 May 2024 19:15:28.859 * RDB memory usage when created 2.31 Mb
16756:M 06 May 2024 19:15:28.859 * RDB is base AOF
16756:M 06 May 2024 19:15:28.860 * Done loading RDB, keys loaded: 503, keys expired: 0.
16756:M 06 May 2024 19:15:28.861 * DB loaded from base file appendonly.aof.7.base.rdb: 0.002 seconds
16756:M 06 May 2024 19:15:28.861 * DB loaded from incr file appendonly.aof.7.incr.aof: 0.000 seconds
16756:M 06 May 2024 19:15:28.861 * DB loaded from append only file: 0.003 seconds
16756:M 06 May 2024 19:15:28.861 * Opening AOF incr file appendonly.aof.7.incr.aof on server start
16756:M 06 May 2024 19:15:28.861 * Ready to accept connections tcp
16756:M 06 May 2024 19:15:28.867 * Configuration change detected. Reconfiguring myself as a replica of d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16756:S 06 May 2024 19:15:28.867 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16756:S 06 May 2024 19:15:28.867 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:28.867 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:28.867 * Cluster state changed: ok
16756:S 06 May 2024 19:15:28.870 * Non blocking connect for SYNC fired the event.
16756:S 06 May 2024 19:15:28.871 * Master replied to PING, replication can continue...
16756:S 06 May 2024 19:15:28.871 * Trying a partial resynchronization (request ad2e7cf57ae1a45a8281b84f60bdf67ff2f0c277:1).
16756:S 06 May 2024 19:15:28.871 * Full resync from master: d6e270031cf72d78c1f37505ea394d35f8bd5140:52580
16756:S 06 May 2024 19:15:28.876 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16756:S 06 May 2024 19:15:28.878 * Discarding previously cached master state.
16756:S 06 May 2024 19:15:28.878 * MASTER <-> REPLICA sync: Flushing old data
16756:S 06 May 2024 19:15:28.879 * MASTER <-> REPLICA sync: Loading DB in memory
16756:S 06 May 2024 19:15:28.881 * Loading RDB produced by valkey version 255.255.255
16756:S 06 May 2024 19:15:28.881 * RDB age 0 seconds
16756:S 06 May 2024 19:15:28.881 * RDB memory usage when created 2.63 Mb
16756:S 06 May 2024 19:15:28.882 * Done loading RDB, keys loaded: 670, keys expired: 0.
16756:S 06 May 2024 19:15:28.882 * MASTER <-> REPLICA sync: Finished with success
16756:S 06 May 2024 19:15:28.882 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16756:S 06 May 2024 19:15:28.883 * Background append only file rewriting started by pid 16763
16763:C 06 May 2024 19:15:28.889 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16763.aof
16763:C 06 May 2024 19:15:28.890 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16756:S 06 May 2024 19:15:28.963 * Background AOF rewrite terminated with success
16756:S 06 May 2024 19:15:28.963 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16763.aof into appendonly.aof.8.base.rdb
16756:S 06 May 2024 19:15:28.963 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.8.incr.aof
16756:S 06 May 2024 19:15:28.965 * Removing the history file appendonly.aof.7.incr.aof in the background
16756:S 06 May 2024 19:15:28.965 * Removing the history file appendonly.aof.7.base.rdb in the background
16756:S 06 May 2024 19:15:28.968 * Background AOF rewrite finished successfully
16756:S 06 May 2024 19:15:28.995 # Missing implement of connection type tls
16756:S 06 May 2024 19:15:29.792 * Connection with master lost.
16756:S 06 May 2024 19:15:29.792 * Caching the disconnected master state.
16756:S 06 May 2024 19:15:29.792 * Reconnecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:29.792 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:29.793 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:15:30.781 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:30.781 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:30.781 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:15:31.791 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:31.792 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:31.792 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:15:32.804 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:32.804 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:32.804 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:15:33.579 * FAIL message received from 671088db4acca8792f424ee7ea50b8153c7529a0 () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16756:S 06 May 2024 19:15:33.579 # Cluster state changed: fail
16756:S 06 May 2024 19:15:33.611 * Start of election delayed for 707 milliseconds (rank #0, offset 53877).
16756:S 06 May 2024 19:15:33.813 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:33.813 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:33.813 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:15:34.320 * Starting a failover election for epoch 29.
16756:S 06 May 2024 19:15:34.329 * Failover election won: I'm the new master.
16756:S 06 May 2024 19:15:34.329 * configEpoch set to 29 after successful failover
16756:M 06 May 2024 19:15:34.329 * Discarding previously cached master state.
16756:M 06 May 2024 19:15:34.329 * Setting secondary replication ID to d6e270031cf72d78c1f37505ea394d35f8bd5140, valid up to offset: 53878. New replication ID is 0a7289b435cefec71ff5b5e1755b0a95f4545973
16756:M 06 May 2024 19:15:34.329 * Cluster state changed: ok
16756:M 06 May 2024 19:15:34.510 * Replica 127.0.0.1:30003 asks for synchronization
16756:M 06 May 2024 19:15:34.511 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '8734b27c87ef105ba6fdda23bc36c014f0a82324', my replication IDs are '0a7289b435cefec71ff5b5e1755b0a95f4545973' and 'd6e270031cf72d78c1f37505ea394d35f8bd5140')
16756:M 06 May 2024 19:15:34.511 * Starting BGSAVE for SYNC with target: replicas sockets
16756:M 06 May 2024 19:15:34.511 * Background RDB transfer started by pid 16829
16829:C 06 May 2024 19:15:34.517 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16756:M 06 May 2024 19:15:34.518 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16756:M 06 May 2024 19:15:34.525 * Background RDB transfer terminated with success
16756:M 06 May 2024 19:15:34.525 * Streamed RDB transfer with replica 127.0.0.1:30003 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16756:M 06 May 2024 19:15:34.525 * Synchronization with replica 127.0.0.1:30003 succeeded
16756:M 06 May 2024 19:15:34.529 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16756:M 06 May 2024 19:15:34.529 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 29
16756:M 06 May 2024 19:15:34.875 * Connection with replica 127.0.0.1:30003 lost.
16756:M 06 May 2024 19:15:34.944 * configEpoch set to 0 via CLUSTER RESET HARD
16756:M 06 May 2024 19:15:34.944 * Node hard reset, now I'm fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665
16756:M 06 May 2024 19:15:34.944 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
16756:M 06 May 2024 19:15:34.944 # Cluster state changed: fail
16756:M 06 May 2024 19:15:34.948 * CONFIG REWRITE executed with success.
16756:M 06 May 2024 19:15:40.387 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is no longer master of shard c39f6f20eed586245932eadb49d37ef3444eeba3; removed all 0 slot(s) it used to own
16756:M 06 May 2024 19:15:40.420 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is now part of shard 3a32b81103a1e70ff832bacb10b5a5a912e9e2dc
16756:S 06 May 2024 19:15:40.971 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16756:S 06 May 2024 19:15:40.971 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:15:40.971 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:15:40.971 * Cluster state changed: ok
16756:S 06 May 2024 19:15:40.973 * Non blocking connect for SYNC fired the event.
16756:S 06 May 2024 19:15:40.973 * Master replied to PING, replication can continue...
16756:S 06 May 2024 19:15:40.973 * Trying a partial resynchronization (request 0a7289b435cefec71ff5b5e1755b0a95f4545973:57767).
16756:S 06 May 2024 19:15:40.974 * Full resync from master: 1ff79c8cd3cd771290b2c20cb3c28347b4b3f89c:57748
16756:S 06 May 2024 19:15:40.976 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16756:S 06 May 2024 19:15:40.976 * Discarding previously cached master state.
16756:S 06 May 2024 19:15:40.976 * MASTER <-> REPLICA sync: Flushing old data
16756:S 06 May 2024 19:15:40.976 * MASTER <-> REPLICA sync: Loading DB in memory
16756:S 06 May 2024 19:15:40.978 * Loading RDB produced by valkey version 255.255.255
16756:S 06 May 2024 19:15:40.979 * RDB age 0 seconds
16756:S 06 May 2024 19:15:40.979 * RDB memory usage when created 2.68 Mb
16756:S 06 May 2024 19:15:40.979 * Done loading RDB, keys loaded: 0, keys expired: 0.
16756:S 06 May 2024 19:15:40.979 * MASTER <-> REPLICA sync: Finished with success
16756:S 06 May 2024 19:15:40.979 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16756:S 06 May 2024 19:15:40.980 * Background append only file rewriting started by pid 16873
16873:C 06 May 2024 19:15:40.981 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16873.aof
16873:C 06 May 2024 19:15:40.982 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16756:S 06 May 2024 19:15:41.081 * Background AOF rewrite terminated with success
16756:S 06 May 2024 19:15:41.081 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16873.aof into appendonly.aof.9.base.rdb
16756:S 06 May 2024 19:15:41.081 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.9.incr.aof
16756:S 06 May 2024 19:15:41.303 * Removing the history file appendonly.aof.8.incr.aof in the background
16756:S 06 May 2024 19:15:41.303 * Removing the history file appendonly.aof.8.base.rdb in the background
16756:S 06 May 2024 19:15:41.505 * Background AOF rewrite finished successfully
16756:S 06 May 2024 19:15:42.416 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is no longer master of shard e64780e5c850ee71f0b1b08d3ba6db17ccf4bce2; removed all 0 slot(s) it used to own
16756:S 06 May 2024 19:15:42.416 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is now part of shard d3a218f528452be7c9ae6dfd00c32dfc82e7170d
16756:S 06 May 2024 19:15:42.500 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is no longer master of shard f65a03c57d7accc16ebbe0644b4f594992bf8dce; removed all 0 slot(s) it used to own
16756:S 06 May 2024 19:15:42.500 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is now part of shard 897f6391c215d2fd35c543f86aae669878fdc874
16756:S 06 May 2024 19:15:42.512 * Node 6a35a86928be660bf08d53b5985287118d92066b () is no longer master of shard 17431b2ec492ed011920dbf31b9d73f688b42f6f; removed all 0 slot(s) it used to own
16756:S 06 May 2024 19:15:42.512 * Node 6a35a86928be660bf08d53b5985287118d92066b () is now part of shard 78600edd67e7ea47e3a029230067904131c90109
16756:S 06 May 2024 19:17:19.786 * Connection with master lost.
16756:S 06 May 2024 19:17:19.786 * Caching the disconnected master state.
16756:S 06 May 2024 19:17:19.786 * Reconnecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:17:19.786 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:17:19.786 # Error condition on socket for SYNC: Connection refused
16756:S 06 May 2024 19:17:19.900 * Connecting to MASTER 127.0.0.1:30003
16756:S 06 May 2024 19:17:19.900 * MASTER <-> REPLICA sync started
16756:S 06 May 2024 19:17:19.900 * Non blocking connect for SYNC fired the event.
16756:S 06 May 2024 19:17:19.901 # Error reply to PING from master: '-LOADING Valkey is loading the dataset in memory'
16756:signal-handler (1715023040) Received SIGTERM scheduling shutdown...
16756:S 06 May 2024 19:17:20.810 * User requested shutdown...
16756:S 06 May 2024 19:17:20.810 # Valkey is now ready to exit, bye bye...
17313:C 06 May 2024 19:17:20.844 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
17313:C 06 May 2024 19:17:20.844 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
17313:C 06 May 2024 19:17:20.844 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=17313, just started
17313:C 06 May 2024 19:17:20.844 * Configuration loaded
17313:M 06 May 2024 19:17:20.845 * monotonic clock: POSIX clock_gettime
17313:M 06 May 2024 19:17:20.846 * Running mode=cluster, port=30008.
17313:M 06 May 2024 19:17:20.852 * Node configuration loaded, I'm fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665
17313:M 06 May 2024 19:17:20.853 * Server initialized
17313:M 06 May 2024 19:17:20.874 * Reading RDB base file on AOF loading...
17313:M 06 May 2024 19:17:20.874 * Loading RDB produced by valkey version 255.255.255
17313:M 06 May 2024 19:17:20.874 * RDB age 100 seconds
17313:M 06 May 2024 19:17:20.874 * RDB memory usage when created 2.71 Mb
17313:M 06 May 2024 19:17:20.874 * RDB is base AOF
17313:M 06 May 2024 19:17:20.874 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:M 06 May 2024 19:17:20.874 * DB loaded from base file appendonly.aof.9.base.rdb: 0.021 seconds
17313:M 06 May 2024 19:17:20.947 * DB loaded from incr file appendonly.aof.9.incr.aof: 0.073 seconds
17313:M 06 May 2024 19:17:20.947 * DB loaded from append only file: 0.095 seconds
17313:M 06 May 2024 19:17:20.947 * Opening AOF incr file appendonly.aof.9.incr.aof on server start
17313:M 06 May 2024 19:17:20.947 * Ready to accept connections tcp
17313:S 06 May 2024 19:17:20.949 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:17:20.949 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:17:20.949 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:17:20.949 * Cluster state changed: ok
17313:S 06 May 2024 19:17:20.951 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:17:20.955 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:17:20.955 * Trying a partial resynchronization (request a7a15b990c192352b6a0f1444e7e415772e05f40:1).
17313:S 06 May 2024 19:17:20.955 * Full resync from master: 33a630f4d7b48a71055db691f9f38204265bc88e:0
17313:S 06 May 2024 19:17:20.960 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:17:20.995 * Discarding previously cached master state.
17313:S 06 May 2024 19:17:20.995 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:17:20.999 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:17:21.001 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:17:21.001 * RDB age 0 seconds
17313:S 06 May 2024 19:17:21.001 * RDB memory usage when created 3.53 Mb
17313:S 06 May 2024 19:17:21.013 * Done loading RDB, keys loaded: 10314, keys expired: 0.
17313:S 06 May 2024 19:17:21.013 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:17:21.013 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
17313:S 06 May 2024 19:17:21.014 * Background append only file rewriting started by pid 17323
17323:C 06 May 2024 19:17:21.051 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17323.aof
17323:C 06 May 2024 19:17:21.054 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 1 MB
17313:S 06 May 2024 19:17:21.152 * Background AOF rewrite terminated with success
17313:S 06 May 2024 19:17:21.152 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17323.aof into appendonly.aof.10.base.rdb
17313:S 06 May 2024 19:17:21.152 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.10.incr.aof
17313:S 06 May 2024 19:17:21.154 * Removing the history file appendonly.aof.9.incr.aof in the background
17313:S 06 May 2024 19:17:21.154 * Removing the history file appendonly.aof.9.base.rdb in the background
17313:S 06 May 2024 19:17:21.156 * Background AOF rewrite finished successfully
17313:M 06 May 2024 19:17:34.800 * Connection with master lost.
17313:M 06 May 2024 19:17:34.800 * Caching the disconnected master state.
17313:M 06 May 2024 19:17:34.800 * Discarding previously cached master state.
17313:M 06 May 2024 19:17:34.800 * Setting secondary replication ID to 33a630f4d7b48a71055db691f9f38204265bc88e, valid up to offset: 56. New replication ID is a484705c0284fdacb2dc5a254c1ad0a2735901fc
17313:M 06 May 2024 19:17:34.802 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:17:34.802 * Node hard reset, now I'm a64f8ad23dbc7e19bceb3d6de42d635121321462
17313:M 06 May 2024 19:17:34.802 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:17:34.802 # Cluster state changed: fail
17313:M 06 May 2024 19:17:34.807 * CONFIG REWRITE executed with success.
17313:M 06 May 2024 19:17:38.007 # Missing implement of connection type tls
17313:M 06 May 2024 19:17:41.775 * Cluster state changed: ok
17313:M 06 May 2024 19:17:42.781 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is no longer master of shard f7d317600bff619f321470e98d7e3c9c27a4af6a; removed all 0 slot(s) it used to own
17313:M 06 May 2024 19:17:43.655 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17313:M 06 May 2024 19:17:43.900 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is no longer master of shard 5acc99edc411149fcdb27d6533feac7207649ef7; removed all 0 slot(s) it used to own
17313:M 06 May 2024 19:17:43.900 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17313:M 06 May 2024 19:17:43.906 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is no longer master of shard 5000b06b9aff419dd88e8f75370c3da32551a1cb; removed all 0 slot(s) it used to own
17313:M 06 May 2024 19:17:43.907 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17313:S 06 May 2024 19:17:43.930 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:17:43.930 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:17:43.931 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:17:43.932 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:17:43.932 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:17:43.932 * Trying a partial resynchronization (request a484705c0284fdacb2dc5a254c1ad0a2735901fc:56).
17313:S 06 May 2024 19:17:43.933 * Full resync from master: 33a630f4d7b48a71055db691f9f38204265bc88e:55
17313:S 06 May 2024 19:17:43.935 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:17:43.935 * Discarding previously cached master state.
17313:S 06 May 2024 19:17:43.935 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:17:43.935 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:17:43.938 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:17:43.938 * RDB age 0 seconds
17313:S 06 May 2024 19:17:43.938 * RDB memory usage when created 2.73 Mb
17313:S 06 May 2024 19:17:43.938 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:17:43.938 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:17:44.507 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is no longer master of shard 54bf45e726f98496ec4871cc44f69eed009b4194; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:44.508 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17313:S 06 May 2024 19:17:44.550 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is no longer master of shard 1a16ede24c20c880468822e63f78780773a34e47; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:44.550 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17313:S 06 May 2024 19:17:45.356 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is no longer master of shard 6c2b56a67af1fb0be8ff8b1b4eefe129c23debb0; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:45.356 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17313:S 06 May 2024 19:17:45.412 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is no longer master of shard 1e88f30dc751b7903da19b3c4c8bc3173a3efb8b; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:45.412 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17313:S 06 May 2024 19:17:45.412 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is no longer master of shard 33c4d632371303bcb565e5c7aba0936e2a76b03a; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:45.412 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17313:S 06 May 2024 19:17:45.414 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is no longer master of shard 873ddfcf8e44ed993d28f2c2cc6dee5b6aa4f588; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:17:45.414 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17313:S 06 May 2024 19:17:49.043 * FAIL message received from 8638bb3919fb522af84a8fe3990ded8caa70968c () about 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ()
17313:S 06 May 2024 19:17:54.685 * Clear FAIL state for node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ():replica is reachable again.
17313:S 06 May 2024 19:17:58.849 * FAIL message received from c82ade1d42699c8d38b49704a4d508edf28962ad () about c6e768b2f8702ccd365a393b642c8ac83ff22379 ()
17313:S 06 May 2024 19:17:58.849 # Cluster state changed: fail
17313:S 06 May 2024 19:17:59.751 * Cluster state changed: ok
17313:S 06 May 2024 19:17:59.806 * Clear FAIL state for node c6e768b2f8702ccd365a393b642c8ac83ff22379 ():master without slots is reachable again.
17313:S 06 May 2024 19:17:59.807 * A failover occurred in shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc; node c6e768b2f8702ccd365a393b642c8ac83ff22379 () lost 0 slot(s) to node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () with a config epoch of 21
17313:M 06 May 2024 19:17:59.907 * Connection with master lost.
17313:M 06 May 2024 19:17:59.907 * Caching the disconnected master state.
17313:M 06 May 2024 19:17:59.907 * Discarding previously cached master state.
17313:M 06 May 2024 19:17:59.907 * Setting secondary replication ID to 33a630f4d7b48a71055db691f9f38204265bc88e, valid up to offset: 1151. New replication ID is ba181b25c0e3c6afde5b548b4b8c5f1d77fbbbc9
17313:M 06 May 2024 19:17:59.908 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:17:59.908 * Node hard reset, now I'm 9735b7886a7f7567bec72eec88999171c7b1481d
17313:M 06 May 2024 19:17:59.908 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:17:59.908 # Cluster state changed: fail
17313:M 06 May 2024 19:17:59.913 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:18:02.786 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:18:02.786 * Connecting to MASTER 127.0.0.1:30002
17313:S 06 May 2024 19:18:02.786 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:18:02.787 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:18:02.787 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:18:02.787 * Trying a partial resynchronization (request ba181b25c0e3c6afde5b548b4b8c5f1d77fbbbc9:1151).
17313:S 06 May 2024 19:18:02.787 * Full resync from master: e2b6958829f686f19f8f716ece21662ac44531c0:1460
17313:S 06 May 2024 19:18:02.789 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:18:02.789 * Discarding previously cached master state.
17313:S 06 May 2024 19:18:02.789 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:18:02.789 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:18:02.791 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:18:02.791 * RDB age 0 seconds
17313:S 06 May 2024 19:18:02.791 * RDB memory usage when created 2.79 Mb
17313:S 06 May 2024 19:18:02.791 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:18:02.791 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:18:02.830 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is no longer master of shard 7d57fa4efb72d70389b55e5fb06b1ed20ee0b8c3; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:02.830 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17313:S 06 May 2024 19:18:03.081 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is no longer master of shard 07caa91caff12c6fc179408189c0b2325234614a; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.081 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17313:S 06 May 2024 19:18:03.384 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is no longer master of shard 30c8fb7d5fcde244056aac469ab0a18d2dfa4e6a; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.384 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17313:S 06 May 2024 19:18:03.532 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is no longer master of shard cb9315006ffe129274468e5ed2feb38a7f33269f; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.532 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17313:S 06 May 2024 19:18:03.533 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is no longer master of shard 1bf2cd731a3dbe35a6cec5639318bdb95d99061d; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.533 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17313:S 06 May 2024 19:18:03.540 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is no longer master of shard 4a54cdb367579758a4e3f891240c194e22265b3d; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.540 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17313:S 06 May 2024 19:18:03.546 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is no longer master of shard dfcb8ddc797b36f3027193f68a243825e66ecb76; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.546 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17313:S 06 May 2024 19:18:03.582 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is no longer master of shard 75182d21b15b8439f1522e589f314577961349dd; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.582 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17313:S 06 May 2024 19:18:03.583 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is no longer master of shard 66a60939691fd1e6ee1492ac1dbc0dece4a724e9; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.583 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17313:S 06 May 2024 19:18:03.585 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is no longer master of shard 1f9cf276999006c5e2fcd79b4eb3d5840c05a58b; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.585 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17313:S 06 May 2024 19:18:03.587 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is no longer master of shard fe0f6b7f23d33e5bedb35ba3947a32d59a5d868f; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.587 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17313:S 06 May 2024 19:18:03.735 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is no longer master of shard 71ddfeaa46e5002962c1f850ecca2bc4f0152083; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:03.735 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17313:S 06 May 2024 19:18:04.037 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is no longer master of shard dae9d79e83baa0dbce3720b336c773fde1c02492; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:04.037 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17313:S 06 May 2024 19:18:04.038 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is no longer master of shard ab9f767e76ee34d7511533a455ad90ed1b50d27b; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:04.038 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17313:S 06 May 2024 19:18:04.542 * Cluster state changed: ok
17313:S 06 May 2024 19:18:11.042 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about 450684f318abb9002cec036680f3cb52f5f919b0 ()
17313:S 06 May 2024 19:18:11.042 # Cluster state changed: fail
17313:S 06 May 2024 19:18:11.774 * Cluster state changed: ok
17313:S 06 May 2024 19:18:16.544 * FAIL message received from 3a8410fdf29d80b74cbb245767c5e44636bc3277 () about 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ()
17313:S 06 May 2024 19:18:16.544 # Cluster state changed: fail
17313:S 06 May 2024 19:18:24.725 * Cluster state changed: ok
17313:S 06 May 2024 19:18:30.542 * FAIL message received from 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () about b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17313:S 06 May 2024 19:18:30.542 # Cluster state changed: fail
17313:S 06 May 2024 19:18:30.607 * Cluster state changed: ok
17313:S 06 May 2024 19:18:35.524 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17313:S 06 May 2024 19:18:35.524 # Cluster state changed: fail
17313:S 06 May 2024 19:18:37.665 * Cluster state changed: ok
17313:S 06 May 2024 19:18:42.075 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about ee4e8801b63592ad76065cf689613b8590e8adbd ()
17313:S 06 May 2024 19:18:42.075 # Cluster state changed: fail
17313:S 06 May 2024 19:18:42.165 * Cluster state changed: ok
17313:S 06 May 2024 19:18:42.252 * Clear FAIL state for node 450684f318abb9002cec036680f3cb52f5f919b0 ():master without slots is reachable again.
17313:S 06 May 2024 19:18:42.252 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 450684f318abb9002cec036680f3cb52f5f919b0 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17313:S 06 May 2024 19:18:42.353 * Clear FAIL state for node b4c96fe54bd211d5373161c89d6e0f09e6bae07a ():master without slots is reachable again.
17313:S 06 May 2024 19:18:42.354 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17313:S 06 May 2024 19:18:42.455 * Clear FAIL state for node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ():master without slots is reachable again.
17313:S 06 May 2024 19:18:42.455 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17313:S 06 May 2024 19:18:42.455 * Clear FAIL state for node ee4e8801b63592ad76065cf689613b8590e8adbd ():master without slots is reachable again.
17313:S 06 May 2024 19:18:42.455 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node ee4e8801b63592ad76065cf689613b8590e8adbd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17313:S 06 May 2024 19:18:42.557 * Clear FAIL state for node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ():master without slots is reachable again.
17313:S 06 May 2024 19:18:42.557 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17313:M 06 May 2024 19:18:42.642 * Connection with master lost.
17313:M 06 May 2024 19:18:42.642 * Caching the disconnected master state.
17313:M 06 May 2024 19:18:42.642 * Discarding previously cached master state.
17313:M 06 May 2024 19:18:42.642 * Setting secondary replication ID to e2b6958829f686f19f8f716ece21662ac44531c0, valid up to offset: 1558. New replication ID is c76be752f40ba624ee4b61f6a2d8beb25d9328f8
17313:M 06 May 2024 19:18:42.643 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:18:42.644 * Node hard reset, now I'm 73160c92fac075f0525b46dc984b9d0fc00a7319
17313:M 06 May 2024 19:18:42.644 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:18:42.644 # Cluster state changed: fail
17313:M 06 May 2024 19:18:42.648 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:18:45.324 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:18:45.324 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:18:45.325 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:18:45.325 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:18:45.325 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:18:45.326 * Trying a partial resynchronization (request c76be752f40ba624ee4b61f6a2d8beb25d9328f8:1558).
17313:S 06 May 2024 19:18:45.326 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:1298
17313:S 06 May 2024 19:18:45.328 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:18:45.328 * Discarding previously cached master state.
17313:S 06 May 2024 19:18:45.328 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:18:45.328 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:18:45.329 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:18:45.329 * RDB age 0 seconds
17313:S 06 May 2024 19:18:45.329 * RDB memory usage when created 2.59 Mb
17313:S 06 May 2024 19:18:45.330 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:18:45.330 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:18:45.504 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is no longer master of shard 7eb196d163c5bf7eb03371da41c37de6ab3a27c7; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:45.504 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is now part of shard d05b2d9c8fad619a6a58ab01b0e5ade057de5ca1
17313:S 06 May 2024 19:18:45.546 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is no longer master of shard 841113874b49ed5f9d1e126879f92ea4392dcdab; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:45.546 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is now part of shard 215dc02bf2afb70a46c0e0788801a0f308924862
17313:S 06 May 2024 19:18:45.575 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is no longer master of shard 91245c2cd07d4494975195dd1dfe064ec2d4cbfe; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:45.575 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is now part of shard 5c56bf8f5102b508a2405dedd16899f245264a91
17313:S 06 May 2024 19:18:45.599 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is no longer master of shard 35470f3f7014665726088479ee7b0dd699dc35de; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:18:45.599 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is now part of shard 482cb579f47fb8ccd86d73983fba9c22f7197882
17313:S 06 May 2024 19:18:46.548 * Cluster state changed: ok
17313:S 06 May 2024 19:18:52.513 * FAIL message received from 6e0ef4878fe1c8835271b94d91093f69119e2c5f () about 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ()
17313:S 06 May 2024 19:19:08.681 * Clear FAIL state for node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ():replica is reachable again.
17313:S 06 May 2024 19:19:12.528 * FAIL message received from e4844e444bd7db045069d93fe2f71d3259852adf () about 03a61a2c512c0aab55d8de4c03711ba741fddb86 ()
17313:S 06 May 2024 19:19:12.528 # Cluster state changed: fail
17313:S 06 May 2024 19:19:18.872 * Clear FAIL state for node 03a61a2c512c0aab55d8de4c03711ba741fddb86 (): is reachable again and nobody is serving its slots after some time.
17313:S 06 May 2024 19:19:18.872 * Cluster state changed: ok
17313:M 06 May 2024 19:19:18.987 * Connection with master lost.
17313:M 06 May 2024 19:19:18.987 * Caching the disconnected master state.
17313:M 06 May 2024 19:19:18.987 * Discarding previously cached master state.
17313:M 06 May 2024 19:19:18.987 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 1382. New replication ID is 853a64299c309267b07f119479c189020ad9de74
17313:M 06 May 2024 19:19:18.989 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:19:18.989 * Node hard reset, now I'm d61b725f6dd02d855fdc907190bd06a397b457f8
17313:M 06 May 2024 19:19:18.989 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:19:18.989 # Cluster state changed: fail
17313:M 06 May 2024 19:19:18.994 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:19:21.924 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:19:21.924 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:19:21.924 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:19:21.925 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:19:21.925 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:19:21.925 * Trying a partial resynchronization (request 853a64299c309267b07f119479c189020ad9de74:1382).
17313:S 06 May 2024 19:19:21.925 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:1381
17313:S 06 May 2024 19:19:21.927 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:19:21.927 * Discarding previously cached master state.
17313:S 06 May 2024 19:19:21.927 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:19:21.927 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:19:21.929 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:19:21.929 * RDB age 0 seconds
17313:S 06 May 2024 19:19:21.929 * RDB memory usage when created 2.66 Mb
17313:S 06 May 2024 19:19:21.929 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:19:21.929 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:19:22.498 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is no longer master of shard b3f6dd9ab5620f4d8bda693091f3d8d36ad0e7d3; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.498 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17313:S 06 May 2024 19:19:22.597 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is no longer master of shard 7c13d5b404a532519713ecb0009d48d7b7148c0e; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.597 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17313:S 06 May 2024 19:19:22.598 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is no longer master of shard 3e228277d31da320b8d2c0ab0d21aa067371dad3; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.598 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17313:S 06 May 2024 19:19:22.601 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is no longer master of shard 01f47c3a12aa4f053a807eeb5eb545c5e07db794; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.601 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17313:S 06 May 2024 19:19:22.602 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is no longer master of shard aae63656731167d9c699175203047b6e26efb4be; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.602 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17313:S 06 May 2024 19:19:22.937 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is no longer master of shard 5d8efc15227cc965bd521169b3cd81eb9295ec19; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:22.937 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17313:S 06 May 2024 19:19:23.038 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is no longer master of shard d8860f9f28d646383a01d2f3aba63d0f3a3e4faf; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:23.038 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17313:S 06 May 2024 19:19:23.507 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is no longer master of shard 1c166555732b739d6e4c1ab4dd8d165e832b3954; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:23.507 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17313:S 06 May 2024 19:19:23.509 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is no longer master of shard 614a1b42f4baa25c7f6d5a11b30ac1fdea247594; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:23.509 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17313:S 06 May 2024 19:19:23.558 * Cluster state changed: ok
17313:S 06 May 2024 19:19:29.020 * FAIL message received from 3d1a6ede1fb4785f3437ca2ee7d37934e6eb5120 () about dfea84e0be1bcd49482af5c67f555de10fdf29b3 ()
17313:S 06 May 2024 19:19:29.350 * FAIL message received from 5a72c35e8f819673e4c536c527b3bc7968d5372b () about 169ce63f84ab73647e330599b7668e357009f9c7 ()
17313:S 06 May 2024 19:19:29.538 * FAIL message received from 067f10da6671d30499813707fa18067ea3206b17 () about 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 ()
17313:S 06 May 2024 19:19:29.608 * FAIL message received from 43d342695b2be171fe42d4b24f7f98ee415c3435 () about 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 ()
17313:S 06 May 2024 19:19:35.299 * Clear FAIL state for node 169ce63f84ab73647e330599b7668e357009f9c7 ():replica is reachable again.
17313:S 06 May 2024 19:19:35.300 * Clear FAIL state for node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 ():replica is reachable again.
17313:S 06 May 2024 19:19:35.400 * Clear FAIL state for node dfea84e0be1bcd49482af5c67f555de10fdf29b3 ():replica is reachable again.
17313:S 06 May 2024 19:19:35.503 * Clear FAIL state for node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 ():replica is reachable again.
17313:M 06 May 2024 19:19:35.561 * Connection with master lost.
17313:M 06 May 2024 19:19:35.561 * Caching the disconnected master state.
17313:M 06 May 2024 19:19:35.561 * Discarding previously cached master state.
17313:M 06 May 2024 19:19:35.561 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 1451. New replication ID is 8a7af3376432d5bf50e812c80b23a7eab7f6f195
17313:M 06 May 2024 19:19:35.562 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:19:35.562 * Node hard reset, now I'm 801403f53dd98a8a4d2f3d43e78a2214c99c863e
17313:M 06 May 2024 19:19:35.562 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:19:35.562 # Cluster state changed: fail
17313:M 06 May 2024 19:19:35.575 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:19:41.479 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:19:41.479 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:19:41.479 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:19:41.480 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:19:41.480 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:19:41.480 * Trying a partial resynchronization (request 8a7af3376432d5bf50e812c80b23a7eab7f6f195:1451).
17313:S 06 May 2024 19:19:41.481 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:1450
17313:S 06 May 2024 19:19:41.482 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:19:41.482 * Discarding previously cached master state.
17313:S 06 May 2024 19:19:41.482 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:19:41.482 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:19:41.484 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:19:41.484 * RDB age 0 seconds
17313:S 06 May 2024 19:19:41.484 * RDB memory usage when created 2.68 Mb
17313:S 06 May 2024 19:19:41.484 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:19:41.484 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:19:41.557 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is no longer master of shard 63920ee806c2e289c54a7d03a407de389e09cdfa; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:41.557 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
17313:S 06 May 2024 19:19:41.568 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is no longer master of shard 1faef2c01b90ce80c9287617b3462fcd0eb7c5f5; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:41.568 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
17313:S 06 May 2024 19:19:41.608 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is no longer master of shard be264f012de4ca3b640590c83301730d9edc2c7c; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:41.608 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
17313:S 06 May 2024 19:19:41.608 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is no longer master of shard 5a93a431ee3c1974750eaeb45212552089e1d70f; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:41.608 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
17313:S 06 May 2024 19:19:42.080 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is no longer master of shard 2cb31efff9b31077914e5d8eee67fad987f900ec; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:42.080 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
17313:S 06 May 2024 19:19:42.080 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is no longer master of shard 5a313ed6e1afc160d88e2ec76f16f8322d6a6195; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:42.080 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
17313:S 06 May 2024 19:19:42.696 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is no longer master of shard b24cc857a20e418c76000cf7bb7c2121d56535c8; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:42.711 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
17313:S 06 May 2024 19:19:42.711 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is no longer master of shard a85808b5f3b195646b5e9c75d437940e5910c539; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:42.711 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
17313:S 06 May 2024 19:19:42.720 * Cluster state changed: ok
17313:S 06 May 2024 19:19:42.724 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is no longer master of shard c707b138b75efd32e96dd0d45165ae682cc6e5bb; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:42.724 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
17313:S 06 May 2024 19:19:49.143 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ()
17313:S 06 May 2024 19:19:49.143 # Cluster state changed: fail
17313:S 06 May 2024 19:19:49.187 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about b6d1f331303ea88d453961a6bc247c12cf3f1746 ()
17313:S 06 May 2024 19:19:50.285 * Cluster state changed: ok
17313:S 06 May 2024 19:19:55.511 * Clear FAIL state for node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ():master without slots is reachable again.
17313:S 06 May 2024 19:19:55.511 * A failover occurred in shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6; node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 () lost 0 slot(s) to node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () with a config epoch of 21
17313:S 06 May 2024 19:19:55.514 * Clear FAIL state for node b6d1f331303ea88d453961a6bc247c12cf3f1746 ():replica is reachable again.
17313:M 06 May 2024 19:19:55.642 * Connection with master lost.
17313:M 06 May 2024 19:19:55.642 * Caching the disconnected master state.
17313:M 06 May 2024 19:19:55.642 * Discarding previously cached master state.
17313:M 06 May 2024 19:19:55.642 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 2722. New replication ID is dd144e226e41327d550447f91c83767c6a19fd1f
17313:M 06 May 2024 19:19:55.644 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:19:55.644 * Node hard reset, now I'm caf83072d9c11f919729af96326335b051c7af49
17313:M 06 May 2024 19:19:55.644 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:19:55.644 # Cluster state changed: fail
17313:M 06 May 2024 19:19:55.649 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:19:58.965 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:19:58.965 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:19:58.965 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:19:58.965 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:19:58.966 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:19:58.966 * Trying a partial resynchronization (request dd144e226e41327d550447f91c83767c6a19fd1f:2722).
17313:S 06 May 2024 19:19:58.966 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:2721
17313:S 06 May 2024 19:19:58.967 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:19:58.968 * Discarding previously cached master state.
17313:S 06 May 2024 19:19:58.968 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:19:58.968 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:19:58.969 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:19:58.969 * RDB age 0 seconds
17313:S 06 May 2024 19:19:58.969 * RDB memory usage when created 2.73 Mb
17313:S 06 May 2024 19:19:58.969 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:19:58.970 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:19:59.538 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is no longer master of shard 5bedd120fdbe94c16f2c067685528596a15fe6e3; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:59.538 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is now part of shard e44c3a3bb3cc89ed450e11c453728db45301a89d
17313:S 06 May 2024 19:19:59.639 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is no longer master of shard 5679c354a9e2cf8cb62b26598f6d9e6a0f79b90d; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:19:59.639 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is now part of shard 5a15e11d32c6794f2f89faac54fc339efde5e362
17313:S 06 May 2024 19:20:00.036 * Cluster state changed: ok
17313:S 06 May 2024 19:20:00.511 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is no longer master of shard 8980e70c0b02f85ca396ef3dc659e0dfe2f2edd4; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:00.511 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is now part of shard 463631d5c660f73be6acc894f4e68e2bc6bf1f04
17313:S 06 May 2024 19:20:00.545 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is no longer master of shard 78683b2cc868ee4a3e989125c612bea0bc5c90c0; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:00.545 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is now part of shard b30cbe386d5d2fec228eb45db97ba04991f5020e
17313:S 06 May 2024 19:20:07.036 * FAIL message received from b2794df9d2dedc8cc3d17b75bd5d3e5c8ca3f180 () about 87c9e3fe2e97200a971dc5aee1822009f9498072 ()
17313:S 06 May 2024 19:20:07.036 # Cluster state changed: fail
17313:S 06 May 2024 19:20:07.921 * Cluster state changed: ok
17313:S 06 May 2024 19:20:12.092 * FAIL message received from a470ecfb84686f1c17778838dfcbeb2ec78c9395 () about ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 ()
17313:S 06 May 2024 19:20:12.093 # Cluster state changed: fail
17313:S 06 May 2024 19:20:12.228 * Clear FAIL state for node 87c9e3fe2e97200a971dc5aee1822009f9498072 ():master without slots is reachable again.
17313:S 06 May 2024 19:20:12.228 * A failover occurred in shard 5a15e11d32c6794f2f89faac54fc339efde5e362; node 87c9e3fe2e97200a971dc5aee1822009f9498072 () lost 0 slot(s) to node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () with a config epoch of 21
17313:S 06 May 2024 19:20:18.263 * Clear FAIL state for node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 (): is reachable again and nobody is serving its slots after some time.
17313:S 06 May 2024 19:20:18.263 * Cluster state changed: ok
17313:M 06 May 2024 19:20:20.649 * Connection with master lost.
17313:M 06 May 2024 19:20:20.649 * Caching the disconnected master state.
17313:M 06 May 2024 19:20:20.649 * Discarding previously cached master state.
17313:M 06 May 2024 19:20:20.649 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 4763. New replication ID is 36fa99808739f08975ac87e4561f522542261ab2
17313:M 06 May 2024 19:20:20.650 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:20:20.650 * Node hard reset, now I'm 4c6bf93f4560c44206e9ae153747809786d36116
17313:M 06 May 2024 19:20:20.650 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:20:20.650 # Cluster state changed: fail
17313:M 06 May 2024 19:20:21.032 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:20:25.007 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:20:25.008 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:20:25.008 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:20:25.008 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:20:25.009 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:20:25.009 * Trying a partial resynchronization (request 36fa99808739f08975ac87e4561f522542261ab2:4763).
17313:S 06 May 2024 19:20:25.009 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:4762
17313:S 06 May 2024 19:20:25.011 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:20:25.011 * Discarding previously cached master state.
17313:S 06 May 2024 19:20:25.011 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:20:25.011 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:20:25.012 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:20:25.012 * RDB age 0 seconds
17313:S 06 May 2024 19:20:25.012 * RDB memory usage when created 2.73 Mb
17313:S 06 May 2024 19:20:25.012 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:20:25.012 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:20:25.518 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is no longer master of shard c5c35fe41b55a11da192d43a8da61be171aa83e6; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:25.518 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is now part of shard e04c9009be37f9eec889c59340dc040d0fa0aeb8
17313:S 06 May 2024 19:20:25.573 * Cluster state changed: ok
17313:S 06 May 2024 19:20:25.576 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is no longer master of shard e8e4bf95f861f348154c047f60ca28434502f73e; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:25.576 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is now part of shard a0b63e81b62eaf4c5711335c6f1846909bf7fd69
17313:S 06 May 2024 19:20:25.597 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is no longer master of shard d22c30d988eed1903566ab8b5147735674e58aca; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:25.597 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is now part of shard 3671e746eaf2973108e1ea8bdd09970af685d6ad
17313:S 06 May 2024 19:20:25.643 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is no longer master of shard ad7c5024c1a4d2a21dcf6f4a1d9b1f45ff1ead2d; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:25.643 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is now part of shard 67e314dae915c111e42745e09df1e650a83178cf
17313:M 06 May 2024 19:20:28.680 * Connection with master lost.
17313:M 06 May 2024 19:20:28.681 * Caching the disconnected master state.
17313:M 06 May 2024 19:20:28.681 * Discarding previously cached master state.
17313:M 06 May 2024 19:20:28.681 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 4804. New replication ID is 1300b3760b05e9e9a4dcca19566e137ec92e8ce1
17313:M 06 May 2024 19:20:28.682 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:20:28.682 * Node hard reset, now I'm 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8
17313:M 06 May 2024 19:20:28.682 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:20:28.682 # Cluster state changed: fail
17313:M 06 May 2024 19:20:28.686 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:20:32.015 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:20:32.016 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:20:32.016 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:20:32.016 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:20:32.016 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:20:32.016 * Trying a partial resynchronization (request 1300b3760b05e9e9a4dcca19566e137ec92e8ce1:4804).
17313:S 06 May 2024 19:20:32.017 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:4803
17313:S 06 May 2024 19:20:32.019 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:20:32.019 * Discarding previously cached master state.
17313:S 06 May 2024 19:20:32.019 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:20:32.019 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:20:32.020 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:20:32.020 * RDB age 0 seconds
17313:S 06 May 2024 19:20:32.020 * RDB memory usage when created 2.78 Mb
17313:S 06 May 2024 19:20:32.020 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:20:32.021 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:20:32.511 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is no longer master of shard 7ac3ac70c33592cdd414687cbff2db517d34143f; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:32.511 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is now part of shard aa921447f701c6e1037283664b481acd5dcc18fa
17313:S 06 May 2024 19:20:32.511 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is no longer master of shard c0ff426f83007655b38a9ba2767c988c6843ca09; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:32.511 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is now part of shard c149d2cb057be6b4c53e6cda4f8361117987c75b
17313:S 06 May 2024 19:20:32.606 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is no longer master of shard b2b496b97d756568bb9cc80df81baa9597a2a1e0; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:32.606 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is now part of shard 9628b03c98fa51a28e8367ab2c3e2abc75d63330
17313:S 06 May 2024 19:20:33.028 * Cluster state changed: ok
17313:S 06 May 2024 19:20:33.514 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is no longer master of shard 10725e1ebc3e8158a7f12329adb9ba56ee854bc1; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:33.514 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is now part of shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba
17313:S 06 May 2024 19:20:37.544 * A failover occurred in shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba; node 0e59ecb0630140741e54370064819bde1c9ca23d () lost 0 slot(s) to node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () with a config epoch of 21
17313:M 06 May 2024 19:20:40.515 * Connection with master lost.
17313:M 06 May 2024 19:20:40.515 * Caching the disconnected master state.
17313:M 06 May 2024 19:20:40.515 * Discarding previously cached master state.
17313:M 06 May 2024 19:20:40.515 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 161845. New replication ID is 692ebdb1989a44de21931de6119eaae50ee4453d
17313:M 06 May 2024 19:20:40.517 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:20:40.517 * Node hard reset, now I'm 03f7c6a10e67bff0c5fac874576c356de6d20fb3
17313:M 06 May 2024 19:20:40.517 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:20:40.517 # Cluster state changed: fail
17313:M 06 May 2024 19:20:40.522 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:20:45.007 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:20:45.007 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:20:45.007 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:20:45.007 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:20:45.008 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:20:45.008 * Trying a partial resynchronization (request 692ebdb1989a44de21931de6119eaae50ee4453d:161845).
17313:S 06 May 2024 19:20:45.009 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:161844
17313:S 06 May 2024 19:20:45.010 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:20:45.010 * Discarding previously cached master state.
17313:S 06 May 2024 19:20:45.010 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:20:45.010 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:20:45.013 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:20:45.013 * RDB age 0 seconds
17313:S 06 May 2024 19:20:45.013 * RDB memory usage when created 2.99 Mb
17313:S 06 May 2024 19:20:45.013 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:20:45.013 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:20:45.043 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is no longer master of shard 065f0cbdfb7ce180a059fcb58c1396f6f36e7c7e; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:45.043 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is now part of shard d13e21ba301f8eb2ca5630faf519e74520eb0320
17313:S 06 May 2024 19:20:45.075 * Node 968e407a2757f6786a27813986652641dfa8c53e () is no longer master of shard c23ee8d4ddce6cd592f3f5d11f12e6f0e7d8357b; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:45.075 * Node 968e407a2757f6786a27813986652641dfa8c53e () is now part of shard 3dd139d2458ce1ba40162b75a210452f2564f13f
17313:S 06 May 2024 19:20:45.091 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is no longer master of shard ac0a659a65ff86d0acb03beca99488cf675db0b9; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:45.091 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is now part of shard 79761404f49765dfc1452b3572ad1557043c4b29
17313:S 06 May 2024 19:20:45.613 * Cluster state changed: ok
17313:S 06 May 2024 19:20:46.519 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is no longer master of shard 5894da5030fb6ea0d4890f9fcaefbcb689467382; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:20:46.519 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is now part of shard 942404fb5086368ee22ef226fc325d99eead816a
17313:S 06 May 2024 19:20:51.740 * FAIL message received from cf6fdb89f625f08d607b33125f0227fce622927f () about 51d1f09230533baaac63ae94738bf70b5db09342 ()
17313:S 06 May 2024 19:20:51.740 # Cluster state changed: fail
17313:S 06 May 2024 19:20:52.698 * Cluster state changed: ok
17313:S 06 May 2024 19:20:58.468 * Clear FAIL state for node 51d1f09230533baaac63ae94738bf70b5db09342 ():master without slots is reachable again.
17313:S 06 May 2024 19:20:58.468 * A failover occurred in shard 942404fb5086368ee22ef226fc325d99eead816a; node 51d1f09230533baaac63ae94738bf70b5db09342 () lost 0 slot(s) to node e0fb6aab67e48702bb3257b35176c6bec33ac399 () with a config epoch of 21
17313:M 06 May 2024 19:20:59.107 * Connection with master lost.
17313:M 06 May 2024 19:20:59.107 * Caching the disconnected master state.
17313:M 06 May 2024 19:20:59.108 * Discarding previously cached master state.
17313:M 06 May 2024 19:20:59.108 * Setting secondary replication ID to 48a0417ba031b40d7bac9faa1f725139214092e5, valid up to offset: 162994. New replication ID is e3570839f5d8345c06a31e2af060284986cd62e3
17313:M 06 May 2024 19:20:59.109 * configEpoch set to 0 via CLUSTER RESET HARD
17313:M 06 May 2024 19:20:59.109 * Node hard reset, now I'm 3213008a052e3d1f53d4bd09bc5dafcc1093df23
17313:M 06 May 2024 19:20:59.109 * configEpoch set to 9 via CLUSTER SET-CONFIG-EPOCH
17313:M 06 May 2024 19:20:59.109 # Cluster state changed: fail
17313:M 06 May 2024 19:20:59.113 * CONFIG REWRITE executed with success.
17313:S 06 May 2024 19:21:02.921 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17313:S 06 May 2024 19:21:02.921 * Connecting to MASTER 127.0.0.1:30003
17313:S 06 May 2024 19:21:02.921 * MASTER <-> REPLICA sync started
17313:S 06 May 2024 19:21:02.922 * Non blocking connect for SYNC fired the event.
17313:S 06 May 2024 19:21:02.922 * Master replied to PING, replication can continue...
17313:S 06 May 2024 19:21:02.922 * Trying a partial resynchronization (request e3570839f5d8345c06a31e2af060284986cd62e3:162994).
17313:S 06 May 2024 19:21:02.922 * Full resync from master: 48a0417ba031b40d7bac9faa1f725139214092e5:162993
17313:S 06 May 2024 19:21:02.925 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17313:S 06 May 2024 19:21:02.925 * Discarding previously cached master state.
17313:S 06 May 2024 19:21:02.925 * MASTER <-> REPLICA sync: Flushing old data
17313:S 06 May 2024 19:21:02.925 * MASTER <-> REPLICA sync: Loading DB in memory
17313:S 06 May 2024 19:21:02.927 * Loading RDB produced by valkey version 255.255.255
17313:S 06 May 2024 19:21:02.927 * RDB age 0 seconds
17313:S 06 May 2024 19:21:02.927 * RDB memory usage when created 2.99 Mb
17313:S 06 May 2024 19:21:02.927 * Done loading RDB, keys loaded: 0, keys expired: 0.
17313:S 06 May 2024 19:21:02.927 * MASTER <-> REPLICA sync: Finished with success
17313:S 06 May 2024 19:21:03.112 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is no longer master of shard c9e0b1451dee8dfc4e893df748587b969a26f679; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:21:03.112 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is now part of shard 75be0656223117757620482d8547e5fe2e896de1
17313:S 06 May 2024 19:21:03.112 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is no longer master of shard 54e8a9afa1bd7f67c860e6b09192d24f3b4b5af0; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:21:03.112 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is now part of shard fdf0c30757c93c36c4d6e2340444130d7ab6fd1c
17313:S 06 May 2024 19:21:03.515 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is no longer master of shard 122cd1fcb92a7460821b86e1e2d20ae156aef07b; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:21:03.515 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is now part of shard 9c3ef340a0f3031400e88f148755f8e3a44c39cc
17313:S 06 May 2024 19:21:03.522 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is no longer master of shard c9cdbc7bf1fd6b9537f6151b0e7952ae57507332; removed all 0 slot(s) it used to own
17313:S 06 May 2024 19:21:03.522 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is now part of shard 1e32518e495bab53ce49d6b0b26890950d0e6932
17313:S 06 May 2024 19:21:04.020 * Cluster state changed: ok
17313:S 06 May 2024 19:21:10.561 * FAIL message received from 4816b0fa4e909697f2f79ac7605b852ec0d22927 () about f1ab9aca89352244cecd21bb2bebff73910a1237 ()
17313:signal-handler (1715023274) Received SIGINT scheduling shutdown...
17313:S 06 May 2024 19:21:14.587 * User requested shutdown...
17313:S 06 May 2024 19:21:14.587 # Valkey is now ready to exit, bye bye...
