15705:C 06 May 2024 19:13:44.930 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
15705:C 06 May 2024 19:13:44.930 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15705:C 06 May 2024 19:13:44.930 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15705:C 06 May 2024 19:13:44.930 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15705, just started
15705:C 06 May 2024 19:13:44.930 * Configuration loaded
15705:M 06 May 2024 19:13:44.931 * monotonic clock: POSIX clock_gettime
15705:M 06 May 2024 19:13:44.931 * Running mode=cluster, port=30006.
15705:M 06 May 2024 19:13:44.932 * No cluster configuration found, I'm 99c122355b0b4f2bbbc66fb4619ce02e521f3e75
15705:M 06 May 2024 19:13:44.935 * Server initialized
15705:M 06 May 2024 19:13:44.936 * Creating AOF base file appendonly.aof.1.base.rdb on server start
15705:M 06 May 2024 19:13:44.938 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
15705:M 06 May 2024 19:13:44.938 * Ready to accept connections tcp
15705:M 06 May 2024 19:13:45.591 * configEpoch set to 0 via CLUSTER RESET HARD
15705:M 06 May 2024 19:13:45.591 * Node hard reset, now I'm 17e9e73860afc104a66cbf20ac810cd5e389b09d
15705:M 06 May 2024 19:13:45.591 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
15705:M 06 May 2024 19:13:45.595 * CONFIG REWRITE executed with success.
15705:M 06 May 2024 19:13:45.718 * IP address for this node updated to 127.0.0.1
15705:M 06 May 2024 19:13:48.323 # Missing implement of connection type tls
15705:M 06 May 2024 19:13:52.494 * Cluster state changed: ok
15705:M 06 May 2024 19:13:52.817 * configEpoch set to 0 via CLUSTER RESET HARD
15705:M 06 May 2024 19:13:52.817 * Node hard reset, now I'm 1e5c8c4a07f43bf52fd4450600942b910335d228
15705:M 06 May 2024 19:13:52.817 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
15705:M 06 May 2024 19:13:52.817 # Cluster state changed: fail
15705:M 06 May 2024 19:13:52.821 * CONFIG REWRITE executed with success.
15705:S 06 May 2024 19:13:56.908 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15705:S 06 May 2024 19:13:56.908 * Connecting to MASTER 127.0.0.1:30001
15705:S 06 May 2024 19:13:56.909 * MASTER <-> REPLICA sync started
15705:S 06 May 2024 19:13:56.909 * Non blocking connect for SYNC fired the event.
15705:S 06 May 2024 19:13:56.909 * Master replied to PING, replication can continue...
15705:S 06 May 2024 19:13:56.910 * Trying a partial resynchronization (request 435e982492c0f30c945ecb83d24d77f1d5688d24:3).
15705:S 06 May 2024 19:13:56.910 * Full resync from master: 1645ad9c0a98c33db55b69c5a4804c02751709e5:19
15705:S 06 May 2024 19:13:56.911 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15705:S 06 May 2024 19:13:56.912 * Discarding previously cached master state.
15705:S 06 May 2024 19:13:56.912 * MASTER <-> REPLICA sync: Flushing old data
15705:S 06 May 2024 19:13:56.912 * MASTER <-> REPLICA sync: Loading DB in memory
15705:S 06 May 2024 19:13:56.913 * Loading RDB produced by valkey version 255.255.255
15705:S 06 May 2024 19:13:56.913 * RDB age 0 seconds
15705:S 06 May 2024 19:13:56.913 * RDB memory usage when created 2.66 Mb
15705:S 06 May 2024 19:13:56.913 * Done loading RDB, keys loaded: 0, keys expired: 0.
15705:S 06 May 2024 19:13:56.913 * MASTER <-> REPLICA sync: Finished with success
15705:S 06 May 2024 19:13:56.913 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15705:S 06 May 2024 19:13:56.914 * Background append only file rewriting started by pid 15855
15855:C 06 May 2024 19:13:56.915 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15855.aof
15855:C 06 May 2024 19:13:56.915 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15705:S 06 May 2024 19:13:56.931 * Background AOF rewrite terminated with success
15705:S 06 May 2024 19:13:56.931 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15855.aof into appendonly.aof.2.base.rdb
15705:S 06 May 2024 19:13:56.931 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.2.incr.aof
15705:S 06 May 2024 19:13:56.933 * Removing the history file appendonly.aof.1.incr.aof in the background
15705:S 06 May 2024 19:13:56.933 * Removing the history file appendonly.aof.1.base.rdb in the background
15705:S 06 May 2024 19:13:56.934 * Background AOF rewrite finished successfully
15705:S 06 May 2024 19:13:57.036 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is no longer master of shard 8d23c540c28858cc5ee10f86f93cd31765bd7b8b; removed all 0 slot(s) it used to own
15705:S 06 May 2024 19:13:57.036 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is now part of shard 5f75c91ff19b41d8200527f5fc06215514a6c230
15705:S 06 May 2024 19:13:57.505 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is no longer master of shard 93a2a8fbb3ebee1719ea78e416af4f62a280e72f; removed all 0 slot(s) it used to own
15705:S 06 May 2024 19:13:57.505 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is now part of shard 0a3baa510e8cc3ec6c1dc6c6aff3c8c121a171ee
15705:S 06 May 2024 19:13:57.506 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is no longer master of shard 2aaea61241128999e7bf4d990a5d6bae89096d2d; removed all 0 slot(s) it used to own
15705:S 06 May 2024 19:13:57.506 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is now part of shard 9009f53e5fcc48bb5a0b06d0bc06b2a661c7165d
15705:S 06 May 2024 19:13:57.568 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is no longer master of shard 45a425a508f35dfb3598266dbd995af0daf15e78; removed all 0 slot(s) it used to own
15705:S 06 May 2024 19:13:57.568 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is now part of shard e89af52affaeadfe3f4c492254ae30531e68445a
15705:S 06 May 2024 19:13:57.585 * Cluster state changed: ok
15705:signal-handler (1715022840) Received SIGTERM scheduling shutdown...
15705:S 06 May 2024 19:14:00.260 * User requested shutdown...
15705:S 06 May 2024 19:14:00.260 * Calling fsync() on the AOF file.
15705:S 06 May 2024 19:14:00.260 # Valkey is now ready to exit, bye bye...
15949:C 06 May 2024 19:14:12.666 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15949:C 06 May 2024 19:14:12.666 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15949:C 06 May 2024 19:14:12.666 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15949, just started
15949:C 06 May 2024 19:14:12.666 * Configuration loaded
15949:M 06 May 2024 19:14:12.667 * monotonic clock: POSIX clock_gettime
15949:M 06 May 2024 19:14:12.668 * Running mode=cluster, port=30006.
15949:M 06 May 2024 19:14:12.674 * Node configuration loaded, I'm 1e5c8c4a07f43bf52fd4450600942b910335d228
15949:M 06 May 2024 19:14:12.674 * Server initialized
15949:M 06 May 2024 19:14:12.753 * Reading RDB base file on AOF loading...
15949:M 06 May 2024 19:14:12.753 * Loading RDB produced by valkey version 255.255.255
15949:M 06 May 2024 19:14:12.753 * RDB age 16 seconds
15949:M 06 May 2024 19:14:12.753 * RDB memory usage when created 2.57 Mb
15949:M 06 May 2024 19:14:12.753 * RDB is base AOF
15949:M 06 May 2024 19:14:12.753 * Done loading RDB, keys loaded: 0, keys expired: 0.
15949:M 06 May 2024 19:14:12.754 * DB loaded from base file appendonly.aof.2.base.rdb: 0.079 seconds
15949:M 06 May 2024 19:14:12.754 * DB loaded from append only file: 0.079 seconds
15949:M 06 May 2024 19:14:12.754 * Opening AOF incr file appendonly.aof.2.incr.aof on server start
15949:M 06 May 2024 19:14:12.754 * Ready to accept connections tcp
15949:S 06 May 2024 19:14:12.756 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15949:S 06 May 2024 19:14:12.756 * Connecting to MASTER 127.0.0.1:30001
15949:S 06 May 2024 19:14:12.756 * MASTER <-> REPLICA sync started
15949:S 06 May 2024 19:14:12.756 * Cluster state changed: ok
15949:S 06 May 2024 19:14:12.759 * Non blocking connect for SYNC fired the event.
15949:S 06 May 2024 19:14:12.763 * Master replied to PING, replication can continue...
15949:S 06 May 2024 19:14:12.880 * Trying a partial resynchronization (request 78ecf04a83e9428acecbac33c98803cf54080170:1).
15949:S 06 May 2024 19:14:12.881 * Full resync from master: 1645ad9c0a98c33db55b69c5a4804c02751709e5:19
15949:S 06 May 2024 19:14:12.885 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15949:S 06 May 2024 19:14:12.886 * Discarding previously cached master state.
15949:S 06 May 2024 19:14:12.886 * MASTER <-> REPLICA sync: Flushing old data
15949:S 06 May 2024 19:14:12.886 * MASTER <-> REPLICA sync: Loading DB in memory
15949:S 06 May 2024 19:14:13.541 * Loading RDB produced by valkey version 255.255.255
15949:S 06 May 2024 19:14:13.541 * RDB age 1 seconds
15949:S 06 May 2024 19:14:13.541 * RDB memory usage when created 2.67 Mb
15949:S 06 May 2024 19:14:13.541 * Done loading RDB, keys loaded: 0, keys expired: 0.
15949:S 06 May 2024 19:14:13.542 * MASTER <-> REPLICA sync: Finished with success
15949:S 06 May 2024 19:14:13.558 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15949:S 06 May 2024 19:14:13.559 * Background append only file rewriting started by pid 15968
15968:C 06 May 2024 19:14:14.024 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15968.aof
15968:C 06 May 2024 19:14:14.025 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15949:S 06 May 2024 19:14:14.062 * Background AOF rewrite terminated with success
15949:S 06 May 2024 19:14:14.062 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15968.aof into appendonly.aof.3.base.rdb
15949:S 06 May 2024 19:14:14.062 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.3.incr.aof
15949:S 06 May 2024 19:14:14.064 * Removing the history file appendonly.aof.2.incr.aof in the background
15949:S 06 May 2024 19:14:14.064 * Removing the history file appendonly.aof.2.base.rdb in the background
15949:S 06 May 2024 19:14:14.066 * Background AOF rewrite finished successfully
15949:M 06 May 2024 19:14:18.480 * Connection with master lost.
15949:M 06 May 2024 19:14:18.480 * Caching the disconnected master state.
15949:M 06 May 2024 19:14:18.480 * Discarding previously cached master state.
15949:M 06 May 2024 19:14:18.480 * Setting secondary replication ID to 1645ad9c0a98c33db55b69c5a4804c02751709e5, valid up to offset: 75. New replication ID is 1d2dc7f8a184e79df8c9974fcfa0b70153c09c68
15949:M 06 May 2024 19:14:18.482 * configEpoch set to 0 via CLUSTER RESET HARD
15949:M 06 May 2024 19:14:18.482 * Node hard reset, now I'm 4565dcea920547db24497a9ea7da3f8ba2d82492
15949:M 06 May 2024 19:14:18.482 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
15949:M 06 May 2024 19:14:18.482 # Cluster state changed: fail
15949:M 06 May 2024 19:14:18.486 * CONFIG REWRITE executed with success.
15949:M 06 May 2024 19:14:20.287 # Missing implement of connection type tls
15949:S 06 May 2024 19:14:21.893 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15949:S 06 May 2024 19:14:21.893 * Connecting to MASTER 127.0.0.1:30001
15949:S 06 May 2024 19:14:21.893 * MASTER <-> REPLICA sync started
15949:S 06 May 2024 19:14:21.894 * Non blocking connect for SYNC fired the event.
15949:S 06 May 2024 19:14:21.894 * Master replied to PING, replication can continue...
15949:S 06 May 2024 19:14:21.895 * Trying a partial resynchronization (request 1d2dc7f8a184e79df8c9974fcfa0b70153c09c68:75).
15949:S 06 May 2024 19:14:21.895 * Full resync from master: 1645ad9c0a98c33db55b69c5a4804c02751709e5:74
15949:S 06 May 2024 19:14:21.896 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15949:S 06 May 2024 19:14:21.897 * Discarding previously cached master state.
15949:S 06 May 2024 19:14:21.897 * MASTER <-> REPLICA sync: Flushing old data
15949:S 06 May 2024 19:14:21.897 * MASTER <-> REPLICA sync: Loading DB in memory
15949:S 06 May 2024 19:14:21.898 * Loading RDB produced by valkey version 255.255.255
15949:S 06 May 2024 19:14:21.898 * RDB age 0 seconds
15949:S 06 May 2024 19:14:21.898 * RDB memory usage when created 2.70 Mb
15949:S 06 May 2024 19:14:21.898 * Done loading RDB, keys loaded: 0, keys expired: 0.
15949:S 06 May 2024 19:14:21.898 * MASTER <-> REPLICA sync: Finished with success
15949:S 06 May 2024 19:14:21.898 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15949:S 06 May 2024 19:14:21.899 * Background append only file rewriting started by pid 16002
16002:C 06 May 2024 19:14:21.900 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16002.aof
16002:C 06 May 2024 19:14:21.901 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15949:S 06 May 2024 19:14:21.923 * Background AOF rewrite terminated with success
15949:S 06 May 2024 19:14:21.924 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16002.aof into appendonly.aof.4.base.rdb
15949:S 06 May 2024 19:14:21.924 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.4.incr.aof
15949:S 06 May 2024 19:14:21.925 * Removing the history file appendonly.aof.3.incr.aof in the background
15949:S 06 May 2024 19:14:21.925 * Removing the history file appendonly.aof.3.base.rdb in the background
15949:S 06 May 2024 19:14:21.927 * Background AOF rewrite finished successfully
15949:S 06 May 2024 19:14:22.028 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is no longer master of shard 10c2f585fc5fd868287fafbf5212b6227ab7017d; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:22.029 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is now part of shard bccca0c1e0e1dccd3b58d5709156154f9f4c9957
15949:S 06 May 2024 19:14:22.330 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is no longer master of shard f07b8fb6b6cbfb692ab1b87f30279a0b975efbbe; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:22.330 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is now part of shard a3bfec7d6b49cc6990a2b3560914e0bb1a7bda0c
15949:S 06 May 2024 19:14:22.976 * Cluster state changed: ok
15949:S 06 May 2024 19:14:23.136 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is no longer master of shard afe0861af93a47ac71d717dd7dd1f03d3dbe6564; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:23.136 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is now part of shard 6bb248ce1980dda8969ac6f65c18efe9f1d758d2
15949:S 06 May 2024 19:14:23.521 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is no longer master of shard a1ec0cec552145a6ed1b39a5162d032008e3e8fd; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:23.521 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is now part of shard 5ed49dbd94affeba17a99f22ab305f897e30146f
15949:S 06 May 2024 19:14:30.031 * FAIL message received from 220970bd9ddf17fd39dbd121039031b6350dc7bd () about 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ()
15949:S 06 May 2024 19:14:30.031 # Cluster state changed: fail
15949:S 06 May 2024 19:14:31.061 * Cluster state changed: ok
15949:M 06 May 2024 19:14:31.319 * Connection with master lost.
15949:M 06 May 2024 19:14:31.319 * Caching the disconnected master state.
15949:M 06 May 2024 19:14:31.319 * Discarding previously cached master state.
15949:M 06 May 2024 19:14:31.319 * Setting secondary replication ID to 1645ad9c0a98c33db55b69c5a4804c02751709e5, valid up to offset: 2314. New replication ID is 20152cf29ef46c8decbea11166932a181bb32344
15949:M 06 May 2024 19:14:31.321 * configEpoch set to 0 via CLUSTER RESET HARD
15949:M 06 May 2024 19:14:31.321 * Node hard reset, now I'm d612f4e24864679d4cb0b4d862c48e2df4373065
15949:M 06 May 2024 19:14:31.321 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
15949:M 06 May 2024 19:14:31.321 # Cluster state changed: fail
15949:M 06 May 2024 19:14:31.326 * CONFIG REWRITE executed with success.
15949:S 06 May 2024 19:14:34.824 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15949:S 06 May 2024 19:14:34.824 * Connecting to MASTER 127.0.0.1:30001
15949:S 06 May 2024 19:14:34.824 * MASTER <-> REPLICA sync started
15949:S 06 May 2024 19:14:34.825 * Non blocking connect for SYNC fired the event.
15949:S 06 May 2024 19:14:34.827 * Master replied to PING, replication can continue...
15949:S 06 May 2024 19:14:34.828 * Trying a partial resynchronization (request 20152cf29ef46c8decbea11166932a181bb32344:2314).
15949:S 06 May 2024 19:14:34.828 * Full resync from master: 1645ad9c0a98c33db55b69c5a4804c02751709e5:2313
15949:S 06 May 2024 19:14:34.829 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15949:S 06 May 2024 19:14:34.829 * Discarding previously cached master state.
15949:S 06 May 2024 19:14:34.830 * MASTER <-> REPLICA sync: Flushing old data
15949:S 06 May 2024 19:14:34.830 * MASTER <-> REPLICA sync: Loading DB in memory
15949:S 06 May 2024 19:14:34.831 * Loading RDB produced by valkey version 255.255.255
15949:S 06 May 2024 19:14:34.831 * RDB age 0 seconds
15949:S 06 May 2024 19:14:34.831 * RDB memory usage when created 2.70 Mb
15949:S 06 May 2024 19:14:34.831 * Done loading RDB, keys loaded: 0, keys expired: 0.
15949:S 06 May 2024 19:14:34.831 * MASTER <-> REPLICA sync: Finished with success
15949:S 06 May 2024 19:14:34.831 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15949:S 06 May 2024 19:14:34.831 * Background append only file rewriting started by pid 16111
16111:C 06 May 2024 19:14:34.832 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16111.aof
16111:C 06 May 2024 19:14:34.833 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15949:S 06 May 2024 19:14:34.922 * Background AOF rewrite terminated with success
15949:S 06 May 2024 19:14:34.922 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16111.aof into appendonly.aof.5.base.rdb
15949:S 06 May 2024 19:14:34.922 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.5.incr.aof
15949:S 06 May 2024 19:14:34.923 * Removing the history file appendonly.aof.4.incr.aof in the background
15949:S 06 May 2024 19:14:34.923 * Removing the history file appendonly.aof.4.base.rdb in the background
15949:S 06 May 2024 19:14:34.925 * Background AOF rewrite finished successfully
15949:S 06 May 2024 19:14:35.531 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is no longer master of shard 76f000c5383c5d2746bf6c7b9d86a92aed4820c2; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:35.531 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is now part of shard 45bde1a3488556534c5594a0f6ace0dba22be32b
15949:S 06 May 2024 19:14:35.531 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is no longer master of shard f88e4fd9e9180c926982aaac2f65e8d0105c55fe; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:35.531 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is now part of shard 1c2b75be0f3bbf57778cad018055649104140ab5
15949:S 06 May 2024 19:14:35.656 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is no longer master of shard 98ec8d4aaf47b8dc3863b2d7c51af4e948cd12a6; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:35.656 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is now part of shard 04e24d93f8b27f67291965144dd39f0ab3b35d25
15949:S 06 May 2024 19:14:36.073 * Node de3054474d708953456bc221c893ce4305ba5136 () is no longer master of shard 77b4d2bf3a14a66eb7635d415bbf20a86e150b57; removed all 0 slot(s) it used to own
15949:S 06 May 2024 19:14:36.073 * Node de3054474d708953456bc221c893ce4305ba5136 () is now part of shard 236cc077a81351c19b896662f24236e227403702
15949:S 06 May 2024 19:14:36.537 * Cluster state changed: ok
15949:S 06 May 2024 19:14:42.795 * FAIL message received from ffdb16089f859482065bad0e5c9a094d823a1410 () about c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ()
15949:S 06 May 2024 19:14:42.795 # Cluster state changed: fail
15949:S 06 May 2024 19:14:43.900 * Cluster state changed: ok
15949:S 06 May 2024 19:14:44.087 * Clear FAIL state for node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ():master without slots is reachable again.
15949:S 06 May 2024 19:14:44.087 * A failover occurred in shard 45bde1a3488556534c5594a0f6ace0dba22be32b; node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff () lost 0 slot(s) to node 671088db4acca8792f424ee7ea50b8153c7529a0 () with a config epoch of 21
15949:signal-handler (1715022884) Received SIGTERM scheduling shutdown...
15949:S 06 May 2024 19:14:44.793 * User requested shutdown...
15949:S 06 May 2024 19:14:44.793 # Valkey is now ready to exit, bye bye...
16204:C 06 May 2024 19:14:44.861 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16204:C 06 May 2024 19:14:44.861 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16204:C 06 May 2024 19:14:44.861 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16204, just started
16204:C 06 May 2024 19:14:44.861 * Configuration loaded
16204:M 06 May 2024 19:14:44.862 * monotonic clock: POSIX clock_gettime
16204:M 06 May 2024 19:14:44.863 * Running mode=cluster, port=30006.
16204:M 06 May 2024 19:14:44.869 * Node configuration loaded, I'm d612f4e24864679d4cb0b4d862c48e2df4373065
16204:M 06 May 2024 19:14:44.870 * Server initialized
16204:M 06 May 2024 19:14:44.870 * Reading RDB base file on AOF loading...
16204:M 06 May 2024 19:14:44.870 * Loading RDB produced by valkey version 255.255.255
16204:M 06 May 2024 19:14:44.870 * RDB age 10 seconds
16204:M 06 May 2024 19:14:44.870 * RDB memory usage when created 2.62 Mb
16204:M 06 May 2024 19:14:44.870 * RDB is base AOF
16204:M 06 May 2024 19:14:44.870 * Done loading RDB, keys loaded: 0, keys expired: 0.
16204:M 06 May 2024 19:14:44.870 * DB loaded from base file appendonly.aof.5.base.rdb: 0.001 seconds
16204:M 06 May 2024 19:14:44.871 * DB loaded from incr file appendonly.aof.5.incr.aof: 0.001 seconds
16204:M 06 May 2024 19:14:44.871 * DB loaded from append only file: 0.002 seconds
16204:M 06 May 2024 19:14:44.871 * Opening AOF incr file appendonly.aof.5.incr.aof on server start
16204:M 06 May 2024 19:14:44.871 * Ready to accept connections tcp
16204:S 06 May 2024 19:14:44.873 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16204:S 06 May 2024 19:14:44.873 * Connecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:44.873 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:44.873 * Cluster state changed: ok
16204:S 06 May 2024 19:14:44.874 * Non blocking connect for SYNC fired the event.
16204:S 06 May 2024 19:14:44.878 * Master replied to PING, replication can continue...
16204:S 06 May 2024 19:14:44.878 * Trying a partial resynchronization (request 3a730501fed39296d6050fe4f28f84fdfc162169:1).
16204:S 06 May 2024 19:14:44.878 * Full resync from master: 1645ad9c0a98c33db55b69c5a4804c02751709e5:14729
16204:S 06 May 2024 19:14:44.881 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16204:S 06 May 2024 19:14:44.882 * Discarding previously cached master state.
16204:S 06 May 2024 19:14:44.882 * MASTER <-> REPLICA sync: Flushing old data
16204:S 06 May 2024 19:14:44.882 * MASTER <-> REPLICA sync: Loading DB in memory
16204:S 06 May 2024 19:14:44.884 * Loading RDB produced by valkey version 255.255.255
16204:S 06 May 2024 19:14:44.884 * RDB age 0 seconds
16204:S 06 May 2024 19:14:44.884 * RDB memory usage when created 2.76 Mb
16204:S 06 May 2024 19:14:44.885 * Done loading RDB, keys loaded: 165, keys expired: 0.
16204:S 06 May 2024 19:14:44.885 * MASTER <-> REPLICA sync: Finished with success
16204:S 06 May 2024 19:14:44.885 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16204:S 06 May 2024 19:14:44.886 * Background append only file rewriting started by pid 16211
16211:C 06 May 2024 19:14:44.889 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16211.aof
16211:C 06 May 2024 19:14:44.890 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16204:S 06 May 2024 19:14:44.974 * Background AOF rewrite terminated with success
16204:S 06 May 2024 19:14:44.975 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16211.aof into appendonly.aof.6.base.rdb
16204:S 06 May 2024 19:14:44.975 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.6.incr.aof
16204:S 06 May 2024 19:14:44.977 * Removing the history file appendonly.aof.5.incr.aof in the background
16204:S 06 May 2024 19:14:44.977 * Removing the history file appendonly.aof.5.base.rdb in the background
16204:S 06 May 2024 19:14:44.978 * Background AOF rewrite finished successfully
16204:S 06 May 2024 19:14:45.325 # Missing implement of connection type tls
16204:S 06 May 2024 19:14:49.728 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16204:S 06 May 2024 19:14:49.728 # Cluster state changed: fail
16204:S 06 May 2024 19:14:50.837 * Cluster state changed: ok
16204:S 06 May 2024 19:14:51.025 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16204:S 06 May 2024 19:14:51.025 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 22
16204:S 06 May 2024 19:14:52.000 * Connection with master lost.
16204:S 06 May 2024 19:14:52.000 * Caching the disconnected master state.
16204:S 06 May 2024 19:14:52.000 * Reconnecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:52.001 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:52.001 # Error condition on socket for SYNC: Connection refused
16204:S 06 May 2024 19:14:52.941 * Connecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:52.941 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:52.941 # Error condition on socket for SYNC: Connection refused
16204:S 06 May 2024 19:14:53.952 * Connecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:53.952 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:53.952 # Error condition on socket for SYNC: Connection refused
16204:S 06 May 2024 19:14:54.962 * Connecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:54.962 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:54.962 # Error condition on socket for SYNC: Connection refused
16204:S 06 May 2024 19:14:55.631 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16204:S 06 May 2024 19:14:55.631 # Cluster state changed: fail
16204:S 06 May 2024 19:14:55.732 * Start of election delayed for 898 milliseconds (rank #0, offset 22429).
16204:S 06 May 2024 19:14:56.035 * Connecting to MASTER 127.0.0.1:30001
16204:S 06 May 2024 19:14:56.036 * MASTER <-> REPLICA sync started
16204:S 06 May 2024 19:14:56.036 # Error condition on socket for SYNC: Connection refused
16204:S 06 May 2024 19:14:56.642 * Starting a failover election for epoch 23.
16204:S 06 May 2024 19:14:56.931 * Failover election won: I'm the new master.
16204:S 06 May 2024 19:14:56.931 * configEpoch set to 23 after successful failover
16204:M 06 May 2024 19:14:56.931 * Discarding previously cached master state.
16204:M 06 May 2024 19:14:56.931 * Setting secondary replication ID to 1645ad9c0a98c33db55b69c5a4804c02751709e5, valid up to offset: 22430. New replication ID is 716eb3818a06cbfc7b11e0c0858338adb82a0f1d
16204:M 06 May 2024 19:14:56.931 * Cluster state changed: ok
16204:M 06 May 2024 19:14:57.257 * Replica 127.0.0.1:30001 asks for synchronization
16204:M 06 May 2024 19:14:57.257 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '98fbcaf94d0332a17df44c35f53a78686009ee8c', my replication IDs are '716eb3818a06cbfc7b11e0c0858338adb82a0f1d' and '1645ad9c0a98c33db55b69c5a4804c02751709e5')
16204:M 06 May 2024 19:14:57.257 * Starting BGSAVE for SYNC with target: replicas sockets
16204:M 06 May 2024 19:14:57.258 * Background RDB transfer started by pid 16366
16366:C 06 May 2024 19:14:57.261 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16204:M 06 May 2024 19:14:57.261 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16204:M 06 May 2024 19:14:57.264 * Background RDB transfer terminated with success
16204:M 06 May 2024 19:14:57.264 * Streamed RDB transfer with replica 127.0.0.1:30001 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16204:M 06 May 2024 19:14:57.264 * Synchronization with replica 127.0.0.1:30001 succeeded
16204:M 06 May 2024 19:14:57.330 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16204:M 06 May 2024 19:14:57.330 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 23
16204:signal-handler (1715022900) Received SIGTERM scheduling shutdown...
16204:M 06 May 2024 19:15:00.044 * User requested shutdown...
16204:M 06 May 2024 19:15:00.044 * Waiting for replicas before shutting down.
16204:M 06 May 2024 19:15:00.748 * 1 of 1 replicas are in sync when shutting down.
16204:M 06 May 2024 19:15:00.749 # Valkey is now ready to exit, bye bye...
16433:C 06 May 2024 19:15:05.009 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16433:C 06 May 2024 19:15:05.009 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16433:C 06 May 2024 19:15:05.009 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16433, just started
16433:C 06 May 2024 19:15:05.009 * Configuration loaded
16433:M 06 May 2024 19:15:05.010 * monotonic clock: POSIX clock_gettime
16433:M 06 May 2024 19:15:05.010 * Running mode=cluster, port=30006.
16433:M 06 May 2024 19:15:05.017 * Node configuration loaded, I'm d612f4e24864679d4cb0b4d862c48e2df4373065
16433:M 06 May 2024 19:15:05.017 * Server initialized
16433:M 06 May 2024 19:15:05.028 * Reading RDB base file on AOF loading...
16433:M 06 May 2024 19:15:05.028 * Loading RDB produced by valkey version 255.255.255
16433:M 06 May 2024 19:15:05.028 * RDB age 21 seconds
16433:M 06 May 2024 19:15:05.028 * RDB memory usage when created 2.25 Mb
16433:M 06 May 2024 19:15:05.028 * RDB is base AOF
16433:M 06 May 2024 19:15:05.029 * Done loading RDB, keys loaded: 165, keys expired: 0.
16433:M 06 May 2024 19:15:05.029 * DB loaded from base file appendonly.aof.6.base.rdb: 0.011 seconds
16433:M 06 May 2024 19:15:05.029 * DB loaded from incr file appendonly.aof.6.incr.aof: 0.001 seconds
16433:M 06 May 2024 19:15:05.029 * DB loaded from append only file: 0.012 seconds
16433:M 06 May 2024 19:15:05.029 * Opening AOF incr file appendonly.aof.6.incr.aof on server start
16433:M 06 May 2024 19:15:05.029 * Ready to accept connections tcp
16433:M 06 May 2024 19:15:05.035 * Configuration change detected. Reconfiguring myself as a replica of c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16433:S 06 May 2024 19:15:05.035 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16433:S 06 May 2024 19:15:05.035 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:05.035 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:05.036 * Cluster state changed: ok
16433:S 06 May 2024 19:15:05.040 * Non blocking connect for SYNC fired the event.
16433:S 06 May 2024 19:15:05.045 * Master replied to PING, replication can continue...
16433:S 06 May 2024 19:15:05.053 * Trying a partial resynchronization (request 7bb009dd0b0bcf058eadbb9d1abfc00a662ef8ba:1).
16433:S 06 May 2024 19:15:05.054 * Full resync from master: 8be18021bebbac3035fe3a303b7ba318c38cdaa7:26840
16433:S 06 May 2024 19:15:05.057 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16433:S 06 May 2024 19:15:05.057 * Discarding previously cached master state.
16433:S 06 May 2024 19:15:05.057 * MASTER <-> REPLICA sync: Flushing old data
16433:S 06 May 2024 19:15:05.058 * MASTER <-> REPLICA sync: Loading DB in memory
16433:S 06 May 2024 19:15:05.059 * Loading RDB produced by valkey version 255.255.255
16433:S 06 May 2024 19:15:05.059 * RDB age 0 seconds
16433:S 06 May 2024 19:15:05.059 * RDB memory usage when created 2.55 Mb
16433:S 06 May 2024 19:15:05.060 * Done loading RDB, keys loaded: 324, keys expired: 0.
16433:S 06 May 2024 19:15:05.060 * MASTER <-> REPLICA sync: Finished with success
16433:S 06 May 2024 19:15:05.060 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16433:S 06 May 2024 19:15:05.060 * Background append only file rewriting started by pid 16440
16440:C 06 May 2024 19:15:05.064 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16440.aof
16440:C 06 May 2024 19:15:05.064 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16433:S 06 May 2024 19:15:05.129 # Missing implement of connection type tls
16433:S 06 May 2024 19:15:05.131 * Background AOF rewrite terminated with success
16433:S 06 May 2024 19:15:05.131 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16440.aof into appendonly.aof.7.base.rdb
16433:S 06 May 2024 19:15:05.131 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.7.incr.aof
16433:S 06 May 2024 19:15:05.133 * Removing the history file appendonly.aof.6.incr.aof in the background
16433:S 06 May 2024 19:15:05.133 * Removing the history file appendonly.aof.6.base.rdb in the background
16433:S 06 May 2024 19:15:05.134 * Background AOF rewrite finished successfully
16433:S 06 May 2024 19:15:05.946 * Connection with master lost.
16433:S 06 May 2024 19:15:05.946 * Caching the disconnected master state.
16433:S 06 May 2024 19:15:05.946 * Reconnecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:05.946 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:05.946 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:15:06.950 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:06.950 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:06.951 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:15:07.961 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:07.961 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:07.961 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:15:08.971 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:08.971 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:08.971 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:15:09.995 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:09.996 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:09.996 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:15:10.099 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16433:S 06 May 2024 19:15:10.099 * Start of election delayed for 790 milliseconds (rank #0, offset 28139).
16433:S 06 May 2024 19:15:10.099 # Cluster state changed: fail
16433:S 06 May 2024 19:15:10.908 * Starting a failover election for epoch 25.
16433:S 06 May 2024 19:15:10.919 * Failover election won: I'm the new master.
16433:S 06 May 2024 19:15:10.919 * configEpoch set to 25 after successful failover
16433:M 06 May 2024 19:15:10.919 * Discarding previously cached master state.
16433:M 06 May 2024 19:15:10.920 * Setting secondary replication ID to 8be18021bebbac3035fe3a303b7ba318c38cdaa7, valid up to offset: 28140. New replication ID is 3e66f59faea0174093573c9fd7bd62eb55ac9076
16433:M 06 May 2024 19:15:10.920 * Cluster state changed: ok
16433:M 06 May 2024 19:15:11.093 * Replica 127.0.0.1:30001 asks for synchronization
16433:M 06 May 2024 19:15:11.093 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '423f4d2b2e196f22c2058c15d86ff88d8017fa70', my replication IDs are '3e66f59faea0174093573c9fd7bd62eb55ac9076' and '8be18021bebbac3035fe3a303b7ba318c38cdaa7')
16433:M 06 May 2024 19:15:11.093 * Starting BGSAVE for SYNC with target: replicas sockets
16433:M 06 May 2024 19:15:11.094 * Background RDB transfer started by pid 16509
16509:C 06 May 2024 19:15:11.098 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16433:M 06 May 2024 19:15:11.098 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16433:M 06 May 2024 19:15:11.108 * Background RDB transfer terminated with success
16433:M 06 May 2024 19:15:11.108 * Streamed RDB transfer with replica 127.0.0.1:30001 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16433:M 06 May 2024 19:15:11.108 * Synchronization with replica 127.0.0.1:30001 succeeded
16433:M 06 May 2024 19:15:11.117 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16433:M 06 May 2024 19:15:11.117 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 25
16433:M 06 May 2024 19:15:15.638 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about 9dad64de16025f02b271510ee2d51a6ca079dc7b ()
16433:M 06 May 2024 19:15:15.638 # Cluster state changed: fail
16433:M 06 May 2024 19:15:16.545 * Failover auth granted to d979b3705974c02442b170e8dacc0ba5e04d56bb () for epoch 26
16433:M 06 May 2024 19:15:16.587 * Cluster state changed: ok
16433:M 06 May 2024 19:15:16.852 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
16433:M 06 May 2024 19:15:16.852 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 26
16433:M 06 May 2024 19:15:16.934 * Connection with replica 127.0.0.1:30001 lost.
16433:M 06 May 2024 19:15:17.005 * Replica 127.0.0.1:30001 asks for synchronization
16433:M 06 May 2024 19:15:17.005 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '14cec3cd54e8197ca6e381c8caa5de9527968c68', my replication IDs are '3e66f59faea0174093573c9fd7bd62eb55ac9076' and '8be18021bebbac3035fe3a303b7ba318c38cdaa7')
16433:M 06 May 2024 19:15:17.005 * Starting BGSAVE for SYNC with target: replicas sockets
16433:M 06 May 2024 19:15:17.006 * Background RDB transfer started by pid 16601
16601:C 06 May 2024 19:15:17.010 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
16433:M 06 May 2024 19:15:17.010 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
16433:M 06 May 2024 19:15:17.013 * Background RDB transfer terminated with success
16433:M 06 May 2024 19:15:17.013 * Streamed RDB transfer with replica 127.0.0.1:30001 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
16433:M 06 May 2024 19:15:17.013 * Synchronization with replica 127.0.0.1:30001 succeeded
16433:M 06 May 2024 19:15:21.822 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16433:M 06 May 2024 19:15:21.822 # Cluster state changed: fail
16433:M 06 May 2024 19:15:22.734 * Failover auth granted to 9dad64de16025f02b271510ee2d51a6ca079dc7b () for epoch 27
16433:M 06 May 2024 19:15:22.742 * Cluster state changed: ok
16433:M 06 May 2024 19:15:22.997 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16433:M 06 May 2024 19:15:22.997 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 27
16433:M 06 May 2024 19:15:27.755 * FAIL message received from 29597550bd8f898394ae88db5a0ba45f1868fd3f () about 9dad64de16025f02b271510ee2d51a6ca079dc7b ()
16433:M 06 May 2024 19:15:27.755 # Cluster state changed: fail
16433:M 06 May 2024 19:15:28.687 * Failover auth granted to d979b3705974c02442b170e8dacc0ba5e04d56bb () for epoch 28
16433:M 06 May 2024 19:15:28.727 * Cluster state changed: ok
16433:M 06 May 2024 19:15:28.945 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
16433:M 06 May 2024 19:15:28.945 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 28
16433:M 06 May 2024 19:15:33.577 * FAIL message received from 671088db4acca8792f424ee7ea50b8153c7529a0 () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16433:M 06 May 2024 19:15:33.577 # Cluster state changed: fail
16433:M 06 May 2024 19:15:34.325 * Failover auth granted to 9dad64de16025f02b271510ee2d51a6ca079dc7b () for epoch 29
16433:M 06 May 2024 19:15:34.367 * Cluster state changed: ok
16433:M 06 May 2024 19:15:34.587 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16433:M 06 May 2024 19:15:34.587 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 29
16433:M 06 May 2024 19:15:34.858 * Connection with replica 127.0.0.1:30001 lost.
16433:M 06 May 2024 19:15:34.926 * configEpoch set to 0 via CLUSTER RESET HARD
16433:M 06 May 2024 19:15:34.926 * Node hard reset, now I'm 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b
16433:M 06 May 2024 19:15:34.926 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
16433:M 06 May 2024 19:15:34.926 # Cluster state changed: fail
16433:M 06 May 2024 19:15:34.934 * CONFIG REWRITE executed with success.
16433:M 06 May 2024 19:15:40.900 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is no longer master of shard c39f6f20eed586245932eadb49d37ef3444eeba3; removed all 0 slot(s) it used to own
16433:M 06 May 2024 19:15:40.902 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is now part of shard 3a32b81103a1e70ff832bacb10b5a5a912e9e2dc
16433:S 06 May 2024 19:15:40.915 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16433:S 06 May 2024 19:15:40.915 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:15:40.915 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:15:40.916 * Cluster state changed: ok
16433:S 06 May 2024 19:15:40.917 * Non blocking connect for SYNC fired the event.
16433:S 06 May 2024 19:15:40.917 * Master replied to PING, replication can continue...
16433:S 06 May 2024 19:15:40.919 * Trying a partial resynchronization (request 3e66f59faea0174093573c9fd7bd62eb55ac9076:58409).
16433:S 06 May 2024 19:15:40.919 * Full resync from master: ec93c8fb9a0d26217a16ede2b0c8826e1619ae1d:58390
16433:S 06 May 2024 19:15:40.921 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16433:S 06 May 2024 19:15:40.921 * Discarding previously cached master state.
16433:S 06 May 2024 19:15:40.921 * MASTER <-> REPLICA sync: Flushing old data
16433:S 06 May 2024 19:15:40.921 * MASTER <-> REPLICA sync: Loading DB in memory
16433:S 06 May 2024 19:15:40.922 * Loading RDB produced by valkey version 255.255.255
16433:S 06 May 2024 19:15:40.923 * RDB age 0 seconds
16433:S 06 May 2024 19:15:40.923 * RDB memory usage when created 2.68 Mb
16433:S 06 May 2024 19:15:40.923 * Done loading RDB, keys loaded: 0, keys expired: 0.
16433:S 06 May 2024 19:15:40.923 * MASTER <-> REPLICA sync: Finished with success
16433:S 06 May 2024 19:15:40.923 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16433:S 06 May 2024 19:15:40.924 * Background append only file rewriting started by pid 16869
16869:C 06 May 2024 19:15:40.925 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16869.aof
16869:C 06 May 2024 19:15:40.925 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16433:S 06 May 2024 19:15:40.979 * Background AOF rewrite terminated with success
16433:S 06 May 2024 19:15:40.979 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16869.aof into appendonly.aof.8.base.rdb
16433:S 06 May 2024 19:15:40.979 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.8.incr.aof
16433:S 06 May 2024 19:15:40.981 * Removing the history file appendonly.aof.7.incr.aof in the background
16433:S 06 May 2024 19:15:40.982 * Removing the history file appendonly.aof.7.base.rdb in the background
16433:S 06 May 2024 19:15:40.983 * Background AOF rewrite finished successfully
16433:S 06 May 2024 19:15:42.383 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is no longer master of shard e64780e5c850ee71f0b1b08d3ba6db17ccf4bce2; removed all 0 slot(s) it used to own
16433:S 06 May 2024 19:15:42.383 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is now part of shard d3a218f528452be7c9ae6dfd00c32dfc82e7170d
16433:S 06 May 2024 19:15:42.498 * Node 6a35a86928be660bf08d53b5985287118d92066b () is no longer master of shard 17431b2ec492ed011920dbf31b9d73f688b42f6f; removed all 0 slot(s) it used to own
16433:S 06 May 2024 19:15:42.498 * Node 6a35a86928be660bf08d53b5985287118d92066b () is now part of shard 78600edd67e7ea47e3a029230067904131c90109
16433:S 06 May 2024 19:15:42.502 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is no longer master of shard 3af8d21a8c53a9b6624c360029651d1f88745f55; removed all 0 slot(s) it used to own
16433:S 06 May 2024 19:15:42.502 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is now part of shard 6b4d9b2c1f3548ebff40277e67444291a3fa23f5
16433:S 06 May 2024 19:17:19.198 * Connection with master lost.
16433:S 06 May 2024 19:17:19.198 * Caching the disconnected master state.
16433:S 06 May 2024 19:17:19.198 * Reconnecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:17:19.199 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:17:19.199 # Error condition on socket for SYNC: Connection refused
16433:S 06 May 2024 19:17:19.798 * Connecting to MASTER 127.0.0.1:30001
16433:S 06 May 2024 19:17:19.798 * MASTER <-> REPLICA sync started
16433:S 06 May 2024 19:17:19.798 * Non blocking connect for SYNC fired the event.
16433:S 06 May 2024 19:17:19.798 * Master replied to PING, replication can continue...
16433:S 06 May 2024 19:17:19.799 * Trying a partial resynchronization (request ec93c8fb9a0d26217a16ede2b0c8826e1619ae1d:1725443).
16433:S 06 May 2024 19:17:19.799 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:0
16433:S 06 May 2024 19:17:19.803 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16433:S 06 May 2024 19:17:19.825 * Discarding previously cached master state.
16433:S 06 May 2024 19:17:19.826 * MASTER <-> REPLICA sync: Flushing old data
16433:S 06 May 2024 19:17:19.829 * MASTER <-> REPLICA sync: Loading DB in memory
16433:S 06 May 2024 19:17:19.831 * Loading RDB produced by valkey version 255.255.255
16433:S 06 May 2024 19:17:19.831 * RDB age 0 seconds
16433:S 06 May 2024 19:17:19.831 * RDB memory usage when created 3.50 Mb
16433:S 06 May 2024 19:17:19.852 * Done loading RDB, keys loaded: 10046, keys expired: 0.
16433:S 06 May 2024 19:17:19.852 * MASTER <-> REPLICA sync: Finished with success
16433:S 06 May 2024 19:17:19.852 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16433:S 06 May 2024 19:17:19.853 * Background append only file rewriting started by pid 17242
17242:C 06 May 2024 19:17:19.881 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17242.aof
17242:C 06 May 2024 19:17:19.881 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 0 MB
16433:S 06 May 2024 19:17:19.900 * Background AOF rewrite terminated with success
16433:S 06 May 2024 19:17:19.900 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17242.aof into appendonly.aof.9.base.rdb
16433:S 06 May 2024 19:17:19.900 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.9.incr.aof
16433:S 06 May 2024 19:17:19.901 * Removing the history file appendonly.aof.8.incr.aof in the background
16433:S 06 May 2024 19:17:19.902 * Removing the history file appendonly.aof.8.base.rdb in the background
16433:S 06 May 2024 19:17:19.903 * Background AOF rewrite finished successfully
16433:signal-handler (1715023040) Received SIGTERM scheduling shutdown...
16433:S 06 May 2024 19:17:20.408 * User requested shutdown...
16433:S 06 May 2024 19:17:20.409 # Valkey is now ready to exit, bye bye...
17283:C 06 May 2024 19:17:20.449 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
17283:C 06 May 2024 19:17:20.449 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
17283:C 06 May 2024 19:17:20.449 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=17283, just started
17283:C 06 May 2024 19:17:20.449 * Configuration loaded
17283:M 06 May 2024 19:17:20.450 * monotonic clock: POSIX clock_gettime
17283:M 06 May 2024 19:17:20.451 * Running mode=cluster, port=30006.
17283:M 06 May 2024 19:17:20.458 * Node configuration loaded, I'm 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b
17283:M 06 May 2024 19:17:20.459 * Server initialized
17283:M 06 May 2024 19:17:20.459 * Reading RDB base file on AOF loading...
17283:M 06 May 2024 19:17:20.459 * Loading RDB produced by valkey version 255.255.255
17283:M 06 May 2024 19:17:20.459 * RDB age 1 seconds
17283:M 06 May 2024 19:17:20.459 * RDB memory usage when created 3.95 Mb
17283:M 06 May 2024 19:17:20.459 * RDB is base AOF
17283:M 06 May 2024 19:17:20.476 * Done loading RDB, keys loaded: 10046, keys expired: 0.
17283:M 06 May 2024 19:17:20.476 * DB loaded from base file appendonly.aof.9.base.rdb: 0.017 seconds
17283:M 06 May 2024 19:17:20.476 * DB loaded from append only file: 0.017 seconds
17283:M 06 May 2024 19:17:20.476 * Opening AOF incr file appendonly.aof.9.incr.aof on server start
17283:M 06 May 2024 19:17:20.476 * Ready to accept connections tcp
17283:S 06 May 2024 19:17:20.478 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17283:S 06 May 2024 19:17:20.478 * Connecting to MASTER 127.0.0.1:30001
17283:S 06 May 2024 19:17:20.478 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:17:20.478 * Cluster state changed: ok
17283:S 06 May 2024 19:17:20.486 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:17:20.490 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:17:20.490 * Trying a partial resynchronization (request 3205d2fa86469555c3fec74ace312afcd41506bc:1).
17283:S 06 May 2024 19:17:20.491 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:14
17283:S 06 May 2024 19:17:20.494 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17283:S 06 May 2024 19:17:20.524 * Discarding previously cached master state.
17283:S 06 May 2024 19:17:20.524 * MASTER <-> REPLICA sync: Flushing old data
17283:S 06 May 2024 19:17:20.526 * MASTER <-> REPLICA sync: Loading DB in memory
17283:S 06 May 2024 19:17:20.528 * Loading RDB produced by valkey version 255.255.255
17283:S 06 May 2024 19:17:20.528 * RDB age 0 seconds
17283:S 06 May 2024 19:17:20.528 * RDB memory usage when created 3.55 Mb
17283:S 06 May 2024 19:17:20.540 * Done loading RDB, keys loaded: 10046, keys expired: 0.
17283:S 06 May 2024 19:17:20.540 * MASTER <-> REPLICA sync: Finished with success
17283:S 06 May 2024 19:17:20.540 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
17283:S 06 May 2024 19:17:20.541 * Background append only file rewriting started by pid 17293
17293:C 06 May 2024 19:17:20.573 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17293.aof
17293:C 06 May 2024 19:17:20.573 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 0 MB
17283:S 06 May 2024 19:17:20.579 * Background AOF rewrite terminated with success
17283:S 06 May 2024 19:17:20.579 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17293.aof into appendonly.aof.10.base.rdb
17283:S 06 May 2024 19:17:20.580 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.10.incr.aof
17283:S 06 May 2024 19:17:20.581 * Removing the history file appendonly.aof.9.incr.aof in the background
17283:S 06 May 2024 19:17:20.581 * Removing the history file appendonly.aof.9.base.rdb in the background
17283:S 06 May 2024 19:17:20.583 * Background AOF rewrite finished successfully
17283:M 06 May 2024 19:17:34.784 * Connection with master lost.
17283:M 06 May 2024 19:17:34.784 * Caching the disconnected master state.
17283:M 06 May 2024 19:17:34.784 * Discarding previously cached master state.
17283:M 06 May 2024 19:17:34.784 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 70. New replication ID is cefd5d91661fdd8fe05e01b6e4cab54595dc05f2
17283:M 06 May 2024 19:17:34.786 * configEpoch set to 0 via CLUSTER RESET HARD
17283:M 06 May 2024 19:17:34.786 * Node hard reset, now I'm e1039ccb24f58e1d87d2782b61596d09ff577abb
17283:M 06 May 2024 19:17:34.786 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
17283:M 06 May 2024 19:17:34.786 # Cluster state changed: fail
17283:M 06 May 2024 19:17:34.790 * CONFIG REWRITE executed with success.
17283:M 06 May 2024 19:17:37.609 # Missing implement of connection type tls
17283:M 06 May 2024 19:17:41.874 * Cluster state changed: ok
17283:S 06 May 2024 19:17:42.631 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17283:S 06 May 2024 19:17:42.631 * Connecting to MASTER 127.0.0.1:30001
17283:S 06 May 2024 19:17:42.631 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:17:42.632 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:17:42.633 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:17:42.633 * Trying a partial resynchronization (request cefd5d91661fdd8fe05e01b6e4cab54595dc05f2:70).
17283:S 06 May 2024 19:17:42.633 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:69
17283:S 06 May 2024 19:17:42.635 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17283:S 06 May 2024 19:17:42.635 * Discarding previously cached master state.
17283:S 06 May 2024 19:17:42.635 * MASTER <-> REPLICA sync: Flushing old data
17283:S 06 May 2024 19:17:42.636 * MASTER <-> REPLICA sync: Loading DB in memory
17283:S 06 May 2024 19:17:42.644 * Loading RDB produced by valkey version 255.255.255
17283:S 06 May 2024 19:17:42.644 * RDB age 0 seconds
17283:S 06 May 2024 19:17:42.644 * RDB memory usage when created 2.73 Mb
17283:S 06 May 2024 19:17:42.644 * Done loading RDB, keys loaded: 0, keys expired: 0.
17283:S 06 May 2024 19:17:42.644 * MASTER <-> REPLICA sync: Finished with success
17283:S 06 May 2024 19:17:43.910 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is no longer master of shard 5000b06b9aff419dd88e8f75370c3da32551a1cb; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:43.910 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17283:S 06 May 2024 19:17:44.090 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is no longer master of shard f7d317600bff619f321470e98d7e3c9c27a4af6a; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:44.091 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17283:S 06 May 2024 19:17:44.153 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is no longer master of shard 1a16ede24c20c880468822e63f78780773a34e47; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:44.153 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17283:S 06 May 2024 19:17:44.207 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is no longer master of shard 873ddfcf8e44ed993d28f2c2cc6dee5b6aa4f588; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:44.207 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17283:S 06 May 2024 19:17:44.349 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is no longer master of shard 6c2b56a67af1fb0be8ff8b1b4eefe129c23debb0; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:44.349 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17283:S 06 May 2024 19:17:45.450 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is no longer master of shard 33c4d632371303bcb565e5c7aba0936e2a76b03a; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:45.450 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17283:S 06 May 2024 19:17:45.452 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is no longer master of shard 1e88f30dc751b7903da19b3c4c8bc3173a3efb8b; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:45.452 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17283:S 06 May 2024 19:17:45.452 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is no longer master of shard 0855cdcd88c49b00e8d3cb7a4549759c3b4ffc32; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:45.452 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17283:S 06 May 2024 19:17:45.559 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is no longer master of shard 54bf45e726f98496ec4871cc44f69eed009b4194; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:17:45.559 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17283:S 06 May 2024 19:17:49.045 * FAIL message received from 8638bb3919fb522af84a8fe3990ded8caa70968c () about 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ()
17283:S 06 May 2024 19:17:54.687 * Clear FAIL state for node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ():replica is reachable again.
17283:S 06 May 2024 19:17:58.849 * FAIL message received from c82ade1d42699c8d38b49704a4d508edf28962ad () about c6e768b2f8702ccd365a393b642c8ac83ff22379 ()
17283:S 06 May 2024 19:17:58.849 # Cluster state changed: fail
17283:S 06 May 2024 19:17:59.751 * Cluster state changed: ok
17283:M 06 May 2024 19:17:59.891 * Connection with master lost.
17283:M 06 May 2024 19:17:59.891 * Caching the disconnected master state.
17283:M 06 May 2024 19:17:59.891 * Discarding previously cached master state.
17283:M 06 May 2024 19:17:59.891 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 1555. New replication ID is 7def704c3091fcfddcd52695acae65e41389de64
17283:M 06 May 2024 19:17:59.892 * configEpoch set to 0 via CLUSTER RESET HARD
17283:M 06 May 2024 19:17:59.892 * Node hard reset, now I'm 1e008697fe34fc59625174f184ad2adc37042797
17283:M 06 May 2024 19:17:59.892 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
17283:M 06 May 2024 19:17:59.892 # Cluster state changed: fail
17283:M 06 May 2024 19:17:59.897 * CONFIG REWRITE executed with success.
17283:S 06 May 2024 19:18:02.772 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17283:S 06 May 2024 19:18:02.772 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:02.772 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:02.772 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:02.773 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:02.773 * Trying a partial resynchronization (request 7def704c3091fcfddcd52695acae65e41389de64:1555).
17283:S 06 May 2024 19:18:02.773 * Full resync from master: 8fa9911ae05d0a1fbbaa8d27c7cfba10046179f9:1150
17283:S 06 May 2024 19:18:02.774 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17283:S 06 May 2024 19:18:02.774 * Discarding previously cached master state.
17283:S 06 May 2024 19:18:02.774 * MASTER <-> REPLICA sync: Flushing old data
17283:S 06 May 2024 19:18:02.775 * MASTER <-> REPLICA sync: Loading DB in memory
17283:S 06 May 2024 19:18:02.776 * Loading RDB produced by valkey version 255.255.255
17283:S 06 May 2024 19:18:02.776 * RDB age 0 seconds
17283:S 06 May 2024 19:18:02.776 * RDB memory usage when created 2.65 Mb
17283:S 06 May 2024 19:18:02.776 * Done loading RDB, keys loaded: 0, keys expired: 0.
17283:S 06 May 2024 19:18:02.776 * MASTER <-> REPLICA sync: Finished with success
17283:S 06 May 2024 19:18:03.090 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is no longer master of shard 7d57fa4efb72d70389b55e5fb06b1ed20ee0b8c3; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.090 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17283:S 06 May 2024 19:18:03.532 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is no longer master of shard d0d012e1f78210fb0613fb1538f24f7814fe81bd; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.532 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17283:S 06 May 2024 19:18:03.540 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is no longer master of shard 4a54cdb367579758a4e3f891240c194e22265b3d; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.540 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17283:S 06 May 2024 19:18:03.542 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is no longer master of shard ab9f767e76ee34d7511533a455ad90ed1b50d27b; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.542 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17283:S 06 May 2024 19:18:03.546 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is no longer master of shard dfcb8ddc797b36f3027193f68a243825e66ecb76; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.546 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17283:S 06 May 2024 19:18:03.579 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is no longer master of shard cb9315006ffe129274468e5ed2feb38a7f33269f; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.579 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17283:S 06 May 2024 19:18:03.581 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is no longer master of shard 75182d21b15b8439f1522e589f314577961349dd; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.581 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17283:S 06 May 2024 19:18:03.584 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is no longer master of shard 66a60939691fd1e6ee1492ac1dbc0dece4a724e9; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.584 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17283:S 06 May 2024 19:18:03.585 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is no longer master of shard 71ddfeaa46e5002962c1f850ecca2bc4f0152083; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.585 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17283:S 06 May 2024 19:18:03.586 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is no longer master of shard 1f9cf276999006c5e2fcd79b4eb3d5840c05a58b; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.586 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17283:S 06 May 2024 19:18:03.588 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is no longer master of shard 30c8fb7d5fcde244056aac469ab0a18d2dfa4e6a; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.588 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17283:S 06 May 2024 19:18:03.588 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is no longer master of shard fe0f6b7f23d33e5bedb35ba3947a32d59a5d868f; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:03.588 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17283:S 06 May 2024 19:18:03.800 * Cluster state changed: ok
17283:S 06 May 2024 19:18:04.101 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is no longer master of shard dae9d79e83baa0dbce3720b336c773fde1c02492; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:04.101 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17283:S 06 May 2024 19:18:04.504 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is no longer master of shard 07caa91caff12c6fc179408189c0b2325234614a; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:04.504 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17283:S 06 May 2024 19:18:07.066 * Connection with master lost.
17283:S 06 May 2024 19:18:07.066 * Caching the disconnected master state.
17283:S 06 May 2024 19:18:07.066 * Reconnecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:07.066 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:07.066 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:07.725 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:07.725 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:07.725 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:08.734 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:08.734 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:08.734 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:09.744 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:09.744 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:09.744 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:10.753 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:10.753 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:10.753 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:11.039 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about 450684f318abb9002cec036680f3cb52f5f919b0 ()
17283:S 06 May 2024 19:18:11.039 # Cluster state changed: fail
17283:S 06 May 2024 19:18:11.055 * Start of election delayed for 857 milliseconds (rank #0, offset 1150).
17283:S 06 May 2024 19:18:11.760 * Connecting to MASTER 127.0.0.1:30000
17283:S 06 May 2024 19:18:11.760 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:11.761 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:11.771 * Configuration change detected. Reconfiguring myself as a replica of 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ()
17283:S 06 May 2024 19:18:11.771 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:11.772 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:11.772 * Cluster state changed: ok
17283:S 06 May 2024 19:18:11.777 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:11.777 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:11.777 * Trying a partial resynchronization (request 8fa9911ae05d0a1fbbaa8d27c7cfba10046179f9:1151).
17283:S 06 May 2024 19:18:11.778 * Successful partial resynchronization with master.
17283:S 06 May 2024 19:18:11.778 * Master replication ID changed to f4193ba52f301b9158bc3de64e6b1033a780a819
17283:S 06 May 2024 19:18:11.778 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17283:S 06 May 2024 19:18:12.839 * Connection with master lost.
17283:S 06 May 2024 19:18:12.839 * Caching the disconnected master state.
17283:S 06 May 2024 19:18:12.839 * Reconnecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:12.839 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:12.839 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:13.778 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:13.779 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:13.779 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:14.790 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:14.790 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:14.790 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:15.802 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:15.802 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:15.802 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:16.544 * FAIL message received from 3a8410fdf29d80b74cbb245767c5e44636bc3277 () about 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ()
17283:S 06 May 2024 19:18:16.544 # Cluster state changed: fail
17283:S 06 May 2024 19:18:16.611 * Starting a failover election for epoch 23.
17283:S 06 May 2024 19:18:16.813 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:16.813 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:16.814 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:17.824 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:17.825 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:17.825 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:18.835 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:18.835 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:18.835 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:19.844 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:19.844 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:19.844 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:20.853 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:20.853 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:20.853 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:21.862 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:21.862 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:21.862 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:22.872 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:22.872 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:22.873 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:23.883 * Connecting to MASTER 127.0.0.1:30015
17283:S 06 May 2024 19:18:23.883 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:23.883 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:23.983 * Start of election delayed for 698 milliseconds (rank #0, offset 1187).
17283:S 06 May 2024 19:18:24.589 * Currently unable to failover: Waiting the delay before I can start a new failover.
17283:S 06 May 2024 19:18:24.692 * Starting a failover election for epoch 25.
17283:S 06 May 2024 19:18:24.726 * Configuration change detected. Reconfiguring myself as a replica of b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17283:S 06 May 2024 19:18:24.726 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:24.727 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:24.728 * Cluster state changed: ok
17283:S 06 May 2024 19:18:24.731 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:24.731 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:24.733 * Trying a partial resynchronization (request f4193ba52f301b9158bc3de64e6b1033a780a819:1188).
17283:S 06 May 2024 19:18:24.734 * Successful partial resynchronization with master.
17283:S 06 May 2024 19:18:24.734 * Master replication ID changed to 451ef21071b0f1b37b526687c34c3e3dfb766090
17283:S 06 May 2024 19:18:24.734 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17283:S 06 May 2024 19:18:25.787 * Connection with master lost.
17283:S 06 May 2024 19:18:25.787 * Caching the disconnected master state.
17283:S 06 May 2024 19:18:25.787 * Reconnecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:25.787 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:25.787 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:25.902 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:25.903 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:25.903 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:26.915 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:26.915 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:26.916 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:27.926 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:27.926 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:27.926 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:28.937 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:28.937 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:28.937 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:29.947 * Connecting to MASTER 127.0.0.1:30003
17283:S 06 May 2024 19:18:29.947 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:29.947 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:30.543 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17283:S 06 May 2024 19:18:30.543 # Cluster state changed: fail
17283:S 06 May 2024 19:18:30.611 * Configuration change detected. Reconfiguring myself as a replica of 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17283:S 06 May 2024 19:18:30.611 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:30.612 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:30.612 * Cluster state changed: ok
17283:S 06 May 2024 19:18:30.616 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:30.616 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:30.616 * Trying a partial resynchronization (request 451ef21071b0f1b37b526687c34c3e3dfb766090:1225).
17283:S 06 May 2024 19:18:30.617 * Successful partial resynchronization with master.
17283:S 06 May 2024 19:18:30.617 * Master replication ID changed to 5b2f1e898e0091b20e1daf794a4836868aeed39e
17283:S 06 May 2024 19:18:30.617 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17283:S 06 May 2024 19:18:31.067 * Connection with master lost.
17283:S 06 May 2024 19:18:31.067 * Caching the disconnected master state.
17283:S 06 May 2024 19:18:31.067 * Reconnecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:31.068 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:31.068 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:31.979 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:31.979 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:31.979 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:32.990 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:32.990 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:32.990 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:34.001 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:34.001 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:34.001 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:35.013 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:35.014 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:35.014 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:35.528 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17283:S 06 May 2024 19:18:35.528 # Cluster state changed: fail
17283:S 06 May 2024 19:18:36.039 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:36.039 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:36.040 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:36.747 * Start of election delayed for 812 milliseconds (rank #0, offset 1261).
17283:S 06 May 2024 19:18:37.051 * Connecting to MASTER 127.0.0.1:30012
17283:S 06 May 2024 19:18:37.051 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:37.051 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:37.629 * Configuration change detected. Reconfiguring myself as a replica of ee4e8801b63592ad76065cf689613b8590e8adbd ()
17283:S 06 May 2024 19:18:37.629 * Connecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:37.629 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:37.630 * Cluster state changed: ok
17283:S 06 May 2024 19:18:37.633 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:37.633 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:37.633 * Trying a partial resynchronization (request 5b2f1e898e0091b20e1daf794a4836868aeed39e:1262).
17283:S 06 May 2024 19:18:37.634 * Successful partial resynchronization with master.
17283:S 06 May 2024 19:18:37.634 * Master replication ID changed to bf02ab8253e6525e24e77c84857f67b67703f9ae
17283:S 06 May 2024 19:18:37.634 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17283:S 06 May 2024 19:18:38.122 * Connection with master lost.
17283:S 06 May 2024 19:18:38.122 * Caching the disconnected master state.
17283:S 06 May 2024 19:18:38.122 * Reconnecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:38.122 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:38.122 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:39.077 * Connecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:39.077 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:39.078 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:40.088 * Connecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:40.089 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:40.089 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:41.102 * Connecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:41.102 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:41.102 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:42.076 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about ee4e8801b63592ad76065cf689613b8590e8adbd ()
17283:S 06 May 2024 19:18:42.076 # Cluster state changed: fail
17283:S 06 May 2024 19:18:42.116 * Connecting to MASTER 127.0.0.1:30009
17283:S 06 May 2024 19:18:42.116 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:42.116 * Starting a failover election for epoch 28.
17283:S 06 May 2024 19:18:42.122 # Error condition on socket for SYNC: Connection refused
17283:S 06 May 2024 19:18:42.126 * Failover election won: I'm the new master.
17283:S 06 May 2024 19:18:42.126 * configEpoch set to 28 after successful failover
17283:M 06 May 2024 19:18:42.126 * Discarding previously cached master state.
17283:M 06 May 2024 19:18:42.127 * Setting secondary replication ID to bf02ab8253e6525e24e77c84857f67b67703f9ae, valid up to offset: 1299. New replication ID is cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e
17283:M 06 May 2024 19:18:42.127 * Cluster state changed: ok
17283:M 06 May 2024 19:18:42.233 * Replica 127.0.0.1:30000 asks for synchronization
17283:M 06 May 2024 19:18:42.233 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '03cd5940b2b29b34b5603d4a44abf064e3a4fdd2', my replication IDs are 'cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e' and 'bf02ab8253e6525e24e77c84857f67b67703f9ae')
17283:M 06 May 2024 19:18:42.233 * Starting BGSAVE for SYNC with target: replicas sockets
17283:M 06 May 2024 19:18:42.234 * Background RDB transfer started by pid 17809
17809:C 06 May 2024 19:18:42.236 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17283:M 06 May 2024 19:18:42.236 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17283:M 06 May 2024 19:18:42.238 * Background RDB transfer terminated with success
17283:M 06 May 2024 19:18:42.238 * Streamed RDB transfer with replica 127.0.0.1:30000 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
17283:M 06 May 2024 19:18:42.238 * Synchronization with replica 127.0.0.1:30000 succeeded
17283:M 06 May 2024 19:18:42.309 * Replica 127.0.0.1:30003 asks for synchronization
17283:M 06 May 2024 19:18:42.309 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '7ac459a89703193e40075d57d322e4efc19147ba', my replication IDs are 'cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e' and 'bf02ab8253e6525e24e77c84857f67b67703f9ae')
17283:M 06 May 2024 19:18:42.309 * Starting BGSAVE for SYNC with target: replicas sockets
17283:M 06 May 2024 19:18:42.310 * Background RDB transfer started by pid 17816
17816:C 06 May 2024 19:18:42.312 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17283:M 06 May 2024 19:18:42.312 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17283:M 06 May 2024 19:18:42.314 * Background RDB transfer terminated with success
17283:M 06 May 2024 19:18:42.314 * Streamed RDB transfer with replica 127.0.0.1:30003 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
17283:M 06 May 2024 19:18:42.314 * Synchronization with replica 127.0.0.1:30003 succeeded
17283:M 06 May 2024 19:18:42.319 * Clear FAIL state for node b4c96fe54bd211d5373161c89d6e0f09e6bae07a ():master without slots is reachable again.
17283:M 06 May 2024 19:18:42.319 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17283:M 06 May 2024 19:18:42.322 * Clear FAIL state for node 450684f318abb9002cec036680f3cb52f5f919b0 ():master without slots is reachable again.
17283:M 06 May 2024 19:18:42.322 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 450684f318abb9002cec036680f3cb52f5f919b0 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17283:M 06 May 2024 19:18:42.368 * Replica 127.0.0.1:30009 asks for synchronization
17283:M 06 May 2024 19:18:42.369 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'fa8cd5db3c2709e8a693584efa1d4e8ce236a8b3', my replication IDs are 'cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e' and 'bf02ab8253e6525e24e77c84857f67b67703f9ae')
17283:M 06 May 2024 19:18:42.369 * Starting BGSAVE for SYNC with target: replicas sockets
17283:M 06 May 2024 19:18:42.369 * Background RDB transfer started by pid 17823
17823:C 06 May 2024 19:18:42.373 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17283:M 06 May 2024 19:18:42.373 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17283:M 06 May 2024 19:18:42.374 * Background RDB transfer terminated with success
17283:M 06 May 2024 19:18:42.374 * Streamed RDB transfer with replica 127.0.0.1:30009 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
17283:M 06 May 2024 19:18:42.374 * Synchronization with replica 127.0.0.1:30009 succeeded
17283:M 06 May 2024 19:18:42.420 * Clear FAIL state for node ee4e8801b63592ad76065cf689613b8590e8adbd ():master without slots is reachable again.
17283:M 06 May 2024 19:18:42.420 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node ee4e8801b63592ad76065cf689613b8590e8adbd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17283:M 06 May 2024 19:18:42.441 * Replica 127.0.0.1:30012 asks for synchronization
17283:M 06 May 2024 19:18:42.441 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '9dcfcc52808ca5d32112fd7cc2729696ca1e3041', my replication IDs are 'cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e' and 'bf02ab8253e6525e24e77c84857f67b67703f9ae')
17283:M 06 May 2024 19:18:42.441 * Starting BGSAVE for SYNC with target: replicas sockets
17283:M 06 May 2024 19:18:42.441 * Background RDB transfer started by pid 17830
17830:C 06 May 2024 19:18:42.443 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17283:M 06 May 2024 19:18:42.443 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17283:M 06 May 2024 19:18:42.445 * Background RDB transfer terminated with success
17283:M 06 May 2024 19:18:42.445 * Streamed RDB transfer with replica 127.0.0.1:30012 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
17283:M 06 May 2024 19:18:42.445 * Synchronization with replica 127.0.0.1:30012 succeeded
17283:M 06 May 2024 19:18:42.508 * Replica 127.0.0.1:30015 asks for synchronization
17283:M 06 May 2024 19:18:42.508 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '6e4e1d407fc0698cfce67b5f481210c124ba3693', my replication IDs are 'cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e' and 'bf02ab8253e6525e24e77c84857f67b67703f9ae')
17283:M 06 May 2024 19:18:42.508 * Starting BGSAVE for SYNC with target: replicas sockets
17283:M 06 May 2024 19:18:42.509 * Background RDB transfer started by pid 17837
17837:C 06 May 2024 19:18:42.512 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17283:M 06 May 2024 19:18:42.512 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17283:M 06 May 2024 19:18:42.513 * Background RDB transfer terminated with success
17283:M 06 May 2024 19:18:42.513 * Streamed RDB transfer with replica 127.0.0.1:30015 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
17283:M 06 May 2024 19:18:42.513 * Synchronization with replica 127.0.0.1:30015 succeeded
17283:M 06 May 2024 19:18:42.521 * Clear FAIL state for node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ():master without slots is reachable again.
17283:M 06 May 2024 19:18:42.521 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17283:M 06 May 2024 19:18:42.524 * Clear FAIL state for node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ():master without slots is reachable again.
17283:M 06 May 2024 19:18:42.524 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17283:M 06 May 2024 19:18:42.549 * Connection with replica 127.0.0.1:30000 lost.
17283:M 06 May 2024 19:18:42.577 * Connection with replica 127.0.0.1:30003 lost.
17283:M 06 May 2024 19:18:42.629 * configEpoch set to 0 via CLUSTER RESET HARD
17283:M 06 May 2024 19:18:42.629 * Node hard reset, now I'm bcfc65dd66e5ac03397a54470d6555c63eea6616
17283:M 06 May 2024 19:18:42.629 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
17283:M 06 May 2024 19:18:42.629 # Cluster state changed: fail
17283:M 06 May 2024 19:18:42.634 * CONFIG REWRITE executed with success.
17283:M 06 May 2024 19:18:42.649 * Connection with replica 127.0.0.1:30009 lost.
17283:M 06 May 2024 19:18:42.702 * Connection with replica 127.0.0.1:30012 lost.
17283:M 06 May 2024 19:18:42.728 * Connection with replica 127.0.0.1:30015 lost.
17283:S 06 May 2024 19:18:45.313 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17283:S 06 May 2024 19:18:45.313 * Connecting to MASTER 127.0.0.1:30001
17283:S 06 May 2024 19:18:45.313 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:18:45.313 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:18:45.313 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:18:45.314 * Trying a partial resynchronization (request cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e:1340).
17283:S 06 May 2024 19:18:45.314 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:1651
17283:S 06 May 2024 19:18:45.316 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17283:S 06 May 2024 19:18:45.316 * Discarding previously cached master state.
17283:S 06 May 2024 19:18:45.316 * MASTER <-> REPLICA sync: Flushing old data
17283:S 06 May 2024 19:18:45.317 * MASTER <-> REPLICA sync: Loading DB in memory
17283:S 06 May 2024 19:18:45.318 * Loading RDB produced by valkey version 255.255.255
17283:S 06 May 2024 19:18:45.318 * RDB age 0 seconds
17283:S 06 May 2024 19:18:45.318 * RDB memory usage when created 2.76 Mb
17283:S 06 May 2024 19:18:45.318 * Done loading RDB, keys loaded: 0, keys expired: 0.
17283:S 06 May 2024 19:18:45.318 * MASTER <-> REPLICA sync: Finished with success
17283:S 06 May 2024 19:18:45.548 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is no longer master of shard 8dd948e148227a2c888397f600e5d52a2509a683; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:45.548 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is now part of shard 769aeb08b175de4efdfee71d709927c7f9afc5bf
17283:S 06 May 2024 19:18:45.550 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is no longer master of shard 7eb196d163c5bf7eb03371da41c37de6ab3a27c7; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:45.550 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is now part of shard d05b2d9c8fad619a6a58ab01b0e5ade057de5ca1
17283:S 06 May 2024 19:18:46.077 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is no longer master of shard 91245c2cd07d4494975195dd1dfe064ec2d4cbfe; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:46.077 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is now part of shard 5c56bf8f5102b508a2405dedd16899f245264a91
17283:S 06 May 2024 19:18:46.203 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is no longer master of shard 35470f3f7014665726088479ee7b0dd699dc35de; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:18:46.203 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is now part of shard 482cb579f47fb8ccd86d73983fba9c22f7197882
17283:S 06 May 2024 19:18:46.332 * Cluster state changed: ok
17283:S 06 May 2024 19:18:52.511 * FAIL message received from 6e0ef4878fe1c8835271b94d91093f69119e2c5f () about 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ()
17283:S 06 May 2024 19:19:08.686 * Clear FAIL state for node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ():replica is reachable again.
17283:S 06 May 2024 19:19:12.531 * FAIL message received from e4844e444bd7db045069d93fe2f71d3259852adf () about 03a61a2c512c0aab55d8de4c03711ba741fddb86 ()
17283:S 06 May 2024 19:19:12.531 # Cluster state changed: fail
17283:M 06 May 2024 19:19:18.965 * Connection with master lost.
17283:M 06 May 2024 19:19:18.965 * Caching the disconnected master state.
17283:M 06 May 2024 19:19:18.965 * Discarding previously cached master state.
17283:M 06 May 2024 19:19:18.965 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 1735. New replication ID is 3ef6aa0eec14563c4e12d52f96e3320211382188
17283:M 06 May 2024 19:19:18.967 * configEpoch set to 0 via CLUSTER RESET HARD
17283:M 06 May 2024 19:19:18.967 * Node hard reset, now I'm 9e613cb2c4712a0062c1b26d3da1ea7442fbf737
17283:M 06 May 2024 19:19:18.967 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
17283:M 06 May 2024 19:19:18.970 * CONFIG REWRITE executed with success.
17283:S 06 May 2024 19:19:21.911 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17283:S 06 May 2024 19:19:21.911 * Connecting to MASTER 127.0.0.1:30001
17283:S 06 May 2024 19:19:21.912 * MASTER <-> REPLICA sync started
17283:S 06 May 2024 19:19:21.912 * Non blocking connect for SYNC fired the event.
17283:S 06 May 2024 19:19:21.912 * Master replied to PING, replication can continue...
17283:S 06 May 2024 19:19:21.912 * Trying a partial resynchronization (request 3ef6aa0eec14563c4e12d52f96e3320211382188:1735).
17283:S 06 May 2024 19:19:21.913 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:1734
17283:S 06 May 2024 19:19:21.915 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17283:S 06 May 2024 19:19:21.915 * Discarding previously cached master state.
17283:S 06 May 2024 19:19:21.915 * MASTER <-> REPLICA sync: Flushing old data
17283:S 06 May 2024 19:19:21.916 * MASTER <-> REPLICA sync: Loading DB in memory
17283:S 06 May 2024 19:19:21.917 * Loading RDB produced by valkey version 255.255.255
17283:S 06 May 2024 19:19:21.917 * RDB age 0 seconds
17283:S 06 May 2024 19:19:21.917 * RDB memory usage when created 2.76 Mb
17283:S 06 May 2024 19:19:21.917 * Done loading RDB, keys loaded: 0, keys expired: 0.
17283:S 06 May 2024 19:19:21.918 * MASTER <-> REPLICA sync: Finished with success
17283:S 06 May 2024 19:19:22.245 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is no longer master of shard 1c166555732b739d6e4c1ab4dd8d165e832b3954; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:22.245 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17283:S 06 May 2024 19:19:22.497 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is no longer master of shard 614a1b42f4baa25c7f6d5a11b30ac1fdea247594; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:22.497 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17283:S 06 May 2024 19:19:22.599 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is no longer master of shard 5d8efc15227cc965bd521169b3cd81eb9295ec19; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:22.599 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17283:S 06 May 2024 19:19:22.599 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is no longer master of shard ab8e805388a16f08bc5fa7bfd304cf9b2f43c122; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:22.599 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17283:S 06 May 2024 19:19:22.600 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is no longer master of shard d8860f9f28d646383a01d2f3aba63d0f3a3e4faf; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:22.600 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17283:S 06 May 2024 19:19:23.105 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is no longer master of shard 3e228277d31da320b8d2c0ab0d21aa067371dad3; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:23.105 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17283:S 06 May 2024 19:19:23.447 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is no longer master of shard 01f47c3a12aa4f053a807eeb5eb545c5e07db794; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:23.447 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17283:S 06 May 2024 19:19:23.504 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is no longer master of shard b3f6dd9ab5620f4d8bda693091f3d8d36ad0e7d3; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:23.504 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17283:S 06 May 2024 19:19:23.508 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is no longer master of shard aae63656731167d9c699175203047b6e26efb4be; removed all 0 slot(s) it used to own
17283:S 06 May 2024 19:19:23.508 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17283:S 06 May 2024 19:19:23.513 * Cluster state changed: ok
17283:signal-handler (1715023165) Received SIGTERM scheduling shutdown...
17283:S 06 May 2024 19:19:25.619 * User requested shutdown...
17283:S 06 May 2024 19:19:25.619 # Valkey is now ready to exit, bye bye...
18077:C 06 May 2024 19:19:35.272 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
18077:C 06 May 2024 19:19:35.272 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
18077:C 06 May 2024 19:19:35.272 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=18077, just started
18077:C 06 May 2024 19:19:35.272 * Configuration loaded
18077:M 06 May 2024 19:19:35.273 * monotonic clock: POSIX clock_gettime
18077:M 06 May 2024 19:19:35.274 * Running mode=cluster, port=30006.
18077:M 06 May 2024 19:19:35.281 * Node configuration loaded, I'm 9e613cb2c4712a0062c1b26d3da1ea7442fbf737
18077:M 06 May 2024 19:19:35.282 * Server initialized
18077:M 06 May 2024 19:19:35.282 * Loading RDB produced by valkey version 255.255.255
18077:M 06 May 2024 19:19:35.282 * RDB age 14 seconds
18077:M 06 May 2024 19:19:35.282 * RDB memory usage when created 2.76 Mb
18077:M 06 May 2024 19:19:35.282 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:M 06 May 2024 19:19:35.282 * DB loaded from disk: 0.000 seconds
18077:M 06 May 2024 19:19:35.283 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:M 06 May 2024 19:19:35.283 * Ready to accept connections tcp
18077:S 06 May 2024 19:19:35.284 * Discarding previously cached master state.
18077:S 06 May 2024 19:19:35.284 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:19:35.284 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:19:35.284 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:19:35.284 * Cluster state changed: ok
18077:S 06 May 2024 19:19:35.285 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:19:35.288 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:19:35.291 * Trying a partial resynchronization (request 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:1735).
18077:S 06 May 2024 19:19:35.291 * Successful partial resynchronization with master.
18077:S 06 May 2024 19:19:35.291 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
18077:M 06 May 2024 19:19:35.533 * Connection with master lost.
18077:M 06 May 2024 19:19:35.533 * Caching the disconnected master state.
18077:M 06 May 2024 19:19:35.533 * Discarding previously cached master state.
18077:M 06 May 2024 19:19:35.533 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 1790. New replication ID is b6e30e7b7beb5d5fbb2078eafffed3169e6bcf31
18077:M 06 May 2024 19:19:35.535 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:19:35.535 * Node hard reset, now I'm 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21
18077:M 06 May 2024 19:19:35.535 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:19:35.535 # Cluster state changed: fail
18077:M 06 May 2024 19:19:35.540 * CONFIG REWRITE executed with success.
18077:M 06 May 2024 19:19:38.907 # Missing implement of connection type tls
18077:S 06 May 2024 19:19:41.454 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:19:41.455 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:19:41.455 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:19:41.455 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:19:41.455 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:19:41.455 * Trying a partial resynchronization (request b6e30e7b7beb5d5fbb2078eafffed3169e6bcf31:1790).
18077:S 06 May 2024 19:19:41.456 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:1789
18077:S 06 May 2024 19:19:41.457 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:19:41.457 * Discarding previously cached master state.
18077:S 06 May 2024 19:19:41.457 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:19:41.457 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:19:41.469 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:19:41.469 * RDB age 0 seconds
18077:S 06 May 2024 19:19:41.469 * RDB memory usage when created 2.76 Mb
18077:S 06 May 2024 19:19:41.469 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:19:41.469 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:19:41.563 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is no longer master of shard c249ff0fc7be1576faf8724b7784974260ba49cf; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:41.563 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
18077:S 06 May 2024 19:19:41.564 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is no longer master of shard 2cb31efff9b31077914e5d8eee67fad987f900ec; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:41.564 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
18077:S 06 May 2024 19:19:42.688 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is no longer master of shard be264f012de4ca3b640590c83301730d9edc2c7c; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.688 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
18077:S 06 May 2024 19:19:42.696 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is no longer master of shard b24cc857a20e418c76000cf7bb7c2121d56535c8; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.697 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
18077:S 06 May 2024 19:19:42.697 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is no longer master of shard a85808b5f3b195646b5e9c75d437940e5910c539; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.697 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
18077:S 06 May 2024 19:19:42.697 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is no longer master of shard 5a93a431ee3c1974750eaeb45212552089e1d70f; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.697 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
18077:S 06 May 2024 19:19:42.697 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is no longer master of shard 1faef2c01b90ce80c9287617b3462fcd0eb7c5f5; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.697 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
18077:S 06 May 2024 19:19:42.703 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is no longer master of shard 5a313ed6e1afc160d88e2ec76f16f8322d6a6195; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:42.703 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
18077:S 06 May 2024 19:19:42.708 * Cluster state changed: ok
18077:S 06 May 2024 19:19:43.893 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is no longer master of shard c707b138b75efd32e96dd0d45165ae682cc6e5bb; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:43.893 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
18077:S 06 May 2024 19:19:49.142 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ()
18077:S 06 May 2024 19:19:49.142 # Cluster state changed: fail
18077:S 06 May 2024 19:19:49.187 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about b6d1f331303ea88d453961a6bc247c12cf3f1746 ()
18077:S 06 May 2024 19:19:50.285 * Cluster state changed: ok
18077:S 06 May 2024 19:19:55.476 * Clear FAIL state for node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ():master without slots is reachable again.
18077:S 06 May 2024 19:19:55.476 * A failover occurred in shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6; node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 () lost 0 slot(s) to node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () with a config epoch of 21
18077:S 06 May 2024 19:19:55.577 * Clear FAIL state for node b6d1f331303ea88d453961a6bc247c12cf3f1746 ():replica is reachable again.
18077:M 06 May 2024 19:19:55.627 * Connection with master lost.
18077:M 06 May 2024 19:19:55.627 * Caching the disconnected master state.
18077:M 06 May 2024 19:19:55.627 * Discarding previously cached master state.
18077:M 06 May 2024 19:19:55.627 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 3067. New replication ID is 8bd0e5bd12bc59db04c38dd3976f5c58885df8f0
18077:M 06 May 2024 19:19:55.629 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:19:55.629 * Node hard reset, now I'm 334beb692e0626a4ea1257acac41aaf9962330a7
18077:M 06 May 2024 19:19:55.629 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:19:55.629 # Cluster state changed: fail
18077:M 06 May 2024 19:19:55.633 * CONFIG REWRITE executed with success.
18077:S 06 May 2024 19:19:58.951 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:19:58.951 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:19:58.952 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:19:58.953 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:19:58.953 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:19:58.953 * Trying a partial resynchronization (request 8bd0e5bd12bc59db04c38dd3976f5c58885df8f0:3067).
18077:S 06 May 2024 19:19:58.954 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:3066
18077:S 06 May 2024 19:19:58.955 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:19:58.955 * Discarding previously cached master state.
18077:S 06 May 2024 19:19:58.955 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:19:58.956 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:19:58.958 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:19:58.958 * RDB age 0 seconds
18077:S 06 May 2024 19:19:58.959 * RDB memory usage when created 2.78 Mb
18077:S 06 May 2024 19:19:58.959 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:19:58.959 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:19:59.513 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is no longer master of shard 8980e70c0b02f85ca396ef3dc659e0dfe2f2edd4; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:59.513 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is now part of shard 463631d5c660f73be6acc894f4e68e2bc6bf1f04
18077:S 06 May 2024 19:19:59.536 * Node caf83072d9c11f919729af96326335b051c7af49 () is no longer master of shard 6dd4af3291b64805920b88cdb4d7d83d44852f08; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:59.537 * Node caf83072d9c11f919729af96326335b051c7af49 () is now part of shard ada3b4025b51be85bbb1fa1df0e1b44871badaa9
18077:S 06 May 2024 19:19:59.541 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is no longer master of shard 78683b2cc868ee4a3e989125c612bea0bc5c90c0; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:59.541 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is now part of shard b30cbe386d5d2fec228eb45db97ba04991f5020e
18077:S 06 May 2024 19:19:59.917 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is no longer master of shard 5679c354a9e2cf8cb62b26598f6d9e6a0f79b90d; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:19:59.917 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is now part of shard 5a15e11d32c6794f2f89faac54fc339efde5e362
18077:S 06 May 2024 19:20:00.521 * Cluster state changed: ok
18077:S 06 May 2024 19:20:07.038 * FAIL message received from ee4a5ef490389f31920942453b9bd1fe2357cbb2 () about 87c9e3fe2e97200a971dc5aee1822009f9498072 ()
18077:S 06 May 2024 19:20:07.038 # Cluster state changed: fail
18077:S 06 May 2024 19:20:07.921 * Cluster state changed: ok
18077:S 06 May 2024 19:20:12.095 * FAIL message received from a470ecfb84686f1c17778838dfcbeb2ec78c9395 () about ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 ()
18077:S 06 May 2024 19:20:12.095 # Cluster state changed: fail
18077:S 06 May 2024 19:20:12.204 * Clear FAIL state for node 87c9e3fe2e97200a971dc5aee1822009f9498072 ():master without slots is reachable again.
18077:S 06 May 2024 19:20:12.204 * A failover occurred in shard 5a15e11d32c6794f2f89faac54fc339efde5e362; node 87c9e3fe2e97200a971dc5aee1822009f9498072 () lost 0 slot(s) to node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () with a config epoch of 21
18077:S 06 May 2024 19:20:18.243 * Clear FAIL state for node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 (): is reachable again and nobody is serving its slots after some time.
18077:S 06 May 2024 19:20:18.243 * Cluster state changed: ok
18077:M 06 May 2024 19:20:20.633 * Connection with master lost.
18077:M 06 May 2024 19:20:20.633 * Caching the disconnected master state.
18077:M 06 May 2024 19:20:20.633 * Discarding previously cached master state.
18077:M 06 May 2024 19:20:20.633 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 5764. New replication ID is 6cabb1c888198d7d1cef6f6d99a52e19458ba182
18077:M 06 May 2024 19:20:20.635 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:20:20.635 * Node hard reset, now I'm f2264c981ff99322862f3de220948842b1f6ba1f
18077:M 06 May 2024 19:20:20.635 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:20:20.635 # Cluster state changed: fail
18077:M 06 May 2024 19:20:20.640 * CONFIG REWRITE executed with success.
18077:S 06 May 2024 19:20:24.991 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:20:24.991 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:20:24.992 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:20:24.992 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:20:24.992 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:20:24.993 * Trying a partial resynchronization (request 6cabb1c888198d7d1cef6f6d99a52e19458ba182:5764).
18077:S 06 May 2024 19:20:24.993 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:5763
18077:S 06 May 2024 19:20:24.995 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:20:24.995 * Discarding previously cached master state.
18077:S 06 May 2024 19:20:24.995 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:20:24.995 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:20:24.996 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:20:24.996 * RDB age 0 seconds
18077:S 06 May 2024 19:20:24.996 * RDB memory usage when created 2.78 Mb
18077:S 06 May 2024 19:20:24.997 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:20:24.997 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:20:25.599 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is no longer master of shard 5736e9db4e80aceb59df82a4860201928e6e2dfe; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:25.599 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is now part of shard 19776fe3f0224c8eb88fdc51c9073d888852d019
18077:S 06 May 2024 19:20:26.102 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is no longer master of shard c5c35fe41b55a11da192d43a8da61be171aa83e6; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:26.102 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is now part of shard e04c9009be37f9eec889c59340dc040d0fa0aeb8
18077:S 06 May 2024 19:20:26.204 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is no longer master of shard e8e4bf95f861f348154c047f60ca28434502f73e; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:26.204 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is now part of shard a0b63e81b62eaf4c5711335c6f1846909bf7fd69
18077:S 06 May 2024 19:20:26.304 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is no longer master of shard ad7c5024c1a4d2a21dcf6f4a1d9b1f45ff1ead2d; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:26.304 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is now part of shard 67e314dae915c111e42745e09df1e650a83178cf
18077:S 06 May 2024 19:20:26.506 * Cluster state changed: ok
18077:M 06 May 2024 19:20:28.667 * Connection with master lost.
18077:M 06 May 2024 19:20:28.667 * Caching the disconnected master state.
18077:M 06 May 2024 19:20:28.667 * Discarding previously cached master state.
18077:M 06 May 2024 19:20:28.667 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 5805. New replication ID is 428180ebdd560d74fbcc6ebf7c50dafcaa3f19a7
18077:M 06 May 2024 19:20:28.668 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:20:28.668 * Node hard reset, now I'm 3500eeb0c27fbcd680bc59f0bd19aca81ac51372
18077:M 06 May 2024 19:20:28.668 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:20:28.669 # Cluster state changed: fail
18077:M 06 May 2024 19:20:28.673 * CONFIG REWRITE executed with success.
18077:S 06 May 2024 19:20:31.996 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:20:31.996 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:20:31.996 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:20:31.997 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:20:31.998 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:20:31.999 * Trying a partial resynchronization (request 428180ebdd560d74fbcc6ebf7c50dafcaa3f19a7:5805).
18077:S 06 May 2024 19:20:31.999 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:5804
18077:S 06 May 2024 19:20:32.001 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:20:32.001 * Discarding previously cached master state.
18077:S 06 May 2024 19:20:32.001 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:20:32.002 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:20:32.003 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:20:32.003 * RDB age 1 seconds
18077:S 06 May 2024 19:20:32.003 * RDB memory usage when created 2.83 Mb
18077:S 06 May 2024 19:20:32.003 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:20:32.003 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:20:32.047 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is no longer master of shard c0ff426f83007655b38a9ba2767c988c6843ca09; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:32.047 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is now part of shard c149d2cb057be6b4c53e6cda4f8361117987c75b
18077:S 06 May 2024 19:20:32.149 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is no longer master of shard b2b496b97d756568bb9cc80df81baa9597a2a1e0; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:32.149 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is now part of shard 9628b03c98fa51a28e8367ab2c3e2abc75d63330
18077:S 06 May 2024 19:20:32.502 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is no longer master of shard 10725e1ebc3e8158a7f12329adb9ba56ee854bc1; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:32.502 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is now part of shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba
18077:S 06 May 2024 19:20:32.508 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is no longer master of shard 5c7f555424b6dd7244521040fed352e8a690f783; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:32.508 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is now part of shard 82ad93ba4e7c80d6359231435757456cc87347ed
18077:S 06 May 2024 19:20:33.532 * Cluster state changed: ok
18077:S 06 May 2024 19:20:37.546 * A failover occurred in shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba; node 0e59ecb0630140741e54370064819bde1c9ca23d () lost 0 slot(s) to node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () with a config epoch of 21
18077:M 06 May 2024 19:20:40.501 * Connection with master lost.
18077:M 06 May 2024 19:20:40.501 * Caching the disconnected master state.
18077:M 06 May 2024 19:20:40.501 * Discarding previously cached master state.
18077:M 06 May 2024 19:20:40.501 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 160048. New replication ID is a3f3c71978e13cc8ce6e87373797dbc86cc75c16
18077:M 06 May 2024 19:20:40.502 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:20:40.502 * Node hard reset, now I'm 8053da283ed5221065f944f98f73e25c61a010dd
18077:M 06 May 2024 19:20:40.502 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:20:40.502 # Cluster state changed: fail
18077:M 06 May 2024 19:20:40.506 * CONFIG REWRITE executed with success.
18077:S 06 May 2024 19:20:44.992 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:20:44.992 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:20:44.993 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:20:44.993 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:20:44.993 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:20:44.993 * Trying a partial resynchronization (request a3f3c71978e13cc8ce6e87373797dbc86cc75c16:160048).
18077:S 06 May 2024 19:20:44.994 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:160047
18077:S 06 May 2024 19:20:44.995 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:20:44.995 * Discarding previously cached master state.
18077:S 06 May 2024 19:20:44.995 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:20:44.996 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:20:44.997 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:20:44.997 * RDB age 0 seconds
18077:S 06 May 2024 19:20:44.997 * RDB memory usage when created 3.01 Mb
18077:S 06 May 2024 19:20:44.997 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:20:44.997 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:20:45.043 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is no longer master of shard f9fc60420f4df6321dff2cb65cfd5443410eaf16; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:45.043 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is now part of shard 2f2e8949560f25624e4d18237b260e2412773d08
18077:S 06 May 2024 19:20:45.044 * Node 968e407a2757f6786a27813986652641dfa8c53e () is no longer master of shard c23ee8d4ddce6cd592f3f5d11f12e6f0e7d8357b; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:45.044 * Node 968e407a2757f6786a27813986652641dfa8c53e () is now part of shard 3dd139d2458ce1ba40162b75a210452f2564f13f
18077:S 06 May 2024 19:20:45.510 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is no longer master of shard 5894da5030fb6ea0d4890f9fcaefbcb689467382; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:45.510 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is now part of shard 942404fb5086368ee22ef226fc325d99eead816a
18077:S 06 May 2024 19:20:45.595 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is no longer master of shard ac0a659a65ff86d0acb03beca99488cf675db0b9; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:20:45.595 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is now part of shard 79761404f49765dfc1452b3572ad1557043c4b29
18077:S 06 May 2024 19:20:46.087 * Cluster state changed: ok
18077:S 06 May 2024 19:20:51.741 * FAIL message received from cf6fdb89f625f08d607b33125f0227fce622927f () about 51d1f09230533baaac63ae94738bf70b5db09342 ()
18077:S 06 May 2024 19:20:51.741 # Cluster state changed: fail
18077:S 06 May 2024 19:20:52.694 * Cluster state changed: ok
18077:S 06 May 2024 19:20:58.466 * Clear FAIL state for node 51d1f09230533baaac63ae94738bf70b5db09342 ():master without slots is reachable again.
18077:S 06 May 2024 19:20:58.466 * A failover occurred in shard 942404fb5086368ee22ef226fc325d99eead816a; node 51d1f09230533baaac63ae94738bf70b5db09342 () lost 0 slot(s) to node e0fb6aab67e48702bb3257b35176c6bec33ac399 () with a config epoch of 21
18077:M 06 May 2024 19:20:59.089 * Connection with master lost.
18077:M 06 May 2024 19:20:59.089 * Caching the disconnected master state.
18077:M 06 May 2024 19:20:59.089 * Discarding previously cached master state.
18077:M 06 May 2024 19:20:59.090 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 160997. New replication ID is e56ce2bd14f6e6c413abf0475b59304210c5b837
18077:M 06 May 2024 19:20:59.091 * configEpoch set to 0 via CLUSTER RESET HARD
18077:M 06 May 2024 19:20:59.091 * Node hard reset, now I'm ea188db865fa34fa05775d699ccded36d0a815f0
18077:M 06 May 2024 19:20:59.091 * configEpoch set to 7 via CLUSTER SET-CONFIG-EPOCH
18077:M 06 May 2024 19:20:59.091 # Cluster state changed: fail
18077:M 06 May 2024 19:20:59.096 * CONFIG REWRITE executed with success.
18077:S 06 May 2024 19:21:02.892 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18077:S 06 May 2024 19:21:02.892 * Connecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:21:02.892 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:21:02.893 * Non blocking connect for SYNC fired the event.
18077:S 06 May 2024 19:21:02.893 * Master replied to PING, replication can continue...
18077:S 06 May 2024 19:21:02.894 * Trying a partial resynchronization (request e56ce2bd14f6e6c413abf0475b59304210c5b837:160997).
18077:S 06 May 2024 19:21:02.894 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:160996
18077:S 06 May 2024 19:21:02.896 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18077:S 06 May 2024 19:21:02.896 * Discarding previously cached master state.
18077:S 06 May 2024 19:21:02.896 * MASTER <-> REPLICA sync: Flushing old data
18077:S 06 May 2024 19:21:02.896 * MASTER <-> REPLICA sync: Loading DB in memory
18077:S 06 May 2024 19:21:02.897 * Loading RDB produced by valkey version 255.255.255
18077:S 06 May 2024 19:21:02.897 * RDB age 0 seconds
18077:S 06 May 2024 19:21:02.897 * RDB memory usage when created 3.01 Mb
18077:S 06 May 2024 19:21:02.897 * Done loading RDB, keys loaded: 0, keys expired: 0.
18077:S 06 May 2024 19:21:02.897 * MASTER <-> REPLICA sync: Finished with success
18077:S 06 May 2024 19:21:03.017 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is no longer master of shard c9e0b1451dee8dfc4e893df748587b969a26f679; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:21:03.017 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is now part of shard 75be0656223117757620482d8547e5fe2e896de1
18077:S 06 May 2024 19:21:03.384 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is no longer master of shard 54e8a9afa1bd7f67c860e6b09192d24f3b4b5af0; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:21:03.384 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is now part of shard fdf0c30757c93c36c4d6e2340444130d7ab6fd1c
18077:S 06 May 2024 19:21:03.516 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is no longer master of shard 48c7ccc13115e6584257112f348f460ed6b3d90b; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:21:03.516 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is now part of shard 4897fecbf064c3a5fe304e8cc5bfc121e96f4d0d
18077:S 06 May 2024 19:21:03.548 * Cluster state changed: ok
18077:S 06 May 2024 19:21:04.032 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is no longer master of shard 122cd1fcb92a7460821b86e1e2d20ae156aef07b; removed all 0 slot(s) it used to own
18077:S 06 May 2024 19:21:04.032 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is now part of shard 9c3ef340a0f3031400e88f148755f8e3a44c39cc
18077:S 06 May 2024 19:21:10.565 * FAIL message received from 4816b0fa4e909697f2f79ac7605b852ec0d22927 () about f1ab9aca89352244cecd21bb2bebff73910a1237 ()
18077:signal-handler (1715023274) Received SIGINT scheduling shutdown...
18077:S 06 May 2024 19:21:14.589 * Connection with master lost.
18077:S 06 May 2024 19:21:14.589 * Caching the disconnected master state.
18077:S 06 May 2024 19:21:14.589 * Reconnecting to MASTER 127.0.0.1:30001
18077:S 06 May 2024 19:21:14.590 * MASTER <-> REPLICA sync started
18077:S 06 May 2024 19:21:14.590 # Error condition on socket for SYNC: Connection refused
18077:S 06 May 2024 19:21:14.594 * User requested shutdown...
18077:S 06 May 2024 19:21:14.594 # Valkey is now ready to exit, bye bye...
