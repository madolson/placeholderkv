15699:C 06 May 2024 19:13:44.901 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
15699:C 06 May 2024 19:13:44.901 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15699:C 06 May 2024 19:13:44.901 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15699:C 06 May 2024 19:13:44.901 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15699, just started
15699:C 06 May 2024 19:13:44.901 * Configuration loaded
15699:M 06 May 2024 19:13:44.902 * monotonic clock: POSIX clock_gettime
15699:M 06 May 2024 19:13:44.903 * Running mode=cluster, port=30005.
15699:M 06 May 2024 19:13:44.904 * No cluster configuration found, I'm 81f0fba3efb63d444a1f45bab55256abe2a0293c
15699:M 06 May 2024 19:13:44.908 * Server initialized
15699:M 06 May 2024 19:13:44.910 * Creating AOF base file appendonly.aof.1.base.rdb on server start
15699:M 06 May 2024 19:13:44.912 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
15699:M 06 May 2024 19:13:44.913 * Ready to accept connections tcp
15699:M 06 May 2024 19:13:45.585 * configEpoch set to 0 via CLUSTER RESET HARD
15699:M 06 May 2024 19:13:45.585 * Node hard reset, now I'm 2acfda17d98165a4e542d0ede11918c65327d8be
15699:M 06 May 2024 19:13:45.586 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
15699:M 06 May 2024 19:13:45.590 * CONFIG REWRITE executed with success.
15699:M 06 May 2024 19:13:45.688 * IP address for this node updated to 127.0.0.1
15699:M 06 May 2024 19:13:48.322 # Missing implement of connection type tls
15699:M 06 May 2024 19:13:52.565 * Cluster state changed: ok
15699:M 06 May 2024 19:13:52.810 * configEpoch set to 0 via CLUSTER RESET HARD
15699:M 06 May 2024 19:13:52.810 * Node hard reset, now I'm 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555
15699:M 06 May 2024 19:13:52.810 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
15699:M 06 May 2024 19:13:52.810 # Cluster state changed: fail
15699:M 06 May 2024 19:13:52.814 * CONFIG REWRITE executed with success.
15699:S 06 May 2024 19:13:56.903 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15699:S 06 May 2024 19:13:56.903 * Connecting to MASTER 127.0.0.1:30000
15699:S 06 May 2024 19:13:56.903 * MASTER <-> REPLICA sync started
15699:S 06 May 2024 19:13:56.903 * Non blocking connect for SYNC fired the event.
15699:S 06 May 2024 19:13:56.904 * Master replied to PING, replication can continue...
15699:S 06 May 2024 19:13:56.904 * Trying a partial resynchronization (request caa8fb1213c82777376aafeb04c086a93df09432:3).
15699:S 06 May 2024 19:13:56.904 * Full resync from master: db1149724e100be74f9a41307e155dd07433ad86:24
15699:S 06 May 2024 19:13:56.906 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15699:S 06 May 2024 19:13:56.906 * Discarding previously cached master state.
15699:S 06 May 2024 19:13:56.906 * MASTER <-> REPLICA sync: Flushing old data
15699:S 06 May 2024 19:13:56.906 * MASTER <-> REPLICA sync: Loading DB in memory
15699:S 06 May 2024 19:13:56.907 * Loading RDB produced by valkey version 255.255.255
15699:S 06 May 2024 19:13:56.907 * RDB age 0 seconds
15699:S 06 May 2024 19:13:56.907 * RDB memory usage when created 2.80 Mb
15699:S 06 May 2024 19:13:56.908 * Done loading RDB, keys loaded: 0, keys expired: 0.
15699:S 06 May 2024 19:13:56.908 * MASTER <-> REPLICA sync: Finished with success
15699:S 06 May 2024 19:13:56.908 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15699:S 06 May 2024 19:13:56.908 * Background append only file rewriting started by pid 15852
15852:C 06 May 2024 19:13:56.909 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15852.aof
15852:C 06 May 2024 19:13:56.910 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15699:S 06 May 2024 19:13:56.997 * Background AOF rewrite terminated with success
15699:S 06 May 2024 19:13:56.997 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15852.aof into appendonly.aof.2.base.rdb
15699:S 06 May 2024 19:13:56.997 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.2.incr.aof
15699:S 06 May 2024 19:13:56.999 * Removing the history file appendonly.aof.1.incr.aof in the background
15699:S 06 May 2024 19:13:56.999 * Removing the history file appendonly.aof.1.base.rdb in the background
15699:S 06 May 2024 19:13:57.001 * Background AOF rewrite finished successfully
15699:S 06 May 2024 19:13:57.103 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is no longer master of shard 45a425a508f35dfb3598266dbd995af0daf15e78; removed all 0 slot(s) it used to own
15699:S 06 May 2024 19:13:57.103 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is now part of shard e89af52affaeadfe3f4c492254ae30531e68445a
15699:S 06 May 2024 19:13:57.508 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is no longer master of shard 2aaea61241128999e7bf4d990a5d6bae89096d2d; removed all 0 slot(s) it used to own
15699:S 06 May 2024 19:13:57.508 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is now part of shard 9009f53e5fcc48bb5a0b06d0bc06b2a661c7165d
15699:S 06 May 2024 19:13:57.509 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is no longer master of shard 67426f1e38c225d7050cb5d4244d7014ede80e1a; removed all 0 slot(s) it used to own
15699:S 06 May 2024 19:13:57.509 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is now part of shard a2427455940b828a38a3ffb09c2b4cdfd4ccf6f5
15699:S 06 May 2024 19:13:57.564 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is no longer master of shard 8d23c540c28858cc5ee10f86f93cd31765bd7b8b; removed all 0 slot(s) it used to own
15699:S 06 May 2024 19:13:57.564 * Node 2b30694b8e24a7702a1229fcee665e160d412a34 () is now part of shard 5f75c91ff19b41d8200527f5fc06215514a6c230
15699:S 06 May 2024 19:13:57.587 * Cluster state changed: ok
15699:signal-handler (1715022840) Received SIGTERM scheduling shutdown...
15699:S 06 May 2024 19:14:00.228 * User requested shutdown...
15699:S 06 May 2024 19:14:00.228 * Calling fsync() on the AOF file.
15699:S 06 May 2024 19:14:00.229 # Valkey is now ready to exit, bye bye...
15943:C 06 May 2024 19:14:11.889 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15943:C 06 May 2024 19:14:11.889 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15943:C 06 May 2024 19:14:11.889 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15943, just started
15943:C 06 May 2024 19:14:11.889 * Configuration loaded
15943:M 06 May 2024 19:14:11.890 * monotonic clock: POSIX clock_gettime
15943:M 06 May 2024 19:14:11.891 * Running mode=cluster, port=30005.
15943:M 06 May 2024 19:14:11.897 * Node configuration loaded, I'm 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555
15943:M 06 May 2024 19:14:11.898 * Server initialized
15943:M 06 May 2024 19:14:12.364 * Reading RDB base file on AOF loading...
15943:M 06 May 2024 19:14:12.364 * Loading RDB produced by valkey version 255.255.255
15943:M 06 May 2024 19:14:12.365 * RDB age 16 seconds
15943:M 06 May 2024 19:14:12.365 * RDB memory usage when created 2.57 Mb
15943:M 06 May 2024 19:14:12.365 * RDB is base AOF
15943:M 06 May 2024 19:14:12.365 * Done loading RDB, keys loaded: 0, keys expired: 0.
15943:M 06 May 2024 19:14:12.365 * DB loaded from base file appendonly.aof.2.base.rdb: 0.467 seconds
15943:M 06 May 2024 19:14:12.365 * DB loaded from append only file: 0.467 seconds
15943:M 06 May 2024 19:14:12.365 * Opening AOF incr file appendonly.aof.2.incr.aof on server start
15943:M 06 May 2024 19:14:12.365 * Ready to accept connections tcp
15943:S 06 May 2024 19:14:12.367 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15943:S 06 May 2024 19:14:12.367 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:12.367 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:12.367 * Cluster state changed: ok
15943:S 06 May 2024 19:14:12.368 * Non blocking connect for SYNC fired the event.
15943:S 06 May 2024 19:14:12.657 * Master replied to PING, replication can continue...
15943:S 06 May 2024 19:14:12.658 * Trying a partial resynchronization (request c64b2b8c141fd2dce46a3dc147e95c01ada24a9b:1).
15943:S 06 May 2024 19:14:12.659 * Full resync from master: 30c6a2d86a0eec577bf64bd257ad9347167b4e10:0
15943:S 06 May 2024 19:14:12.660 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15943:S 06 May 2024 19:14:12.660 * Discarding previously cached master state.
15943:S 06 May 2024 19:14:12.660 * MASTER <-> REPLICA sync: Flushing old data
15943:S 06 May 2024 19:14:12.661 * MASTER <-> REPLICA sync: Loading DB in memory
15943:S 06 May 2024 19:14:12.664 * Loading RDB produced by valkey version 255.255.255
15943:S 06 May 2024 19:14:12.664 * RDB age 0 seconds
15943:S 06 May 2024 19:14:12.664 * RDB memory usage when created 2.33 Mb
15943:S 06 May 2024 19:14:12.664 * Done loading RDB, keys loaded: 0, keys expired: 0.
15943:S 06 May 2024 19:14:12.664 * MASTER <-> REPLICA sync: Finished with success
15943:S 06 May 2024 19:14:12.664 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15943:S 06 May 2024 19:14:12.665 * Background append only file rewriting started by pid 15950
15950:C 06 May 2024 19:14:12.666 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15950.aof
15950:C 06 May 2024 19:14:12.667 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15943:S 06 May 2024 19:14:12.758 * Background AOF rewrite terminated with success
15943:S 06 May 2024 19:14:12.758 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15950.aof into appendonly.aof.3.base.rdb
15943:S 06 May 2024 19:14:12.758 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.3.incr.aof
15943:S 06 May 2024 19:14:12.761 * Removing the history file appendonly.aof.2.incr.aof in the background
15943:S 06 May 2024 19:14:12.761 * Removing the history file appendonly.aof.2.base.rdb in the background
15943:S 06 May 2024 19:14:12.764 * Background AOF rewrite finished successfully
15943:M 06 May 2024 19:14:18.473 * Connection with master lost.
15943:M 06 May 2024 19:14:18.473 * Caching the disconnected master state.
15943:M 06 May 2024 19:14:18.473 * Discarding previously cached master state.
15943:M 06 May 2024 19:14:18.473 * Setting secondary replication ID to 30c6a2d86a0eec577bf64bd257ad9347167b4e10, valid up to offset: 56. New replication ID is f4d56b50770d7be18551a67c438eb8b372c7c244
15943:M 06 May 2024 19:14:18.475 * configEpoch set to 0 via CLUSTER RESET HARD
15943:M 06 May 2024 19:14:18.475 * Node hard reset, now I'm a47fca07157b154ce1988d97b2afcdaa40ce66b2
15943:M 06 May 2024 19:14:18.475 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
15943:M 06 May 2024 19:14:18.475 # Cluster state changed: fail
15943:M 06 May 2024 19:14:18.479 * CONFIG REWRITE executed with success.
15943:M 06 May 2024 19:14:20.287 # Missing implement of connection type tls
15943:S 06 May 2024 19:14:21.888 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15943:S 06 May 2024 19:14:21.888 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:21.888 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:21.889 * Non blocking connect for SYNC fired the event.
15943:S 06 May 2024 19:14:21.889 * Master replied to PING, replication can continue...
15943:S 06 May 2024 19:14:21.889 * Trying a partial resynchronization (request f4d56b50770d7be18551a67c438eb8b372c7c244:56).
15943:S 06 May 2024 19:14:21.891 * Full resync from master: 30c6a2d86a0eec577bf64bd257ad9347167b4e10:55
15943:S 06 May 2024 19:14:21.891 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15943:S 06 May 2024 19:14:21.892 * Discarding previously cached master state.
15943:S 06 May 2024 19:14:21.892 * MASTER <-> REPLICA sync: Flushing old data
15943:S 06 May 2024 19:14:21.892 * MASTER <-> REPLICA sync: Loading DB in memory
15943:S 06 May 2024 19:14:21.893 * Loading RDB produced by valkey version 255.255.255
15943:S 06 May 2024 19:14:21.893 * RDB age 0 seconds
15943:S 06 May 2024 19:14:21.893 * RDB memory usage when created 2.66 Mb
15943:S 06 May 2024 19:14:21.893 * Done loading RDB, keys loaded: 0, keys expired: 0.
15943:S 06 May 2024 19:14:21.893 * MASTER <-> REPLICA sync: Finished with success
15943:S 06 May 2024 19:14:21.893 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15943:S 06 May 2024 19:14:21.894 * Background append only file rewriting started by pid 16000
16000:C 06 May 2024 19:14:21.895 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16000.aof
16000:C 06 May 2024 19:14:21.896 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15943:S 06 May 2024 19:14:21.940 * Background AOF rewrite terminated with success
15943:S 06 May 2024 19:14:21.940 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16000.aof into appendonly.aof.4.base.rdb
15943:S 06 May 2024 19:14:21.940 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.4.incr.aof
15943:S 06 May 2024 19:14:21.942 * Removing the history file appendonly.aof.3.incr.aof in the background
15943:S 06 May 2024 19:14:21.942 * Removing the history file appendonly.aof.3.base.rdb in the background
15943:S 06 May 2024 19:14:21.944 * Background AOF rewrite finished successfully
15943:S 06 May 2024 19:14:22.028 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is no longer master of shard f4049c4ab89f7706b4043fb6168626bdb0c6d5fc; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:22.028 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is now part of shard 1109f6e41ce862f21e8f082deeae031962ef83ac
15943:S 06 May 2024 19:14:22.046 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is no longer master of shard a1ec0cec552145a6ed1b39a5162d032008e3e8fd; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:22.046 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is now part of shard 5ed49dbd94affeba17a99f22ab305f897e30146f
15943:S 06 May 2024 19:14:22.549 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is no longer master of shard afe0861af93a47ac71d717dd7dd1f03d3dbe6564; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:22.549 * Node 24553aed57a0fc014e31d20913d6ddb67daf993b () is now part of shard 6bb248ce1980dda8969ac6f65c18efe9f1d758d2
15943:S 06 May 2024 19:14:22.550 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is no longer master of shard f07b8fb6b6cbfb692ab1b87f30279a0b975efbbe; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:22.550 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is now part of shard a3bfec7d6b49cc6990a2b3560914e0bb1a7bda0c
15943:S 06 May 2024 19:14:23.580 * Cluster state changed: ok
15943:S 06 May 2024 19:14:26.225 * Connection with master lost.
15943:S 06 May 2024 19:14:26.225 * Caching the disconnected master state.
15943:S 06 May 2024 19:14:26.225 * Reconnecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:26.225 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:26.225 # Error condition on socket for SYNC: Connection refused
15943:S 06 May 2024 19:14:27.180 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:27.181 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:27.181 # Error condition on socket for SYNC: Connection refused
15943:S 06 May 2024 19:14:28.187 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:28.187 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:28.187 # Error condition on socket for SYNC: Connection refused
15943:S 06 May 2024 19:14:29.195 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:29.195 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:29.196 # Error condition on socket for SYNC: Connection refused
15943:S 06 May 2024 19:14:30.033 * FAIL message received from 220970bd9ddf17fd39dbd121039031b6350dc7bd () about 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ()
15943:S 06 May 2024 19:14:30.033 # Cluster state changed: fail
15943:S 06 May 2024 19:14:30.103 * Start of election delayed for 815 milliseconds (rank #0, offset 1433).
15943:S 06 May 2024 19:14:30.205 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:30.205 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:30.205 # Error condition on socket for SYNC: Connection refused
15943:S 06 May 2024 19:14:31.011 * Starting a failover election for epoch 21.
15943:S 06 May 2024 19:14:31.020 * Failover election won: I'm the new master.
15943:S 06 May 2024 19:14:31.020 * configEpoch set to 21 after successful failover
15943:M 06 May 2024 19:14:31.020 * Discarding previously cached master state.
15943:M 06 May 2024 19:14:31.020 * Setting secondary replication ID to 30c6a2d86a0eec577bf64bd257ad9347167b4e10, valid up to offset: 1434. New replication ID is ab8bc90c5604dae91fa7c221a1a9a247776d0d30
15943:M 06 May 2024 19:14:31.020 * Cluster state changed: ok
15943:M 06 May 2024 19:14:31.213 * Clear FAIL state for node 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ():master without slots is reachable again.
15943:M 06 May 2024 19:14:31.213 * A failover occurred in shard bccca0c1e0e1dccd3b58d5709156154f9f4c9957; node 4ac21db3dc729d28334184ee1a8bd51324f5ca54 () lost 0 slot(s) to node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () with a config epoch of 21
15943:M 06 May 2024 19:14:31.218 * Replica 127.0.0.1:30000 asks for synchronization
15943:M 06 May 2024 19:14:31.218 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'a885299edcf176196fe3d900bfa95fdbf2c19909', my replication IDs are 'ab8bc90c5604dae91fa7c221a1a9a247776d0d30' and '30c6a2d86a0eec577bf64bd257ad9347167b4e10')
15943:M 06 May 2024 19:14:31.218 * Starting BGSAVE for SYNC with target: replicas sockets
15943:M 06 May 2024 19:14:31.218 * Background RDB transfer started by pid 16077
16077:C 06 May 2024 19:14:31.220 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
15943:M 06 May 2024 19:14:31.221 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
15943:M 06 May 2024 19:14:31.223 * Background RDB transfer terminated with success
15943:M 06 May 2024 19:14:31.223 * Streamed RDB transfer with replica 127.0.0.1:30000 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
15943:M 06 May 2024 19:14:31.223 * Synchronization with replica 127.0.0.1:30000 succeeded
15943:M 06 May 2024 19:14:31.261 * Connection with replica 127.0.0.1:30000 lost.
15943:M 06 May 2024 19:14:31.313 * configEpoch set to 0 via CLUSTER RESET HARD
15943:M 06 May 2024 19:14:31.313 * Node hard reset, now I'm 29597550bd8f898394ae88db5a0ba45f1868fd3f
15943:M 06 May 2024 19:14:31.313 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
15943:M 06 May 2024 19:14:31.313 # Cluster state changed: fail
15943:M 06 May 2024 19:14:31.318 * CONFIG REWRITE executed with success.
15943:S 06 May 2024 19:14:34.818 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15943:S 06 May 2024 19:14:34.818 * Connecting to MASTER 127.0.0.1:30000
15943:S 06 May 2024 19:14:34.818 * MASTER <-> REPLICA sync started
15943:S 06 May 2024 19:14:34.818 * Non blocking connect for SYNC fired the event.
15943:S 06 May 2024 19:14:34.819 * Master replied to PING, replication can continue...
15943:S 06 May 2024 19:14:34.819 * Trying a partial resynchronization (request ab8bc90c5604dae91fa7c221a1a9a247776d0d30:2816).
15943:S 06 May 2024 19:14:34.821 * Full resync from master: 091861482ceb045f831c49702e90317b1a85f90f:2774
15943:S 06 May 2024 19:14:34.821 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15943:S 06 May 2024 19:14:34.822 * Discarding previously cached master state.
15943:S 06 May 2024 19:14:34.822 * MASTER <-> REPLICA sync: Flushing old data
15943:S 06 May 2024 19:14:34.823 * MASTER <-> REPLICA sync: Loading DB in memory
15943:S 06 May 2024 19:14:34.825 * Loading RDB produced by valkey version 255.255.255
15943:S 06 May 2024 19:14:34.825 * RDB age 0 seconds
15943:S 06 May 2024 19:14:34.825 * RDB memory usage when created 2.61 Mb
15943:S 06 May 2024 19:14:34.826 * Done loading RDB, keys loaded: 0, keys expired: 0.
15943:S 06 May 2024 19:14:34.826 * MASTER <-> REPLICA sync: Finished with success
15943:S 06 May 2024 19:14:34.826 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15943:S 06 May 2024 19:14:34.826 * Background append only file rewriting started by pid 16109
16109:C 06 May 2024 19:14:34.827 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16109.aof
16109:C 06 May 2024 19:14:34.828 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15943:S 06 May 2024 19:14:34.844 * Background AOF rewrite terminated with success
15943:S 06 May 2024 19:14:34.845 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16109.aof into appendonly.aof.5.base.rdb
15943:S 06 May 2024 19:14:34.845 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.5.incr.aof
15943:S 06 May 2024 19:14:34.848 * Removing the history file appendonly.aof.4.incr.aof in the background
15943:S 06 May 2024 19:14:34.848 * Removing the history file appendonly.aof.4.base.rdb in the background
15943:S 06 May 2024 19:14:34.850 * Background AOF rewrite finished successfully
15943:S 06 May 2024 19:14:35.556 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is no longer master of shard 76f000c5383c5d2746bf6c7b9d86a92aed4820c2; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:35.556 * Node 671088db4acca8792f424ee7ea50b8153c7529a0 () is now part of shard 45bde1a3488556534c5594a0f6ace0dba22be32b
15943:S 06 May 2024 19:14:35.556 * Node de3054474d708953456bc221c893ce4305ba5136 () is no longer master of shard 77b4d2bf3a14a66eb7635d415bbf20a86e150b57; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:35.556 * Node de3054474d708953456bc221c893ce4305ba5136 () is now part of shard 236cc077a81351c19b896662f24236e227403702
15943:S 06 May 2024 19:14:35.557 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is no longer master of shard f88e4fd9e9180c926982aaac2f65e8d0105c55fe; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:35.557 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is now part of shard 1c2b75be0f3bbf57778cad018055649104140ab5
15943:S 06 May 2024 19:14:35.657 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is no longer master of shard a577fdbb713c832cf9850e989f6b72796108c745; removed all 0 slot(s) it used to own
15943:S 06 May 2024 19:14:35.657 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is now part of shard e2220f97589610879561e699a57b8481b7124689
15943:S 06 May 2024 19:14:36.583 * Cluster state changed: ok
15943:S 06 May 2024 19:14:42.796 * FAIL message received from ffdb16089f859482065bad0e5c9a094d823a1410 () about c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ()
15943:S 06 May 2024 19:14:42.796 # Cluster state changed: fail
15943:S 06 May 2024 19:14:43.895 * Cluster state changed: ok
15943:S 06 May 2024 19:14:44.117 * Clear FAIL state for node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ():master without slots is reachable again.
15943:S 06 May 2024 19:14:44.117 * A failover occurred in shard 45bde1a3488556534c5594a0f6ace0dba22be32b; node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff () lost 0 slot(s) to node 671088db4acca8792f424ee7ea50b8153c7529a0 () with a config epoch of 21
15943:signal-handler (1715022884) Received SIGTERM scheduling shutdown...
15943:S 06 May 2024 19:14:44.521 * User requested shutdown...
15943:S 06 May 2024 19:14:44.521 # Valkey is now ready to exit, bye bye...
16188:C 06 May 2024 19:14:44.575 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16188:C 06 May 2024 19:14:44.575 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16188:C 06 May 2024 19:14:44.575 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16188, just started
16188:C 06 May 2024 19:14:44.575 * Configuration loaded
16188:M 06 May 2024 19:14:44.576 * monotonic clock: POSIX clock_gettime
16188:M 06 May 2024 19:14:44.577 * Running mode=cluster, port=30005.
16188:M 06 May 2024 19:14:44.583 * Node configuration loaded, I'm 29597550bd8f898394ae88db5a0ba45f1868fd3f
16188:M 06 May 2024 19:14:44.584 * Server initialized
16188:M 06 May 2024 19:14:44.584 * Reading RDB base file on AOF loading...
16188:M 06 May 2024 19:14:44.584 * Loading RDB produced by valkey version 255.255.255
16188:M 06 May 2024 19:14:44.584 * RDB age 10 seconds
16188:M 06 May 2024 19:14:44.584 * RDB memory usage when created 2.71 Mb
16188:M 06 May 2024 19:14:44.584 * RDB is base AOF
16188:M 06 May 2024 19:14:44.584 * Done loading RDB, keys loaded: 0, keys expired: 0.
16188:M 06 May 2024 19:14:44.584 * DB loaded from base file appendonly.aof.5.base.rdb: 0.000 seconds
16188:M 06 May 2024 19:14:44.585 * DB loaded from incr file appendonly.aof.5.incr.aof: 0.001 seconds
16188:M 06 May 2024 19:14:44.585 * DB loaded from append only file: 0.001 seconds
16188:M 06 May 2024 19:14:44.585 * Opening AOF incr file appendonly.aof.5.incr.aof on server start
16188:M 06 May 2024 19:14:44.585 * Ready to accept connections tcp
16188:S 06 May 2024 19:14:44.587 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16188:S 06 May 2024 19:14:44.587 * Connecting to MASTER 127.0.0.1:30000
16188:S 06 May 2024 19:14:44.587 * MASTER <-> REPLICA sync started
16188:S 06 May 2024 19:14:44.587 * Cluster state changed: ok
16188:S 06 May 2024 19:14:44.588 * Non blocking connect for SYNC fired the event.
16188:S 06 May 2024 19:14:44.593 * Master replied to PING, replication can continue...
16188:S 06 May 2024 19:14:44.593 * Trying a partial resynchronization (request 1f5271f26ced03f73c5b28b4dea8fc2556fe54ab:1).
16188:S 06 May 2024 19:14:44.593 * Full resync from master: 091861482ceb045f831c49702e90317b1a85f90f:11053
16188:S 06 May 2024 19:14:44.596 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16188:S 06 May 2024 19:14:44.596 * Discarding previously cached master state.
16188:S 06 May 2024 19:14:44.596 * MASTER <-> REPLICA sync: Flushing old data
16188:S 06 May 2024 19:14:44.596 * MASTER <-> REPLICA sync: Loading DB in memory
16188:S 06 May 2024 19:14:44.598 * Loading RDB produced by valkey version 255.255.255
16188:S 06 May 2024 19:14:44.598 * RDB age 0 seconds
16188:S 06 May 2024 19:14:44.598 * RDB memory usage when created 2.75 Mb
16188:S 06 May 2024 19:14:44.598 * Done loading RDB, keys loaded: 110, keys expired: 0.
16188:S 06 May 2024 19:14:44.598 * MASTER <-> REPLICA sync: Finished with success
16188:S 06 May 2024 19:14:44.598 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16188:S 06 May 2024 19:14:44.599 * Background append only file rewriting started by pid 16195
16195:C 06 May 2024 19:14:44.601 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16195.aof
16195:C 06 May 2024 19:14:44.602 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16188:S 06 May 2024 19:14:44.688 * Background AOF rewrite terminated with success
16188:S 06 May 2024 19:14:44.688 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16195.aof into appendonly.aof.6.base.rdb
16188:S 06 May 2024 19:14:44.688 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.6.incr.aof
16188:S 06 May 2024 19:14:44.690 * Removing the history file appendonly.aof.5.incr.aof in the background
16188:S 06 May 2024 19:14:44.690 * Removing the history file appendonly.aof.5.base.rdb in the background
16188:S 06 May 2024 19:14:44.692 * Background AOF rewrite finished successfully
16188:S 06 May 2024 19:14:45.320 # Missing implement of connection type tls
16188:S 06 May 2024 19:14:49.727 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16188:S 06 May 2024 19:14:49.727 # Cluster state changed: fail
16188:S 06 May 2024 19:14:50.867 * Cluster state changed: ok
16188:S 06 May 2024 19:14:51.039 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16188:S 06 May 2024 19:14:51.039 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 22
16188:S 06 May 2024 19:14:55.628 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16188:S 06 May 2024 19:14:55.628 # Cluster state changed: fail
16188:S 06 May 2024 19:14:57.024 * Cluster state changed: ok
16188:S 06 May 2024 19:14:57.344 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16188:S 06 May 2024 19:14:57.347 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 23
16188:S 06 May 2024 19:15:04.032 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about d612f4e24864679d4cb0b4d862c48e2df4373065 ()
16188:S 06 May 2024 19:15:04.032 # Cluster state changed: fail
16188:S 06 May 2024 19:15:04.891 * Cluster state changed: ok
16188:S 06 May 2024 19:15:05.046 * Clear FAIL state for node d612f4e24864679d4cb0b4d862c48e2df4373065 ():master without slots is reachable again.
16188:S 06 May 2024 19:15:05.046 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node d612f4e24864679d4cb0b4d862c48e2df4373065 () lost 0 slot(s) to node c26ea12fd513a731ced8b60c63390b5dbe84235d () with a config epoch of 24
16188:S 06 May 2024 19:15:10.098 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
16188:S 06 May 2024 19:15:10.098 # Cluster state changed: fail
16188:S 06 May 2024 19:15:10.958 * Cluster state changed: ok
16188:S 06 May 2024 19:15:11.114 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
16188:S 06 May 2024 19:15:11.114 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 25
16188:S 06 May 2024 19:15:15.638 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about 9dad64de16025f02b271510ee2d51a6ca079dc7b ()
16188:S 06 May 2024 19:15:15.638 # Cluster state changed: fail
16188:S 06 May 2024 19:15:16.594 * Cluster state changed: ok
16188:S 06 May 2024 19:15:16.847 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
16188:S 06 May 2024 19:15:16.847 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 26
16188:S 06 May 2024 19:15:21.824 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16188:S 06 May 2024 19:15:21.824 # Cluster state changed: fail
16188:S 06 May 2024 19:15:22.742 * Cluster state changed: ok
16188:S 06 May 2024 19:15:22.990 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16188:S 06 May 2024 19:15:22.990 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 27
16188:signal-handler (1715022923) Received SIGTERM scheduling shutdown...
16188:S 06 May 2024 19:15:23.090 * User requested shutdown...
16188:S 06 May 2024 19:15:23.090 # Valkey is now ready to exit, bye bye...
16702:C 06 May 2024 19:15:23.157 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
16702:C 06 May 2024 19:15:23.157 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
16702:C 06 May 2024 19:15:23.157 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=16702, just started
16702:C 06 May 2024 19:15:23.157 * Configuration loaded
16702:M 06 May 2024 19:15:23.157 * monotonic clock: POSIX clock_gettime
16702:M 06 May 2024 19:15:23.158 * Running mode=cluster, port=30005.
16702:M 06 May 2024 19:15:23.166 * Node configuration loaded, I'm 29597550bd8f898394ae88db5a0ba45f1868fd3f
16702:M 06 May 2024 19:15:23.167 * Server initialized
16702:M 06 May 2024 19:15:23.201 * Reading RDB base file on AOF loading...
16702:M 06 May 2024 19:15:23.201 * Loading RDB produced by valkey version 255.255.255
16702:M 06 May 2024 19:15:23.201 * RDB age 39 seconds
16702:M 06 May 2024 19:15:23.201 * RDB memory usage when created 2.24 Mb
16702:M 06 May 2024 19:15:23.201 * RDB is base AOF
16702:M 06 May 2024 19:15:23.202 * Done loading RDB, keys loaded: 110, keys expired: 0.
16702:M 06 May 2024 19:15:23.202 * DB loaded from base file appendonly.aof.6.base.rdb: 0.035 seconds
16702:M 06 May 2024 19:15:23.204 * DB loaded from incr file appendonly.aof.6.incr.aof: 0.001 seconds
16702:M 06 May 2024 19:15:23.204 * DB loaded from append only file: 0.037 seconds
16702:M 06 May 2024 19:15:23.204 * Opening AOF incr file appendonly.aof.6.incr.aof on server start
16702:M 06 May 2024 19:15:23.204 * Ready to accept connections tcp
16702:S 06 May 2024 19:15:23.206 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16702:S 06 May 2024 19:15:23.206 * Connecting to MASTER 127.0.0.1:30000
16702:S 06 May 2024 19:15:23.206 * MASTER <-> REPLICA sync started
16702:S 06 May 2024 19:15:23.206 * Cluster state changed: ok
16702:S 06 May 2024 19:15:23.207 * Non blocking connect for SYNC fired the event.
16702:S 06 May 2024 19:15:23.211 * Master replied to PING, replication can continue...
16702:S 06 May 2024 19:15:23.214 * Trying a partial resynchronization (request 35e9d859b82f5c39b0f53877a239ac5eb5caf59d:1).
16702:S 06 May 2024 19:15:23.216 * Full resync from master: 091861482ceb045f831c49702e90317b1a85f90f:48038
16702:S 06 May 2024 19:15:23.227 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16702:S 06 May 2024 19:15:23.229 * Discarding previously cached master state.
16702:S 06 May 2024 19:15:23.229 * MASTER <-> REPLICA sync: Flushing old data
16702:S 06 May 2024 19:15:23.229 * MASTER <-> REPLICA sync: Loading DB in memory
16702:S 06 May 2024 19:15:23.231 * Loading RDB produced by valkey version 255.255.255
16702:S 06 May 2024 19:15:23.231 * RDB age 0 seconds
16702:S 06 May 2024 19:15:23.231 * RDB memory usage when created 2.88 Mb
16702:S 06 May 2024 19:15:23.232 * Done loading RDB, keys loaded: 604, keys expired: 0.
16702:S 06 May 2024 19:15:23.232 * MASTER <-> REPLICA sync: Finished with success
16702:S 06 May 2024 19:15:23.232 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16702:S 06 May 2024 19:15:23.233 * Background append only file rewriting started by pid 16709
16709:C 06 May 2024 19:15:23.238 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16709.aof
16709:C 06 May 2024 19:15:23.239 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16702:S 06 May 2024 19:15:23.307 * Background AOF rewrite terminated with success
16702:S 06 May 2024 19:15:23.307 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16709.aof into appendonly.aof.7.base.rdb
16702:S 06 May 2024 19:15:23.307 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.7.incr.aof
16702:S 06 May 2024 19:15:23.309 * Removing the history file appendonly.aof.6.incr.aof in the background
16702:S 06 May 2024 19:15:23.309 * Removing the history file appendonly.aof.6.base.rdb in the background
16702:S 06 May 2024 19:15:23.311 * Background AOF rewrite finished successfully
16702:S 06 May 2024 19:15:23.525 # Missing implement of connection type tls
16702:S 06 May 2024 19:15:27.751 * Marking node 9dad64de16025f02b271510ee2d51a6ca079dc7b () as failing (quorum reached).
16702:S 06 May 2024 19:15:27.752 # Cluster state changed: fail
16702:S 06 May 2024 19:15:28.730 * Cluster state changed: ok
16702:S 06 May 2024 19:15:28.867 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
16702:S 06 May 2024 19:15:30.368 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 28
16702:S 06 May 2024 19:15:33.577 * FAIL message received from 671088db4acca8792f424ee7ea50b8153c7529a0 () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
16702:S 06 May 2024 19:15:33.577 # Cluster state changed: fail
16702:S 06 May 2024 19:15:34.372 * Cluster state changed: ok
16702:S 06 May 2024 19:15:34.507 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
16702:M 06 May 2024 19:15:34.916 * Connection with master lost.
16702:M 06 May 2024 19:15:34.916 * Caching the disconnected master state.
16702:M 06 May 2024 19:15:34.916 * Discarding previously cached master state.
16702:M 06 May 2024 19:15:34.916 * Setting secondary replication ID to 091861482ceb045f831c49702e90317b1a85f90f, valid up to offset: 61426. New replication ID is 0d896a9ec5ff02ba2b19e9d8c5db64b7f07f62c5
16702:M 06 May 2024 19:15:34.918 * configEpoch set to 0 via CLUSTER RESET HARD
16702:M 06 May 2024 19:15:34.918 * Node hard reset, now I'm f618c2d793fec3c1be72eac543ed42af49103087
16702:M 06 May 2024 19:15:34.918 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
16702:M 06 May 2024 19:15:34.918 # Cluster state changed: fail
16702:M 06 May 2024 19:15:34.923 * CONFIG REWRITE executed with success.
16702:S 06 May 2024 19:15:38.895 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
16702:S 06 May 2024 19:15:39.092 * Connecting to MASTER 127.0.0.1:30000
16702:S 06 May 2024 19:15:39.092 * MASTER <-> REPLICA sync started
16702:S 06 May 2024 19:15:39.093 * Non blocking connect for SYNC fired the event.
16702:S 06 May 2024 19:15:39.093 * Master replied to PING, replication can continue...
16702:S 06 May 2024 19:15:40.438 * Trying a partial resynchronization (request 0d896a9ec5ff02ba2b19e9d8c5db64b7f07f62c5:61426).
16702:S 06 May 2024 19:15:40.881 * Full resync from master: 091861482ceb045f831c49702e90317b1a85f90f:61425
16702:S 06 May 2024 19:15:40.898 * Cluster state changed: ok
16702:S 06 May 2024 19:15:40.906 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16702:S 06 May 2024 19:15:40.906 * Discarding previously cached master state.
16702:S 06 May 2024 19:15:40.907 * MASTER <-> REPLICA sync: Flushing old data
16702:S 06 May 2024 19:15:40.907 * MASTER <-> REPLICA sync: Loading DB in memory
16702:S 06 May 2024 19:15:40.909 * Loading RDB produced by valkey version 255.255.255
16702:S 06 May 2024 19:15:40.909 * RDB age 0 seconds
16702:S 06 May 2024 19:15:40.909 * RDB memory usage when created 2.75 Mb
16702:S 06 May 2024 19:15:40.909 * Done loading RDB, keys loaded: 0, keys expired: 0.
16702:S 06 May 2024 19:15:40.909 * MASTER <-> REPLICA sync: Finished with success
16702:S 06 May 2024 19:15:40.909 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16702:S 06 May 2024 19:15:40.910 * Background append only file rewriting started by pid 16867
16867:C 06 May 2024 19:15:40.911 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16867.aof
16867:C 06 May 2024 19:15:40.912 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
16702:S 06 May 2024 19:15:41.000 * Background AOF rewrite terminated with success
16702:S 06 May 2024 19:15:41.000 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16867.aof into appendonly.aof.8.base.rdb
16702:S 06 May 2024 19:15:41.000 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.8.incr.aof
16702:S 06 May 2024 19:15:41.001 * Removing the history file appendonly.aof.7.incr.aof in the background
16702:S 06 May 2024 19:15:41.001 * Removing the history file appendonly.aof.7.base.rdb in the background
16702:S 06 May 2024 19:15:41.003 * Background AOF rewrite finished successfully
16702:S 06 May 2024 19:15:41.505 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is no longer master of shard f65a03c57d7accc16ebbe0644b4f594992bf8dce; removed all 0 slot(s) it used to own
16702:S 06 May 2024 19:15:41.505 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is now part of shard 897f6391c215d2fd35c543f86aae669878fdc874
16702:S 06 May 2024 19:15:42.008 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is no longer master of shard 3af8d21a8c53a9b6624c360029651d1f88745f55; removed all 0 slot(s) it used to own
16702:S 06 May 2024 19:15:42.008 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is now part of shard 6b4d9b2c1f3548ebff40277e67444291a3fa23f5
16702:S 06 May 2024 19:15:42.416 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is no longer master of shard e64780e5c850ee71f0b1b08d3ba6db17ccf4bce2; removed all 0 slot(s) it used to own
16702:S 06 May 2024 19:15:42.416 * Node a0009d5f604d884e784a3056b063090422c8ce55 () is now part of shard d3a218f528452be7c9ae6dfd00c32dfc82e7170d
16702:S 06 May 2024 19:15:42.507 * Node 6a35a86928be660bf08d53b5985287118d92066b () is no longer master of shard 17431b2ec492ed011920dbf31b9d73f688b42f6f; removed all 0 slot(s) it used to own
16702:S 06 May 2024 19:15:42.507 * Node 6a35a86928be660bf08d53b5985287118d92066b () is now part of shard 78600edd67e7ea47e3a029230067904131c90109
16702:S 06 May 2024 19:17:18.947 * Connection with master lost.
16702:S 06 May 2024 19:17:18.947 * Caching the disconnected master state.
16702:S 06 May 2024 19:17:18.947 * Reconnecting to MASTER 127.0.0.1:30000
16702:S 06 May 2024 19:17:18.947 * MASTER <-> REPLICA sync started
16702:S 06 May 2024 19:17:18.947 # Error condition on socket for SYNC: Connection refused
16702:S 06 May 2024 19:17:19.192 * Connecting to MASTER 127.0.0.1:30000
16702:S 06 May 2024 19:17:19.193 * MASTER <-> REPLICA sync started
16702:S 06 May 2024 19:17:19.193 * Non blocking connect for SYNC fired the event.
16702:S 06 May 2024 19:17:19.193 * Master replied to PING, replication can continue...
16702:S 06 May 2024 19:17:19.193 * Trying a partial resynchronization (request 091861482ceb045f831c49702e90317b1a85f90f:1815594).
16702:S 06 May 2024 19:17:19.193 * Full resync from master: d15c200a350b478216a6c7c144a8299560016a04:0
16702:S 06 May 2024 19:17:19.197 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
16702:S 06 May 2024 19:17:19.226 * Discarding previously cached master state.
16702:S 06 May 2024 19:17:19.226 * MASTER <-> REPLICA sync: Flushing old data
16702:S 06 May 2024 19:17:19.230 * MASTER <-> REPLICA sync: Loading DB in memory
16702:S 06 May 2024 19:17:19.232 * Loading RDB produced by valkey version 255.255.255
16702:S 06 May 2024 19:17:19.233 * RDB age 0 seconds
16702:S 06 May 2024 19:17:19.233 * RDB memory usage when created 3.48 Mb
16702:S 06 May 2024 19:17:19.248 * Done loading RDB, keys loaded: 10231, keys expired: 0.
16702:S 06 May 2024 19:17:19.248 * MASTER <-> REPLICA sync: Finished with success
16702:S 06 May 2024 19:17:19.248 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
16702:S 06 May 2024 19:17:19.249 * Background append only file rewriting started by pid 17210
17210:C 06 May 2024 19:17:19.276 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17210.aof
17210:C 06 May 2024 19:17:19.276 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 0 MB
16702:S 06 May 2024 19:17:19.294 * Background AOF rewrite terminated with success
16702:S 06 May 2024 19:17:19.294 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17210.aof into appendonly.aof.9.base.rdb
16702:S 06 May 2024 19:17:19.294 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.9.incr.aof
16702:S 06 May 2024 19:17:19.296 * Removing the history file appendonly.aof.8.incr.aof in the background
16702:S 06 May 2024 19:17:19.297 * Removing the history file appendonly.aof.8.base.rdb in the background
16702:S 06 May 2024 19:17:19.298 * Background AOF rewrite finished successfully
16702:signal-handler (1715023040) Received SIGTERM scheduling shutdown...
16702:S 06 May 2024 19:17:20.208 * User requested shutdown...
16702:S 06 May 2024 19:17:20.208 # Valkey is now ready to exit, bye bye...
17266:C 06 May 2024 19:17:20.251 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
17266:C 06 May 2024 19:17:20.251 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
17266:C 06 May 2024 19:17:20.251 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=17266, just started
17266:C 06 May 2024 19:17:20.251 * Configuration loaded
17266:M 06 May 2024 19:17:20.251 * monotonic clock: POSIX clock_gettime
17266:M 06 May 2024 19:17:20.252 * Running mode=cluster, port=30005.
17266:M 06 May 2024 19:17:20.259 * Node configuration loaded, I'm f618c2d793fec3c1be72eac543ed42af49103087
17266:M 06 May 2024 19:17:20.259 * Server initialized
17266:M 06 May 2024 19:17:20.260 * Reading RDB base file on AOF loading...
17266:M 06 May 2024 19:17:20.260 * Loading RDB produced by valkey version 255.255.255
17266:M 06 May 2024 19:17:20.260 * RDB age 1 seconds
17266:M 06 May 2024 19:17:20.260 * RDB memory usage when created 3.87 Mb
17266:M 06 May 2024 19:17:20.260 * RDB is base AOF
17266:M 06 May 2024 19:17:20.274 * Done loading RDB, keys loaded: 10231, keys expired: 0.
17266:M 06 May 2024 19:17:20.274 * DB loaded from base file appendonly.aof.9.base.rdb: 0.015 seconds
17266:M 06 May 2024 19:17:20.275 * DB loaded from append only file: 0.015 seconds
17266:M 06 May 2024 19:17:20.275 * Opening AOF incr file appendonly.aof.9.incr.aof on server start
17266:M 06 May 2024 19:17:20.275 * Ready to accept connections tcp
17266:S 06 May 2024 19:17:20.276 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17266:S 06 May 2024 19:17:20.276 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:20.277 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:20.277 * Cluster state changed: ok
17266:S 06 May 2024 19:17:20.277 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:17:20.281 * Master replied to PING, replication can continue...
17266:S 06 May 2024 19:17:20.281 * Trying a partial resynchronization (request 75929e2ea3ddca232e4a8020154f67af1b69971a:1).
17266:S 06 May 2024 19:17:20.282 * Full resync from master: d15c200a350b478216a6c7c144a8299560016a04:14
17266:S 06 May 2024 19:17:20.285 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17266:S 06 May 2024 19:17:20.307 * Discarding previously cached master state.
17266:S 06 May 2024 19:17:20.307 * MASTER <-> REPLICA sync: Flushing old data
17266:S 06 May 2024 19:17:20.309 * MASTER <-> REPLICA sync: Loading DB in memory
17266:S 06 May 2024 19:17:20.311 * Loading RDB produced by valkey version 255.255.255
17266:S 06 May 2024 19:17:20.311 * RDB age 0 seconds
17266:S 06 May 2024 19:17:20.311 * RDB memory usage when created 3.52 Mb
17266:S 06 May 2024 19:17:20.325 * Done loading RDB, keys loaded: 10231, keys expired: 0.
17266:S 06 May 2024 19:17:20.325 * MASTER <-> REPLICA sync: Finished with success
17266:S 06 May 2024 19:17:20.325 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
17266:S 06 May 2024 19:17:20.326 * Background append only file rewriting started by pid 17274
17274:C 06 May 2024 19:17:20.353 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17274.aof
17274:C 06 May 2024 19:17:20.354 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 0 MB
17266:S 06 May 2024 19:17:20.377 * Background AOF rewrite terminated with success
17266:S 06 May 2024 19:17:20.377 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17274.aof into appendonly.aof.10.base.rdb
17266:S 06 May 2024 19:17:20.377 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.10.incr.aof
17266:S 06 May 2024 19:17:20.379 * Removing the history file appendonly.aof.9.incr.aof in the background
17266:S 06 May 2024 19:17:20.379 * Removing the history file appendonly.aof.9.base.rdb in the background
17266:S 06 May 2024 19:17:20.381 * Background AOF rewrite finished successfully
17266:M 06 May 2024 19:17:34.770 * Connection with master lost.
17266:M 06 May 2024 19:17:34.770 * Caching the disconnected master state.
17266:M 06 May 2024 19:17:34.770 * Discarding previously cached master state.
17266:M 06 May 2024 19:17:34.770 * Setting secondary replication ID to d15c200a350b478216a6c7c144a8299560016a04, valid up to offset: 70. New replication ID is 5cd533faaf50e6af239ba19a8805907891b55801
17266:M 06 May 2024 19:17:34.772 * configEpoch set to 0 via CLUSTER RESET HARD
17266:M 06 May 2024 19:17:34.772 * Node hard reset, now I'm 078932a058b3293ad41a7bf812f0c25bcb0049d7
17266:M 06 May 2024 19:17:34.772 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
17266:M 06 May 2024 19:17:34.772 # Cluster state changed: fail
17266:M 06 May 2024 19:17:34.779 * CONFIG REWRITE executed with success.
17266:M 06 May 2024 19:17:37.608 # Missing implement of connection type tls
17266:M 06 May 2024 19:17:41.910 * Cluster state changed: ok
17266:S 06 May 2024 19:17:42.588 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17266:S 06 May 2024 19:17:42.588 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:42.588 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:42.614 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:17:42.615 * Master replied to PING, replication can continue...
17266:S 06 May 2024 19:17:42.616 * Trying a partial resynchronization (request 5cd533faaf50e6af239ba19a8805907891b55801:70).
17266:S 06 May 2024 19:17:42.616 * Full resync from master: d15c200a350b478216a6c7c144a8299560016a04:69
17266:S 06 May 2024 19:17:42.618 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17266:S 06 May 2024 19:17:42.619 * Discarding previously cached master state.
17266:S 06 May 2024 19:17:42.619 * MASTER <-> REPLICA sync: Flushing old data
17266:S 06 May 2024 19:17:42.620 * MASTER <-> REPLICA sync: Loading DB in memory
17266:S 06 May 2024 19:17:42.621 * Loading RDB produced by valkey version 255.255.255
17266:S 06 May 2024 19:17:42.621 * RDB age 0 seconds
17266:S 06 May 2024 19:17:42.621 * RDB memory usage when created 2.73 Mb
17266:S 06 May 2024 19:17:42.621 * Done loading RDB, keys loaded: 0, keys expired: 0.
17266:S 06 May 2024 19:17:42.622 * MASTER <-> REPLICA sync: Finished with success
17266:S 06 May 2024 19:17:43.908 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is no longer master of shard 5000b06b9aff419dd88e8f75370c3da32551a1cb; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:43.908 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17266:S 06 May 2024 19:17:44.092 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is no longer master of shard 5acc99edc411149fcdb27d6533feac7207649ef7; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.092 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17266:S 06 May 2024 19:17:44.148 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is no longer master of shard 33c4d632371303bcb565e5c7aba0936e2a76b03a; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.148 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17266:S 06 May 2024 19:17:44.148 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is no longer master of shard 1a16ede24c20c880468822e63f78780773a34e47; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.148 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17266:S 06 May 2024 19:17:44.150 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is no longer master of shard 6c2b56a67af1fb0be8ff8b1b4eefe129c23debb0; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.150 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17266:S 06 May 2024 19:17:44.152 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is no longer master of shard 1e88f30dc751b7903da19b3c4c8bc3173a3efb8b; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.152 * Node 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17266:S 06 May 2024 19:17:44.205 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is no longer master of shard 873ddfcf8e44ed993d28f2c2cc6dee5b6aa4f588; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:44.205 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17266:S 06 May 2024 19:17:45.501 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is no longer master of shard 0855cdcd88c49b00e8d3cb7a4549759c3b4ffc32; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:45.501 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17266:S 06 May 2024 19:17:45.659 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is no longer master of shard 54bf45e726f98496ec4871cc44f69eed009b4194; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:17:45.659 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17266:S 06 May 2024 19:17:49.041 * FAIL message received from 8638bb3919fb522af84a8fe3990ded8caa70968c () about 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ()
17266:S 06 May 2024 19:17:54.685 * Clear FAIL state for node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ():replica is reachable again.
17266:S 06 May 2024 19:17:54.760 * Connection with master lost.
17266:S 06 May 2024 19:17:54.761 * Caching the disconnected master state.
17266:S 06 May 2024 19:17:54.761 * Reconnecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:54.761 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:54.761 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:55.261 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:55.261 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:55.262 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:56.273 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:56.273 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:56.273 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:57.280 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:57.280 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:57.280 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:58.289 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:58.289 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:58.290 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:58.848 * FAIL message received from c82ade1d42699c8d38b49704a4d508edf28962ad () about c6e768b2f8702ccd365a393b642c8ac83ff22379 ()
17266:S 06 May 2024 19:17:58.848 # Cluster state changed: fail
17266:S 06 May 2024 19:17:58.896 * Start of election delayed for 774 milliseconds (rank #0, offset 1150).
17266:S 06 May 2024 19:17:59.300 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:17:59.300 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:17:59.300 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:17:59.705 * Starting a failover election for epoch 21.
17266:S 06 May 2024 19:17:59.720 * Failover election won: I'm the new master.
17266:S 06 May 2024 19:17:59.721 * configEpoch set to 21 after successful failover
17266:M 06 May 2024 19:17:59.721 * Discarding previously cached master state.
17266:M 06 May 2024 19:17:59.721 * Setting secondary replication ID to d15c200a350b478216a6c7c144a8299560016a04, valid up to offset: 1151. New replication ID is 5e899519bec2e2cdd4a7b61ea586bd98560e7bac
17266:M 06 May 2024 19:17:59.721 * Cluster state changed: ok
17266:M 06 May 2024 19:17:59.759 * Replica 127.0.0.1:30010 asks for synchronization
17266:M 06 May 2024 19:17:59.759 * Partial resynchronization request from 127.0.0.1:30010 accepted. Sending 536 bytes of backlog starting from offset 615.
17266:M 06 May 2024 19:17:59.807 * Clear FAIL state for node c6e768b2f8702ccd365a393b642c8ac83ff22379 ():master without slots is reachable again.
17266:M 06 May 2024 19:17:59.807 * A failover occurred in shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc; node c6e768b2f8702ccd365a393b642c8ac83ff22379 () lost 0 slot(s) to node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () with a config epoch of 21
17266:M 06 May 2024 19:17:59.811 * Replica 127.0.0.1:30000 asks for synchronization
17266:M 06 May 2024 19:17:59.811 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'aae9d25481cb6002c313a9721fe18a0646fad75f', my replication IDs are '5e899519bec2e2cdd4a7b61ea586bd98560e7bac' and 'd15c200a350b478216a6c7c144a8299560016a04')
17266:M 06 May 2024 19:17:59.811 * Starting BGSAVE for SYNC with target: replicas sockets
17266:M 06 May 2024 19:17:59.811 * Background RDB transfer started by pid 17601
17601:C 06 May 2024 19:17:59.817 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
17266:M 06 May 2024 19:17:59.818 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
17266:M 06 May 2024 19:17:59.846 * Connection with replica 127.0.0.1:30000 lost.
17266:M 06 May 2024 19:17:59.885 * configEpoch set to 0 via CLUSTER RESET HARD
17266:M 06 May 2024 19:17:59.885 * Node hard reset, now I'm 3a8410fdf29d80b74cbb245767c5e44636bc3277
17266:M 06 May 2024 19:17:59.885 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
17266:M 06 May 2024 19:17:59.885 # Cluster state changed: fail
17266:M 06 May 2024 19:17:59.890 * CONFIG REWRITE executed with success.
17266:M 06 May 2024 19:17:59.907 * Background RDB transfer terminated with success
17266:M 06 May 2024 19:17:59.922 * Connection with replica 127.0.0.1:30010 lost.
17266:S 06 May 2024 19:18:02.766 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17266:S 06 May 2024 19:18:02.766 * Connecting to MASTER 127.0.0.1:30002
17266:S 06 May 2024 19:18:02.766 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:18:02.766 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:18:02.767 * Master replied to PING, replication can continue...
17266:S 06 May 2024 19:18:02.767 * Trying a partial resynchronization (request 5e899519bec2e2cdd4a7b61ea586bd98560e7bac:1192).
17266:S 06 May 2024 19:18:02.767 * Full resync from master: e2b6958829f686f19f8f716ece21662ac44531c0:1460
17266:S 06 May 2024 19:18:02.769 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17266:S 06 May 2024 19:18:02.769 * Discarding previously cached master state.
17266:S 06 May 2024 19:18:02.769 * MASTER <-> REPLICA sync: Flushing old data
17266:S 06 May 2024 19:18:02.769 * MASTER <-> REPLICA sync: Loading DB in memory
17266:S 06 May 2024 19:18:02.771 * Loading RDB produced by valkey version 255.255.255
17266:S 06 May 2024 19:18:02.771 * RDB age 0 seconds
17266:S 06 May 2024 19:18:02.771 * RDB memory usage when created 2.76 Mb
17266:S 06 May 2024 19:18:02.771 * Done loading RDB, keys loaded: 0, keys expired: 0.
17266:S 06 May 2024 19:18:02.771 * MASTER <-> REPLICA sync: Finished with success
17266:S 06 May 2024 19:18:02.831 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is no longer master of shard d0d012e1f78210fb0613fb1538f24f7814fe81bd; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:02.831 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17266:S 06 May 2024 19:18:03.090 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is no longer master of shard 1bf2cd731a3dbe35a6cec5639318bdb95d99061d; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.090 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17266:S 06 May 2024 19:18:03.334 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is no longer master of shard dfcb8ddc797b36f3027193f68a243825e66ecb76; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.334 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17266:S 06 May 2024 19:18:03.537 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is no longer master of shard 71ddfeaa46e5002962c1f850ecca2bc4f0152083; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.537 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17266:S 06 May 2024 19:18:03.538 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is no longer master of shard 1f9cf276999006c5e2fcd79b4eb3d5840c05a58b; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.538 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17266:S 06 May 2024 19:18:03.539 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is no longer master of shard 66a60939691fd1e6ee1492ac1dbc0dece4a724e9; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.539 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17266:S 06 May 2024 19:18:03.544 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is no longer master of shard 4a54cdb367579758a4e3f891240c194e22265b3d; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.544 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17266:S 06 May 2024 19:18:03.544 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is no longer master of shard ab9f767e76ee34d7511533a455ad90ed1b50d27b; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.544 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17266:S 06 May 2024 19:18:03.585 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is no longer master of shard 07caa91caff12c6fc179408189c0b2325234614a; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.585 * Node ee4e8801b63592ad76065cf689613b8590e8adbd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17266:S 06 May 2024 19:18:03.682 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is no longer master of shard 75182d21b15b8439f1522e589f314577961349dd; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.682 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17266:S 06 May 2024 19:18:03.840 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is no longer master of shard 30c8fb7d5fcde244056aac469ab0a18d2dfa4e6a; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:03.840 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17266:S 06 May 2024 19:18:04.042 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is no longer master of shard fe0f6b7f23d33e5bedb35ba3947a32d59a5d868f; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:04.042 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17266:S 06 May 2024 19:18:04.043 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is no longer master of shard cb9315006ffe129274468e5ed2feb38a7f33269f; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:04.043 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17266:S 06 May 2024 19:18:04.043 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is no longer master of shard dae9d79e83baa0dbce3720b336c773fde1c02492; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:04.043 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17266:S 06 May 2024 19:18:04.545 * Cluster state changed: ok
17266:S 06 May 2024 19:18:11.039 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about 450684f318abb9002cec036680f3cb52f5f919b0 ()
17266:S 06 May 2024 19:18:11.039 # Cluster state changed: fail
17266:S 06 May 2024 19:18:11.772 * Cluster state changed: ok
17266:S 06 May 2024 19:18:16.540 * Marking node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () as failing (quorum reached).
17266:S 06 May 2024 19:18:16.540 # Cluster state changed: fail
17266:S 06 May 2024 19:18:24.726 * Cluster state changed: ok
17266:S 06 May 2024 19:18:30.545 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17266:S 06 May 2024 19:18:30.545 # Cluster state changed: fail
17266:S 06 May 2024 19:18:30.607 * Cluster state changed: ok
17266:S 06 May 2024 19:18:35.524 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17266:S 06 May 2024 19:18:35.524 # Cluster state changed: fail
17266:S 06 May 2024 19:18:37.663 * Cluster state changed: ok
17266:S 06 May 2024 19:18:42.072 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about ee4e8801b63592ad76065cf689613b8590e8adbd ()
17266:S 06 May 2024 19:18:42.072 # Cluster state changed: fail
17266:S 06 May 2024 19:18:42.163 * Cluster state changed: ok
17266:S 06 May 2024 19:18:42.248 * Clear FAIL state for node 450684f318abb9002cec036680f3cb52f5f919b0 ():master without slots is reachable again.
17266:S 06 May 2024 19:18:42.248 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 450684f318abb9002cec036680f3cb52f5f919b0 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17266:S 06 May 2024 19:18:42.349 * Clear FAIL state for node b4c96fe54bd211d5373161c89d6e0f09e6bae07a ():master without slots is reachable again.
17266:S 06 May 2024 19:18:42.349 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17266:S 06 May 2024 19:18:42.450 * Clear FAIL state for node ee4e8801b63592ad76065cf689613b8590e8adbd ():master without slots is reachable again.
17266:S 06 May 2024 19:18:42.450 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node ee4e8801b63592ad76065cf689613b8590e8adbd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17266:S 06 May 2024 19:18:42.450 * Clear FAIL state for node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ():master without slots is reachable again.
17266:S 06 May 2024 19:18:42.450 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17266:S 06 May 2024 19:18:42.551 * Clear FAIL state for node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ():master without slots is reachable again.
17266:S 06 May 2024 19:18:42.551 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17266:M 06 May 2024 19:18:42.619 * Connection with master lost.
17266:M 06 May 2024 19:18:42.619 * Caching the disconnected master state.
17266:M 06 May 2024 19:18:42.619 * Discarding previously cached master state.
17266:M 06 May 2024 19:18:42.619 * Setting secondary replication ID to e2b6958829f686f19f8f716ece21662ac44531c0, valid up to offset: 1558. New replication ID is b1d3798a48160c62cff577878e172f39b729996d
17266:M 06 May 2024 19:18:42.620 * configEpoch set to 0 via CLUSTER RESET HARD
17266:M 06 May 2024 19:18:42.620 * Node hard reset, now I'm 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9
17266:M 06 May 2024 19:18:42.620 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
17266:M 06 May 2024 19:18:42.620 # Cluster state changed: fail
17266:M 06 May 2024 19:18:42.624 * CONFIG REWRITE executed with success.
17266:S 06 May 2024 19:18:45.309 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17266:S 06 May 2024 19:18:45.309 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:18:45.310 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:18:45.310 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:18:45.310 * Master replied to PING, replication can continue...
17266:S 06 May 2024 19:18:45.310 * Trying a partial resynchronization (request b1d3798a48160c62cff577878e172f39b729996d:1558).
17266:S 06 May 2024 19:18:45.311 * Full resync from master: 5f260c7433fd3c141e3e8291b44b8f032f4fec14:1298
17266:S 06 May 2024 19:18:45.312 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17266:S 06 May 2024 19:18:45.312 * Discarding previously cached master state.
17266:S 06 May 2024 19:18:45.312 * MASTER <-> REPLICA sync: Flushing old data
17266:S 06 May 2024 19:18:45.312 * MASTER <-> REPLICA sync: Loading DB in memory
17266:S 06 May 2024 19:18:45.315 * Loading RDB produced by valkey version 255.255.255
17266:S 06 May 2024 19:18:45.315 * RDB age 0 seconds
17266:S 06 May 2024 19:18:45.315 * RDB memory usage when created 2.59 Mb
17266:S 06 May 2024 19:18:45.315 * Done loading RDB, keys loaded: 0, keys expired: 0.
17266:S 06 May 2024 19:18:45.315 * MASTER <-> REPLICA sync: Finished with success
17266:S 06 May 2024 19:18:45.576 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is no longer master of shard 7274a8e7bc1e3d5fb81631bf0adc052f6d6a740c; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:45.576 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is now part of shard 769aeb08b175de4efdfee71d709927c7f9afc5bf
17266:S 06 May 2024 19:18:45.577 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is no longer master of shard 7eb196d163c5bf7eb03371da41c37de6ab3a27c7; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:45.577 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is now part of shard d05b2d9c8fad619a6a58ab01b0e5ade057de5ca1
17266:S 06 May 2024 19:18:46.079 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is no longer master of shard 841113874b49ed5f9d1e126879f92ea4392dcdab; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:46.079 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is now part of shard 215dc02bf2afb70a46c0e0788801a0f308924862
17266:S 06 May 2024 19:18:46.583 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is no longer master of shard 35470f3f7014665726088479ee7b0dd699dc35de; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:18:46.583 * Node 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88 () is now part of shard 482cb579f47fb8ccd86d73983fba9c22f7197882
17266:S 06 May 2024 19:18:46.587 * Cluster state changed: ok
17266:S 06 May 2024 19:18:48.646 * Connection with master lost.
17266:S 06 May 2024 19:18:48.646 * Caching the disconnected master state.
17266:S 06 May 2024 19:18:48.646 * Reconnecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:18:48.646 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:08.647 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:19:08.748 # Failed to read response from the server: Success
17266:S 06 May 2024 19:19:08.748 # Master did not respond to command during SYNC handshake
17266:S 06 May 2024 19:19:09.653 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:09.653 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:09.653 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:10.659 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:10.660 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:10.660 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:11.666 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:11.666 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:11.666 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:12.529 * FAIL message received from e4844e444bd7db045069d93fe2f71d3259852adf () about 03a61a2c512c0aab55d8de4c03711ba741fddb86 ()
17266:S 06 May 2024 19:19:12.529 # Cluster state changed: fail
17266:S 06 May 2024 19:19:12.674 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:12.674 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:12.674 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:13.681 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:13.681 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:13.681 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:14.688 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:14.688 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:14.688 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:15.696 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:15.696 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:15.696 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:16.703 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:16.703 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:16.703 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:17.710 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:17.710 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:17.710 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:18.717 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:18.718 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:18.718 # Error condition on socket for SYNC: Connection refused
17266:S 06 May 2024 19:19:18.872 * Clear FAIL state for node 03a61a2c512c0aab55d8de4c03711ba741fddb86 (): is reachable again and nobody is serving its slots after some time.
17266:S 06 May 2024 19:19:18.872 * Cluster state changed: ok
17266:M 06 May 2024 19:19:18.957 * Discarding previously cached master state.
17266:M 06 May 2024 19:19:18.957 * Setting secondary replication ID to 5f260c7433fd3c141e3e8291b44b8f032f4fec14, valid up to offset: 1299. New replication ID is 272cc800834217625d6ca615c2b465caac3352c7
17266:M 06 May 2024 19:19:18.959 * configEpoch set to 0 via CLUSTER RESET HARD
17266:M 06 May 2024 19:19:18.959 * Node hard reset, now I'm 169ce63f84ab73647e330599b7668e357009f9c7
17266:M 06 May 2024 19:19:18.959 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
17266:M 06 May 2024 19:19:18.959 # Cluster state changed: fail
17266:M 06 May 2024 19:19:18.964 * CONFIG REWRITE executed with success.
17266:S 06 May 2024 19:19:21.906 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17266:S 06 May 2024 19:19:21.906 * Connecting to MASTER 127.0.0.1:30000
17266:S 06 May 2024 19:19:21.906 * MASTER <-> REPLICA sync started
17266:S 06 May 2024 19:19:21.907 * Non blocking connect for SYNC fired the event.
17266:S 06 May 2024 19:19:21.907 * Master replied to PING, replication can continue...
17266:S 06 May 2024 19:19:21.907 * Trying a partial resynchronization (request 272cc800834217625d6ca615c2b465caac3352c7:1299).
17266:S 06 May 2024 19:19:21.907 * Full resync from master: 79b6738feadf1eb2596309e6b619ae2da5678456:1339
17266:S 06 May 2024 19:19:21.910 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17266:S 06 May 2024 19:19:21.910 * Discarding previously cached master state.
17266:S 06 May 2024 19:19:21.910 * MASTER <-> REPLICA sync: Flushing old data
17266:S 06 May 2024 19:19:21.910 * MASTER <-> REPLICA sync: Loading DB in memory
17266:S 06 May 2024 19:19:21.913 * Loading RDB produced by valkey version 255.255.255
17266:S 06 May 2024 19:19:21.913 * RDB age 0 seconds
17266:S 06 May 2024 19:19:21.913 * RDB memory usage when created 2.63 Mb
17266:S 06 May 2024 19:19:21.913 * Done loading RDB, keys loaded: 0, keys expired: 0.
17266:S 06 May 2024 19:19:21.913 * MASTER <-> REPLICA sync: Finished with success
17266:S 06 May 2024 19:19:22.510 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is no longer master of shard 614a1b42f4baa25c7f6d5a11b30ac1fdea247594; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.510 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17266:S 06 May 2024 19:19:22.541 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is no longer master of shard 01f47c3a12aa4f053a807eeb5eb545c5e07db794; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.541 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17266:S 06 May 2024 19:19:22.600 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is no longer master of shard ab8e805388a16f08bc5fa7bfd304cf9b2f43c122; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.600 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17266:S 06 May 2024 19:19:22.608 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is no longer master of shard b3f6dd9ab5620f4d8bda693091f3d8d36ad0e7d3; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.608 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17266:S 06 May 2024 19:19:22.608 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is no longer master of shard 5d8efc15227cc965bd521169b3cd81eb9295ec19; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.608 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17266:S 06 May 2024 19:19:22.618 * Cluster state changed: ok
17266:S 06 May 2024 19:19:22.935 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is no longer master of shard aae63656731167d9c699175203047b6e26efb4be; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:22.935 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17266:S 06 May 2024 19:19:23.038 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is no longer master of shard d8860f9f28d646383a01d2f3aba63d0f3a3e4faf; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:23.038 * Node 91aab1feff1314a17b72183eb4e8308f320601b3 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17266:S 06 May 2024 19:19:23.051 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is no longer master of shard 1c166555732b739d6e4c1ab4dd8d165e832b3954; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:23.051 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17266:S 06 May 2024 19:19:23.101 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is no longer master of shard 7c13d5b404a532519713ecb0009d48d7b7148c0e; removed all 0 slot(s) it used to own
17266:S 06 May 2024 19:19:23.102 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17266:signal-handler (1715023165) Received SIGTERM scheduling shutdown...
17266:S 06 May 2024 19:19:25.424 * User requested shutdown...
17266:S 06 May 2024 19:19:25.424 # Valkey is now ready to exit, bye bye...
18071:C 06 May 2024 19:19:35.202 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
18071:C 06 May 2024 19:19:35.202 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
18071:C 06 May 2024 19:19:35.202 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=18071, just started
18071:C 06 May 2024 19:19:35.202 * Configuration loaded
18071:M 06 May 2024 19:19:35.202 * monotonic clock: POSIX clock_gettime
18071:M 06 May 2024 19:19:35.203 * Running mode=cluster, port=30005.
18071:M 06 May 2024 19:19:35.209 * Node configuration loaded, I'm 169ce63f84ab73647e330599b7668e357009f9c7
18071:M 06 May 2024 19:19:35.210 * Server initialized
18071:M 06 May 2024 19:19:35.210 * Loading RDB produced by valkey version 255.255.255
18071:M 06 May 2024 19:19:35.210 * RDB age 14 seconds
18071:M 06 May 2024 19:19:35.211 * RDB memory usage when created 2.63 Mb
18071:M 06 May 2024 19:19:35.211 * Done loading RDB, keys loaded: 0, keys expired: 0.
18071:M 06 May 2024 19:19:35.211 * DB loaded from disk: 0.000 seconds
18071:M 06 May 2024 19:19:35.211 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18071:M 06 May 2024 19:19:35.211 * Ready to accept connections tcp
18071:S 06 May 2024 19:19:35.212 * Discarding previously cached master state.
18071:S 06 May 2024 19:19:35.212 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18071:S 06 May 2024 19:19:35.212 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:19:35.212 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:19:35.212 * Cluster state changed: ok
18071:S 06 May 2024 19:19:35.213 * Non blocking connect for SYNC fired the event.
18071:S 06 May 2024 19:19:35.216 * Master replied to PING, replication can continue...
18071:S 06 May 2024 19:19:35.219 * Trying a partial resynchronization (request 79b6738feadf1eb2596309e6b619ae2da5678456:1340).
18071:S 06 May 2024 19:19:35.220 * Successful partial resynchronization with master.
18071:S 06 May 2024 19:19:35.220 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
18071:M 06 May 2024 19:19:35.526 * Connection with master lost.
18071:M 06 May 2024 19:19:35.526 * Caching the disconnected master state.
18071:M 06 May 2024 19:19:35.526 * Discarding previously cached master state.
18071:M 06 May 2024 19:19:35.526 * Setting secondary replication ID to 79b6738feadf1eb2596309e6b619ae2da5678456, valid up to offset: 1381. New replication ID is 961e232fa61297979e494c98a031662af4e17d5c
18071:M 06 May 2024 19:19:35.528 * configEpoch set to 0 via CLUSTER RESET HARD
18071:M 06 May 2024 19:19:35.528 * Node hard reset, now I'm 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df
18071:M 06 May 2024 19:19:35.528 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18071:M 06 May 2024 19:19:35.528 # Cluster state changed: fail
18071:M 06 May 2024 19:19:35.532 * CONFIG REWRITE executed with success.
18071:M 06 May 2024 19:19:38.906 # Missing implement of connection type tls
18071:S 06 May 2024 19:19:41.450 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18071:S 06 May 2024 19:19:41.450 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:19:41.451 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:19:41.451 * Non blocking connect for SYNC fired the event.
18071:S 06 May 2024 19:19:41.451 * Master replied to PING, replication can continue...
18071:S 06 May 2024 19:19:41.451 * Trying a partial resynchronization (request 961e232fa61297979e494c98a031662af4e17d5c:1381).
18071:S 06 May 2024 19:19:41.452 * Full resync from master: 79b6738feadf1eb2596309e6b619ae2da5678456:1380
18071:S 06 May 2024 19:19:41.453 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18071:S 06 May 2024 19:19:41.453 * Discarding previously cached master state.
18071:S 06 May 2024 19:19:41.453 * MASTER <-> REPLICA sync: Flushing old data
18071:S 06 May 2024 19:19:41.454 * MASTER <-> REPLICA sync: Loading DB in memory
18071:S 06 May 2024 19:19:41.469 * Loading RDB produced by valkey version 255.255.255
18071:S 06 May 2024 19:19:41.469 * RDB age 0 seconds
18071:S 06 May 2024 19:19:41.469 * RDB memory usage when created 2.68 Mb
18071:S 06 May 2024 19:19:41.469 * Done loading RDB, keys loaded: 0, keys expired: 0.
18071:S 06 May 2024 19:19:41.469 * MASTER <-> REPLICA sync: Finished with success
18071:S 06 May 2024 19:19:41.575 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is no longer master of shard 5a313ed6e1afc160d88e2ec76f16f8322d6a6195; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:41.575 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
18071:S 06 May 2024 19:19:42.681 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is no longer master of shard c249ff0fc7be1576faf8724b7784974260ba49cf; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.682 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
18071:S 06 May 2024 19:19:42.690 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is no longer master of shard 2cb31efff9b31077914e5d8eee67fad987f900ec; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.690 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
18071:S 06 May 2024 19:19:42.690 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is no longer master of shard b24cc857a20e418c76000cf7bb7c2121d56535c8; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.690 * Node 80ff208fc4ecb27a6794991c3774578fada90ceb () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
18071:S 06 May 2024 19:19:42.692 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is no longer master of shard a85808b5f3b195646b5e9c75d437940e5910c539; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.692 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
18071:S 06 May 2024 19:19:42.702 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is no longer master of shard 5a93a431ee3c1974750eaeb45212552089e1d70f; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.705 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
18071:S 06 May 2024 19:19:42.705 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is no longer master of shard 1faef2c01b90ce80c9287617b3462fcd0eb7c5f5; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.705 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
18071:S 06 May 2024 19:19:42.705 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is no longer master of shard 63920ee806c2e289c54a7d03a407de389e09cdfa; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:42.711 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
18071:S 06 May 2024 19:19:42.712 * Cluster state changed: ok
18071:S 06 May 2024 19:19:44.292 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is no longer master of shard c707b138b75efd32e96dd0d45165ae682cc6e5bb; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:44.292 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
18071:S 06 May 2024 19:19:49.145 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ()
18071:S 06 May 2024 19:19:49.145 # Cluster state changed: fail
18071:S 06 May 2024 19:19:49.190 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about b6d1f331303ea88d453961a6bc247c12cf3f1746 ()
18071:S 06 May 2024 19:19:50.289 * Cluster state changed: ok
18071:S 06 May 2024 19:19:55.480 * Clear FAIL state for node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ():master without slots is reachable again.
18071:S 06 May 2024 19:19:55.480 * A failover occurred in shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6; node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 () lost 0 slot(s) to node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () with a config epoch of 21
18071:S 06 May 2024 19:19:55.580 * Clear FAIL state for node b6d1f331303ea88d453961a6bc247c12cf3f1746 ():replica is reachable again.
18071:M 06 May 2024 19:19:55.621 * Connection with master lost.
18071:M 06 May 2024 19:19:55.621 * Caching the disconnected master state.
18071:M 06 May 2024 19:19:55.621 * Discarding previously cached master state.
18071:M 06 May 2024 19:19:55.621 * Setting secondary replication ID to 79b6738feadf1eb2596309e6b619ae2da5678456, valid up to offset: 2312. New replication ID is f91e3c1fd947ee54538c2d1116825b3f19ce5055
18071:M 06 May 2024 19:19:55.623 * configEpoch set to 0 via CLUSTER RESET HARD
18071:M 06 May 2024 19:19:55.623 * Node hard reset, now I'm ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19
18071:M 06 May 2024 19:19:55.623 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18071:M 06 May 2024 19:19:55.623 # Cluster state changed: fail
18071:M 06 May 2024 19:19:55.626 * CONFIG REWRITE executed with success.
18071:S 06 May 2024 19:19:58.947 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18071:S 06 May 2024 19:19:58.947 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:19:58.947 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:19:58.948 * Non blocking connect for SYNC fired the event.
18071:S 06 May 2024 19:19:58.948 * Master replied to PING, replication can continue...
18071:S 06 May 2024 19:19:58.948 * Trying a partial resynchronization (request f91e3c1fd947ee54538c2d1116825b3f19ce5055:2312).
18071:S 06 May 2024 19:19:58.949 * Full resync from master: 79b6738feadf1eb2596309e6b619ae2da5678456:2311
18071:S 06 May 2024 19:19:58.951 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18071:S 06 May 2024 19:19:58.951 * Discarding previously cached master state.
18071:S 06 May 2024 19:19:58.951 * MASTER <-> REPLICA sync: Flushing old data
18071:S 06 May 2024 19:19:58.951 * MASTER <-> REPLICA sync: Loading DB in memory
18071:S 06 May 2024 19:19:58.953 * Loading RDB produced by valkey version 255.255.255
18071:S 06 May 2024 19:19:58.953 * RDB age 0 seconds
18071:S 06 May 2024 19:19:58.953 * RDB memory usage when created 2.73 Mb
18071:S 06 May 2024 19:19:58.953 * Done loading RDB, keys loaded: 0, keys expired: 0.
18071:S 06 May 2024 19:19:58.953 * MASTER <-> REPLICA sync: Finished with success
18071:S 06 May 2024 19:19:59.440 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is no longer master of shard 78683b2cc868ee4a3e989125c612bea0bc5c90c0; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:59.440 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is now part of shard b30cbe386d5d2fec228eb45db97ba04991f5020e
18071:S 06 May 2024 19:19:59.518 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is no longer master of shard 8980e70c0b02f85ca396ef3dc659e0dfe2f2edd4; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:59.518 * Node 163d3de19db2cb163010c38f385297acaacd0fe6 () is now part of shard 463631d5c660f73be6acc894f4e68e2bc6bf1f04
18071:S 06 May 2024 19:19:59.530 * Cluster state changed: ok
18071:S 06 May 2024 19:19:59.637 * Node caf83072d9c11f919729af96326335b051c7af49 () is no longer master of shard 6dd4af3291b64805920b88cdb4d7d83d44852f08; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:59.637 * Node caf83072d9c11f919729af96326335b051c7af49 () is now part of shard ada3b4025b51be85bbb1fa1df0e1b44871badaa9
18071:S 06 May 2024 19:19:59.918 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is no longer master of shard 5bedd120fdbe94c16f2c067685528596a15fe6e3; removed all 0 slot(s) it used to own
18071:S 06 May 2024 19:19:59.918 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is now part of shard e44c3a3bb3cc89ed450e11c453728db45301a89d
18071:S 06 May 2024 19:20:03.086 * Connection with master lost.
18071:S 06 May 2024 19:20:03.086 * Caching the disconnected master state.
18071:S 06 May 2024 19:20:03.086 * Reconnecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:20:03.086 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:20:03.086 # Error condition on socket for SYNC: Connection refused
18071:S 06 May 2024 19:20:04.045 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:20:04.046 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:20:04.046 # Error condition on socket for SYNC: Connection refused
18071:S 06 May 2024 19:20:05.052 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:20:05.052 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:20:05.053 # Error condition on socket for SYNC: Connection refused
18071:S 06 May 2024 19:20:06.061 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:20:06.061 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:20:06.061 # Error condition on socket for SYNC: Connection refused
18071:S 06 May 2024 19:20:07.040 * FAIL message received from b2794df9d2dedc8cc3d17b75bd5d3e5c8ca3f180 () about 87c9e3fe2e97200a971dc5aee1822009f9498072 ()
18071:S 06 May 2024 19:20:07.040 # Cluster state changed: fail
18071:S 06 May 2024 19:20:07.067 * Connecting to MASTER 127.0.0.1:30000
18071:S 06 May 2024 19:20:07.067 * MASTER <-> REPLICA sync started
18071:S 06 May 2024 19:20:07.067 * Start of election delayed for 761 milliseconds (rank #0, offset 3316).
18071:S 06 May 2024 19:20:07.067 # Error condition on socket for SYNC: Connection refused
18071:S 06 May 2024 19:20:07.873 * Starting a failover election for epoch 21.
18071:S 06 May 2024 19:20:07.882 * Failover election won: I'm the new master.
18071:S 06 May 2024 19:20:07.882 * configEpoch set to 21 after successful failover
18071:M 06 May 2024 19:20:07.882 * Discarding previously cached master state.
18071:M 06 May 2024 19:20:07.882 * Setting secondary replication ID to 79b6738feadf1eb2596309e6b619ae2da5678456, valid up to offset: 3317. New replication ID is 735202a2519ce90e70c5a01c76f7255a0144185e
18071:M 06 May 2024 19:20:07.882 * Cluster state changed: ok
18071:signal-handler (1715023208) Received SIGTERM scheduling shutdown...
18071:M 06 May 2024 19:20:08.074 * User requested shutdown...
18071:M 06 May 2024 19:20:08.074 # Valkey is now ready to exit, bye bye...
18263:C 06 May 2024 19:20:12.191 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
18263:C 06 May 2024 19:20:12.191 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
18263:C 06 May 2024 19:20:12.192 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=18263, just started
18263:C 06 May 2024 19:20:12.192 * Configuration loaded
18263:M 06 May 2024 19:20:12.193 * monotonic clock: POSIX clock_gettime
18263:M 06 May 2024 19:20:12.194 * Running mode=cluster, port=30005.
18263:M 06 May 2024 19:20:12.202 * Node configuration loaded, I'm ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19
18263:M 06 May 2024 19:20:12.203 * Server initialized
18263:M 06 May 2024 19:20:12.204 * Loading RDB produced by valkey version 255.255.255
18263:M 06 May 2024 19:20:12.204 * RDB age 14 seconds
18263:M 06 May 2024 19:20:12.204 * RDB memory usage when created 2.73 Mb
18263:M 06 May 2024 19:20:12.204 * Done loading RDB, keys loaded: 0, keys expired: 0.
18263:M 06 May 2024 19:20:12.204 * DB loaded from disk: 0.001 seconds
18263:M 06 May 2024 19:20:12.204 * Ready to accept connections tcp
18263:M 06 May 2024 19:20:12.213 * Clear FAIL state for node 87c9e3fe2e97200a971dc5aee1822009f9498072 ():master without slots is reachable again.
18263:M 06 May 2024 19:20:12.213 * A failover occurred in shard 5a15e11d32c6794f2f89faac54fc339efde5e362; node 87c9e3fe2e97200a971dc5aee1822009f9498072 () lost 0 slot(s) to node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () with a config epoch of 21
18263:M 06 May 2024 19:20:13.164 * Replica 127.0.0.1:30000 asks for synchronization
18263:M 06 May 2024 19:20:13.165 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'ea6871f4695f2adbea0015ce4dd56d8ed66c669f', my replication IDs are 'abc59aafd2034796a4762391f517a891a259367b' and '79b6738feadf1eb2596309e6b619ae2da5678456')
18263:M 06 May 2024 19:20:13.165 * Starting BGSAVE for SYNC with target: replicas sockets
18263:M 06 May 2024 19:20:13.166 * Background RDB transfer started by pid 18269
18269:C 06 May 2024 19:20:13.167 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
18263:M 06 May 2024 19:20:13.168 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
18263:M 06 May 2024 19:20:13.170 * Background RDB transfer terminated with success
18263:M 06 May 2024 19:20:13.170 * Streamed RDB transfer with replica 127.0.0.1:30000 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
18263:M 06 May 2024 19:20:13.170 * Synchronization with replica 127.0.0.1:30000 succeeded
18263:M 06 May 2024 19:20:14.218 * Cluster state changed: ok
18263:M 06 May 2024 19:20:19.206 * Connection with replica 127.0.0.1:30000 lost.
18263:M 06 May 2024 19:20:20.628 * configEpoch set to 0 via CLUSTER RESET HARD
18263:M 06 May 2024 19:20:20.628 * Node hard reset, now I'm 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed
18263:M 06 May 2024 19:20:20.628 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18263:M 06 May 2024 19:20:20.628 # Cluster state changed: fail
18263:M 06 May 2024 19:20:20.632 * CONFIG REWRITE executed with success.
18263:M 06 May 2024 19:20:23.887 # Missing implement of connection type tls
18263:S 06 May 2024 19:20:24.985 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18263:S 06 May 2024 19:20:24.985 * Connecting to MASTER 127.0.0.1:30000
18263:S 06 May 2024 19:20:24.985 * MASTER <-> REPLICA sync started
18263:S 06 May 2024 19:20:24.986 * Non blocking connect for SYNC fired the event.
18263:S 06 May 2024 19:20:24.987 * Master replied to PING, replication can continue...
18263:S 06 May 2024 19:20:24.987 * Trying a partial resynchronization (request abc59aafd2034796a4762391f517a891a259367b:2353).
18263:S 06 May 2024 19:20:24.987 * Full resync from master: 143323aa95c107e33c9dcc07aa7f0bdb0caaacbf:2311
18263:S 06 May 2024 19:20:24.989 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18263:S 06 May 2024 19:20:24.989 * Discarding previously cached master state.
18263:S 06 May 2024 19:20:24.989 * MASTER <-> REPLICA sync: Flushing old data
18263:S 06 May 2024 19:20:24.990 * MASTER <-> REPLICA sync: Loading DB in memory
18263:S 06 May 2024 19:20:24.991 * Loading RDB produced by valkey version 255.255.255
18263:S 06 May 2024 19:20:24.991 * RDB age 0 seconds
18263:S 06 May 2024 19:20:24.991 * RDB memory usage when created 2.61 Mb
18263:S 06 May 2024 19:20:24.991 * Done loading RDB, keys loaded: 0, keys expired: 0.
18263:S 06 May 2024 19:20:24.992 * MASTER <-> REPLICA sync: Finished with success
18263:S 06 May 2024 19:20:25.554 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is no longer master of shard ad7c5024c1a4d2a21dcf6f4a1d9b1f45ff1ead2d; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:25.554 * Node 568aac2645b6f747cc54704b8839b3eba6ea8563 () is now part of shard 67e314dae915c111e42745e09df1e650a83178cf
18263:S 06 May 2024 19:20:25.571 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is no longer master of shard 5736e9db4e80aceb59df82a4860201928e6e2dfe; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:25.571 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is now part of shard 19776fe3f0224c8eb88fdc51c9073d888852d019
18263:S 06 May 2024 19:20:25.600 * Cluster state changed: ok
18263:S 06 May 2024 19:20:25.719 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is no longer master of shard c5c35fe41b55a11da192d43a8da61be171aa83e6; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:25.719 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is now part of shard e04c9009be37f9eec889c59340dc040d0fa0aeb8
18263:S 06 May 2024 19:20:26.205 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is no longer master of shard d22c30d988eed1903566ab8b5147735674e58aca; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:26.205 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is now part of shard 3671e746eaf2973108e1ea8bdd09970af685d6ad
18263:M 06 May 2024 19:20:28.655 * Connection with master lost.
18263:M 06 May 2024 19:20:28.655 * Caching the disconnected master state.
18263:M 06 May 2024 19:20:28.655 * Discarding previously cached master state.
18263:M 06 May 2024 19:20:28.655 * Setting secondary replication ID to 143323aa95c107e33c9dcc07aa7f0bdb0caaacbf, valid up to offset: 2353. New replication ID is fdd81661eb90a9c8e86d04291905cd379521f671
18263:M 06 May 2024 19:20:28.656 * configEpoch set to 0 via CLUSTER RESET HARD
18263:M 06 May 2024 19:20:28.656 * Node hard reset, now I'm 50f8af8bddb3cb25a8322a76a04954c53eeb77b9
18263:M 06 May 2024 19:20:28.656 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18263:M 06 May 2024 19:20:28.662 # Cluster state changed: fail
18263:M 06 May 2024 19:20:28.666 * CONFIG REWRITE executed with success.
18263:S 06 May 2024 19:20:31.991 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18263:S 06 May 2024 19:20:31.991 * Connecting to MASTER 127.0.0.1:30000
18263:S 06 May 2024 19:20:31.991 * MASTER <-> REPLICA sync started
18263:S 06 May 2024 19:20:31.992 * Non blocking connect for SYNC fired the event.
18263:S 06 May 2024 19:20:31.992 * Master replied to PING, replication can continue...
18263:S 06 May 2024 19:20:31.992 * Trying a partial resynchronization (request fdd81661eb90a9c8e86d04291905cd379521f671:2353).
18263:S 06 May 2024 19:20:31.992 * Full resync from master: 143323aa95c107e33c9dcc07aa7f0bdb0caaacbf:2352
18263:S 06 May 2024 19:20:31.995 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18263:S 06 May 2024 19:20:31.995 * Discarding previously cached master state.
18263:S 06 May 2024 19:20:31.995 * MASTER <-> REPLICA sync: Flushing old data
18263:S 06 May 2024 19:20:31.995 * MASTER <-> REPLICA sync: Loading DB in memory
18263:S 06 May 2024 19:20:31.998 * Loading RDB produced by valkey version 255.255.255
18263:S 06 May 2024 19:20:31.998 * RDB age 0 seconds
18263:S 06 May 2024 19:20:31.998 * RDB memory usage when created 2.75 Mb
18263:S 06 May 2024 19:20:31.998 * Done loading RDB, keys loaded: 0, keys expired: 0.
18263:S 06 May 2024 19:20:31.998 * MASTER <-> REPLICA sync: Finished with success
18263:S 06 May 2024 19:20:32.503 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is no longer master of shard 7ac3ac70c33592cdd414687cbff2db517d34143f; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:32.503 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is now part of shard aa921447f701c6e1037283664b481acd5dcc18fa
18263:S 06 May 2024 19:20:33.116 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is no longer master of shard b2b496b97d756568bb9cc80df81baa9597a2a1e0; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:33.116 * Node 825390a470534406b83be059e6fecaf93dbe83a3 () is now part of shard 9628b03c98fa51a28e8367ab2c3e2abc75d63330
18263:S 06 May 2024 19:20:33.514 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is no longer master of shard c0ff426f83007655b38a9ba2767c988c6843ca09; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:33.514 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is now part of shard c149d2cb057be6b4c53e6cda4f8361117987c75b
18263:S 06 May 2024 19:20:33.516 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is no longer master of shard 5c7f555424b6dd7244521040fed352e8a690f783; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:33.517 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is now part of shard 82ad93ba4e7c80d6359231435757456cc87347ed
18263:S 06 May 2024 19:20:33.517 * Cluster state changed: ok
18263:S 06 May 2024 19:20:37.000 * Manual failover user request accepted.
18263:S 06 May 2024 19:20:37.000 * Received replication offset for paused master manual failover: 72398
18263:S 06 May 2024 19:20:37.000 * All master replication stream processed, manual failover can start.
18263:S 06 May 2024 19:20:37.000 * Start of election delayed for 0 milliseconds (rank #0, offset 72398).
18263:S 06 May 2024 19:20:37.000 * Starting a failover election for epoch 21.
18263:S 06 May 2024 19:20:37.006 * Currently unable to failover: Waiting for votes, but majority still not reached.
18263:S 06 May 2024 19:20:37.006 * Needed quorum: 3. Number of votes received so far: 1
18263:S 06 May 2024 19:20:37.006 * Failover election won: I'm the new master.
18263:S 06 May 2024 19:20:37.007 * configEpoch set to 21 after successful failover
18263:M 06 May 2024 19:20:37.007 * Connection with master lost.
18263:M 06 May 2024 19:20:37.007 * Caching the disconnected master state.
18263:M 06 May 2024 19:20:37.007 * Discarding previously cached master state.
18263:M 06 May 2024 19:20:37.007 * Setting secondary replication ID to 143323aa95c107e33c9dcc07aa7f0bdb0caaacbf, valid up to offset: 72399. New replication ID is 75d5db7019b3a404c97e423b3c184ae5fbd8182b
18263:M 06 May 2024 19:20:37.048 * Replica 127.0.0.1:30000 asks for synchronization
18263:M 06 May 2024 19:20:37.048 * Partial resynchronization request from 127.0.0.1:30000 accepted. Sending 0 bytes of backlog starting from offset 72399.
18263:M 06 May 2024 19:20:38.045 * A failover occurred in shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba; node 0e59ecb0630140741e54370064819bde1c9ca23d () lost 0 slot(s) to node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () with a config epoch of 21
18263:M 06 May 2024 19:20:40.446 * Connection with replica 127.0.0.1:30000 lost.
18263:M 06 May 2024 19:20:40.495 * configEpoch set to 0 via CLUSTER RESET HARD
18263:M 06 May 2024 19:20:40.495 * Node hard reset, now I'm e0fb6aab67e48702bb3257b35176c6bec33ac399
18263:M 06 May 2024 19:20:40.495 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18263:M 06 May 2024 19:20:40.495 # Cluster state changed: fail
18263:M 06 May 2024 19:20:40.500 * CONFIG REWRITE executed with success.
18263:S 06 May 2024 19:20:44.987 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18263:S 06 May 2024 19:20:44.987 * Connecting to MASTER 127.0.0.1:30000
18263:S 06 May 2024 19:20:44.987 * MASTER <-> REPLICA sync started
18263:S 06 May 2024 19:20:44.988 * Non blocking connect for SYNC fired the event.
18263:S 06 May 2024 19:20:44.988 * Master replied to PING, replication can continue...
18263:S 06 May 2024 19:20:44.988 * Trying a partial resynchronization (request 75d5db7019b3a404c97e423b3c184ae5fbd8182b:149475).
18263:S 06 May 2024 19:20:44.988 * Full resync from master: 15b0ad990a9f466395f7a3ab758b9dce8caee40c:149456
18263:S 06 May 2024 19:20:44.990 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18263:S 06 May 2024 19:20:44.990 * Discarding previously cached master state.
18263:S 06 May 2024 19:20:44.990 * MASTER <-> REPLICA sync: Flushing old data
18263:S 06 May 2024 19:20:44.991 * MASTER <-> REPLICA sync: Loading DB in memory
18263:S 06 May 2024 19:20:44.992 * Loading RDB produced by valkey version 255.255.255
18263:S 06 May 2024 19:20:44.992 * RDB age 0 seconds
18263:S 06 May 2024 19:20:44.992 * RDB memory usage when created 3.01 Mb
18263:S 06 May 2024 19:20:44.992 * Done loading RDB, keys loaded: 0, keys expired: 0.
18263:S 06 May 2024 19:20:44.992 * MASTER <-> REPLICA sync: Finished with success
18263:S 06 May 2024 19:20:45.511 * Node 968e407a2757f6786a27813986652641dfa8c53e () is no longer master of shard c23ee8d4ddce6cd592f3f5d11f12e6f0e7d8357b; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:45.516 * Node 968e407a2757f6786a27813986652641dfa8c53e () is now part of shard 3dd139d2458ce1ba40162b75a210452f2564f13f
18263:S 06 May 2024 19:20:45.516 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is no longer master of shard 065f0cbdfb7ce180a059fcb58c1396f6f36e7c7e; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:45.516 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is now part of shard d13e21ba301f8eb2ca5630faf519e74520eb0320
18263:S 06 May 2024 19:20:45.595 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is no longer master of shard ac0a659a65ff86d0acb03beca99488cf675db0b9; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:45.595 * Node 8162a5e33aefd5da3d8e53c51436fb8e5f04157d () is now part of shard 79761404f49765dfc1452b3572ad1557043c4b29
18263:S 06 May 2024 19:20:46.115 * Cluster state changed: ok
18263:S 06 May 2024 19:20:46.524 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is no longer master of shard f9fc60420f4df6321dff2cb65cfd5443410eaf16; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:20:46.524 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is now part of shard 2f2e8949560f25624e4d18237b260e2412773d08
18263:S 06 May 2024 19:20:48.455 * Manual failover user request accepted.
18263:S 06 May 2024 19:20:51.737 * FAIL message received from cf6fdb89f625f08d607b33125f0227fce622927f () about 51d1f09230533baaac63ae94738bf70b5db09342 ()
18263:S 06 May 2024 19:20:51.737 # Cluster state changed: fail
18263:S 06 May 2024 19:20:51.750 * Start of election delayed for 0 milliseconds (rank #0, offset 150465).
18263:S 06 May 2024 19:20:51.750 * Starting a failover election for epoch 21.
18263:S 06 May 2024 19:20:52.690 * Failover election won: I'm the new master.
18263:S 06 May 2024 19:20:52.690 * configEpoch set to 21 after successful failover
18263:M 06 May 2024 19:20:52.690 * Connection with master lost.
18263:M 06 May 2024 19:20:52.690 * Caching the disconnected master state.
18263:M 06 May 2024 19:20:52.690 * Discarding previously cached master state.
18263:M 06 May 2024 19:20:52.690 * Setting secondary replication ID to 15b0ad990a9f466395f7a3ab758b9dce8caee40c, valid up to offset: 150466. New replication ID is f8d077f2b079888df830d22ea8c1d0f0908d1598
18263:M 06 May 2024 19:20:52.691 * Cluster state changed: ok
18263:M 06 May 2024 19:20:58.468 * Clear FAIL state for node 51d1f09230533baaac63ae94738bf70b5db09342 ():master without slots is reachable again.
18263:M 06 May 2024 19:20:58.468 * A failover occurred in shard 942404fb5086368ee22ef226fc325d99eead816a; node 51d1f09230533baaac63ae94738bf70b5db09342 () lost 0 slot(s) to node e0fb6aab67e48702bb3257b35176c6bec33ac399 () with a config epoch of 21
18263:M 06 May 2024 19:20:58.475 * Replica 127.0.0.1:30000 asks for synchronization
18263:M 06 May 2024 19:20:58.475 * Partial resynchronization request from 127.0.0.1:30000 accepted. Sending 0 bytes of backlog starting from offset 150466.
18263:M 06 May 2024 19:20:58.483 * Connection with replica 127.0.0.1:30000 lost.
18263:M 06 May 2024 19:20:59.084 * configEpoch set to 0 via CLUSTER RESET HARD
18263:M 06 May 2024 19:20:59.084 * Node hard reset, now I'm 9b69ab04795d6bc643cc24ff909006021bb42465
18263:M 06 May 2024 19:20:59.084 * configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH
18263:M 06 May 2024 19:20:59.084 # Cluster state changed: fail
18263:M 06 May 2024 19:20:59.089 * CONFIG REWRITE executed with success.
18263:S 06 May 2024 19:21:02.878 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
18263:S 06 May 2024 19:21:02.879 * Connecting to MASTER 127.0.0.1:30000
18263:S 06 May 2024 19:21:02.879 * MASTER <-> REPLICA sync started
18263:S 06 May 2024 19:21:02.880 * Non blocking connect for SYNC fired the event.
18263:S 06 May 2024 19:21:02.880 * Master replied to PING, replication can continue...
18263:S 06 May 2024 19:21:02.881 * Trying a partial resynchronization (request f8d077f2b079888df830d22ea8c1d0f0908d1598:150507).
18263:S 06 May 2024 19:21:02.881 * Full resync from master: 26808aaf9be29dafcb94bfd1ac43b89f4e105947:150465
18263:S 06 May 2024 19:21:02.883 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
18263:S 06 May 2024 19:21:02.883 * Discarding previously cached master state.
18263:S 06 May 2024 19:21:02.884 * MASTER <-> REPLICA sync: Flushing old data
18263:S 06 May 2024 19:21:02.884 * MASTER <-> REPLICA sync: Loading DB in memory
18263:S 06 May 2024 19:21:02.886 * Loading RDB produced by valkey version 255.255.255
18263:S 06 May 2024 19:21:02.886 * RDB age 0 seconds
18263:S 06 May 2024 19:21:02.886 * RDB memory usage when created 3.01 Mb
18263:S 06 May 2024 19:21:02.886 * Done loading RDB, keys loaded: 0, keys expired: 0.
18263:S 06 May 2024 19:21:02.886 * MASTER <-> REPLICA sync: Finished with success
18263:S 06 May 2024 19:21:03.082 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is no longer master of shard 122cd1fcb92a7460821b86e1e2d20ae156aef07b; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:21:03.082 * Node c30b7ede25963c304ed3b564e73b8fe0144a0723 () is now part of shard 9c3ef340a0f3031400e88f148755f8e3a44c39cc
18263:S 06 May 2024 19:21:03.111 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is no longer master of shard 48c7ccc13115e6584257112f348f460ed6b3d90b; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:21:03.111 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is now part of shard 4897fecbf064c3a5fe304e8cc5bfc121e96f4d0d
18263:S 06 May 2024 19:21:03.385 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is no longer master of shard c9cdbc7bf1fd6b9537f6151b0e7952ae57507332; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:21:03.385 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is now part of shard 1e32518e495bab53ce49d6b0b26890950d0e6932
18263:S 06 May 2024 19:21:03.595 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is no longer master of shard c9e0b1451dee8dfc4e893df748587b969a26f679; removed all 0 slot(s) it used to own
18263:S 06 May 2024 19:21:03.595 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is now part of shard 75be0656223117757620482d8547e5fe2e896de1
18263:S 06 May 2024 19:21:04.320 * Cluster state changed: ok
18263:S 06 May 2024 19:21:06.734 * Forced failover user request accepted.
18263:S 06 May 2024 19:21:06.809 * Start of election delayed for 0 milliseconds (rank #0, offset 151757).
18263:S 06 May 2024 19:21:06.809 * Starting a failover election for epoch 21.
18263:S 06 May 2024 19:21:06.816 * Failover election won: I'm the new master.
18263:S 06 May 2024 19:21:06.816 * configEpoch set to 21 after successful failover
18263:M 06 May 2024 19:21:06.816 * Connection with master lost.
18263:M 06 May 2024 19:21:06.816 * Caching the disconnected master state.
18263:M 06 May 2024 19:21:06.817 * Discarding previously cached master state.
18263:M 06 May 2024 19:21:06.817 * Setting secondary replication ID to 26808aaf9be29dafcb94bfd1ac43b89f4e105947, valid up to offset: 151758. New replication ID is 6395bbd6f616c7aba33f39a2a31d75227c29ab6d
18263:M 06 May 2024 19:21:10.564 * FAIL message received from 4816b0fa4e909697f2f79ac7605b852ec0d22927 () about f1ab9aca89352244cecd21bb2bebff73910a1237 ()
18263:signal-handler (1715023274) Received SIGINT scheduling shutdown...
18263:M 06 May 2024 19:21:14.597 * Clear FAIL state for node f1ab9aca89352244cecd21bb2bebff73910a1237 ():master without slots is reachable again.
18263:M 06 May 2024 19:21:14.597 * A failover occurred in shard fdf0c30757c93c36c4d6e2340444130d7ab6fd1c; node f1ab9aca89352244cecd21bb2bebff73910a1237 () lost 0 slot(s) to node 9b69ab04795d6bc643cc24ff909006021bb42465 () with a config epoch of 21
18263:M 06 May 2024 19:21:14.602 * Replica 127.0.0.1:30000 asks for synchronization
18263:M 06 May 2024 19:21:14.602 * Partial resynchronization not accepted: Requested offset for second ID was 151795, but I can reply up to 151758
18263:M 06 May 2024 19:21:14.602 * Starting BGSAVE for SYNC with target: replicas sockets
18263:M 06 May 2024 19:21:14.603 * Background RDB transfer started by pid 18450
18450:C 06 May 2024 19:21:14.606 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
18263:M 06 May 2024 19:21:14.606 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
18263:M 06 May 2024 19:21:14.608 * Background RDB transfer terminated with success
18263:M 06 May 2024 19:21:14.608 * Streamed RDB transfer with replica 127.0.0.1:30000 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
18263:M 06 May 2024 19:21:14.608 * Synchronization with replica 127.0.0.1:30000 succeeded
18263:M 06 May 2024 19:21:14.657 * User requested shutdown...
18263:M 06 May 2024 19:21:14.657 * 1 of 1 replicas are in sync when shutting down.
18263:M 06 May 2024 19:21:14.657 # Valkey is now ready to exit, bye bye...
