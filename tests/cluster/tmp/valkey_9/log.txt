15723:C 06 May 2024 19:13:45.060 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
15723:C 06 May 2024 19:13:45.060 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
15723:C 06 May 2024 19:13:45.060 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
15723:C 06 May 2024 19:13:45.060 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=15723, just started
15723:C 06 May 2024 19:13:45.060 * Configuration loaded
15723:M 06 May 2024 19:13:45.061 * monotonic clock: POSIX clock_gettime
15723:M 06 May 2024 19:13:45.062 * Running mode=cluster, port=30009.
15723:M 06 May 2024 19:13:45.062 * No cluster configuration found, I'm 1dfdef8fb3821fc4e3a5ff8543b5cf4e6f2ce691
15723:M 06 May 2024 19:13:45.065 * Server initialized
15723:M 06 May 2024 19:13:45.066 * Creating AOF base file appendonly.aof.1.base.rdb on server start
15723:M 06 May 2024 19:13:45.068 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
15723:M 06 May 2024 19:13:45.068 * Ready to accept connections tcp
15723:M 06 May 2024 19:13:45.611 * configEpoch set to 0 via CLUSTER RESET HARD
15723:M 06 May 2024 19:13:45.611 * Node hard reset, now I'm 0804759eff336c643f5569ffe038ab2fa5349130
15723:M 06 May 2024 19:13:45.611 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
15723:M 06 May 2024 19:13:45.616 * CONFIG REWRITE executed with success.
15723:M 06 May 2024 19:13:45.714 * IP address for this node updated to 127.0.0.1
15723:M 06 May 2024 19:13:48.679 # Missing implement of connection type tls
15723:M 06 May 2024 19:13:52.521 * Cluster state changed: ok
15723:M 06 May 2024 19:13:52.839 * configEpoch set to 0 via CLUSTER RESET HARD
15723:M 06 May 2024 19:13:52.839 * Node hard reset, now I'm 2b30694b8e24a7702a1229fcee665e160d412a34
15723:M 06 May 2024 19:13:52.839 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
15723:M 06 May 2024 19:13:52.839 # Cluster state changed: fail
15723:M 06 May 2024 19:13:52.843 * CONFIG REWRITE executed with success.
15723:S 06 May 2024 19:13:56.938 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15723:S 06 May 2024 19:13:56.938 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:13:56.938 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:13:56.938 * Non blocking connect for SYNC fired the event.
15723:S 06 May 2024 19:13:56.939 * Master replied to PING, replication can continue...
15723:S 06 May 2024 19:13:56.939 * Trying a partial resynchronization (request 7092255ca8f3c5270e50e6233b06f782f03735d3:3).
15723:S 06 May 2024 19:13:56.939 * Full resync from master: b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0:27
15723:S 06 May 2024 19:13:56.941 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15723:S 06 May 2024 19:13:56.941 * Discarding previously cached master state.
15723:S 06 May 2024 19:13:56.941 * MASTER <-> REPLICA sync: Flushing old data
15723:S 06 May 2024 19:13:56.941 * MASTER <-> REPLICA sync: Loading DB in memory
15723:S 06 May 2024 19:13:56.943 * Loading RDB produced by valkey version 255.255.255
15723:S 06 May 2024 19:13:56.943 * RDB age 0 seconds
15723:S 06 May 2024 19:13:56.943 * RDB memory usage when created 2.66 Mb
15723:S 06 May 2024 19:13:56.943 * Done loading RDB, keys loaded: 0, keys expired: 0.
15723:S 06 May 2024 19:13:56.943 * MASTER <-> REPLICA sync: Finished with success
15723:S 06 May 2024 19:13:56.943 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15723:S 06 May 2024 19:13:56.944 * Background append only file rewriting started by pid 15864
15864:C 06 May 2024 19:13:56.945 * Successfully created the temporary AOF base file temp-rewriteaof-bg-15864.aof
15864:C 06 May 2024 19:13:56.945 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15723:S 06 May 2024 19:13:56.956 * Background AOF rewrite terminated with success
15723:S 06 May 2024 19:13:56.957 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-15864.aof into appendonly.aof.2.base.rdb
15723:S 06 May 2024 19:13:56.957 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.2.incr.aof
15723:S 06 May 2024 19:13:56.959 * Removing the history file appendonly.aof.1.incr.aof in the background
15723:S 06 May 2024 19:13:56.959 * Removing the history file appendonly.aof.1.base.rdb in the background
15723:S 06 May 2024 19:13:56.960 * Background AOF rewrite finished successfully
15723:S 06 May 2024 19:13:57.036 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is no longer master of shard 67426f1e38c225d7050cb5d4244d7014ede80e1a; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:13:57.036 * Node 1e5c8c4a07f43bf52fd4450600942b910335d228 () is now part of shard a2427455940b828a38a3ffb09c2b4cdfd4ccf6f5
15723:S 06 May 2024 19:13:57.506 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is no longer master of shard 2aaea61241128999e7bf4d990a5d6bae89096d2d; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:13:57.506 * Node 14870674fd1761a0b545ac9b6860bd07cc3c82a1 () is now part of shard 9009f53e5fcc48bb5a0b06d0bc06b2a661c7165d
15723:S 06 May 2024 19:13:57.565 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is no longer master of shard 93a2a8fbb3ebee1719ea78e416af4f62a280e72f; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:13:57.566 * Node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 () is now part of shard 0a3baa510e8cc3ec6c1dc6c6aff3c8c121a171ee
15723:S 06 May 2024 19:13:57.573 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is no longer master of shard 45a425a508f35dfb3598266dbd995af0daf15e78; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:13:57.573 * Node 9e2f09d33ea8e59e7726a43c9e211c32f945654f () is now part of shard e89af52affaeadfe3f4c492254ae30531e68445a
15723:S 06 May 2024 19:13:58.573 * Cluster state changed: ok
15723:S 06 May 2024 19:14:04.016 * FAIL message received from 9fcb85d514e112d10016edd497098e3fbe9e110e () about 1e5c8c4a07f43bf52fd4450600942b910335d228 ()
15723:S 06 May 2024 19:14:04.020 * FAIL message received from 85ed6fd7ba097e6b2c80626b17e130cb93740a3c () about 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 ()
15723:S 06 May 2024 19:14:04.551 * FAIL message received from 7ce15aada6af76e304df222f463282e865e22db0 () about 125263e49707331bd8e7a9d6681fe581aca463f2 ()
15723:S 06 May 2024 19:14:04.551 # Cluster state changed: fail
15723:S 06 May 2024 19:14:10.667 * Clear FAIL state for node 125263e49707331bd8e7a9d6681fe581aca463f2 (): is reachable again and nobody is serving its slots after some time.
15723:S 06 May 2024 19:14:10.667 * Cluster state changed: ok
15723:S 06 May 2024 19:14:12.656 * Clear FAIL state for node 44cbffb6ea4e9f1a9f3f6fea0c4ba7b57dcf0555 ():replica is reachable again.
15723:S 06 May 2024 19:14:12.763 * Clear FAIL state for node 1e5c8c4a07f43bf52fd4450600942b910335d228 ():replica is reachable again.
15723:M 06 May 2024 19:14:18.508 * Connection with master lost.
15723:M 06 May 2024 19:14:18.508 * Caching the disconnected master state.
15723:M 06 May 2024 19:14:18.508 * Discarding previously cached master state.
15723:M 06 May 2024 19:14:18.508 * Setting secondary replication ID to b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0, valid up to offset: 97. New replication ID is bc8d01d96f15a4faca7d3cdf6c98d523fadfacfa
15723:M 06 May 2024 19:14:18.509 * configEpoch set to 0 via CLUSTER RESET HARD
15723:M 06 May 2024 19:14:18.509 * Node hard reset, now I'm 24553aed57a0fc014e31d20913d6ddb67daf993b
15723:M 06 May 2024 19:14:18.509 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
15723:M 06 May 2024 19:14:18.509 # Cluster state changed: fail
15723:M 06 May 2024 19:14:18.515 * CONFIG REWRITE executed with success.
15723:S 06 May 2024 19:14:21.917 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15723:S 06 May 2024 19:14:21.917 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:21.917 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:21.918 * Non blocking connect for SYNC fired the event.
15723:S 06 May 2024 19:14:21.918 * Master replied to PING, replication can continue...
15723:S 06 May 2024 19:14:21.918 * Trying a partial resynchronization (request bc8d01d96f15a4faca7d3cdf6c98d523fadfacfa:97).
15723:S 06 May 2024 19:14:21.918 * Full resync from master: b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0:96
15723:S 06 May 2024 19:14:21.920 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15723:S 06 May 2024 19:14:21.920 * Discarding previously cached master state.
15723:S 06 May 2024 19:14:21.920 * MASTER <-> REPLICA sync: Flushing old data
15723:S 06 May 2024 19:14:21.920 * MASTER <-> REPLICA sync: Loading DB in memory
15723:S 06 May 2024 19:14:21.921 * Loading RDB produced by valkey version 255.255.255
15723:S 06 May 2024 19:14:21.922 * RDB age 0 seconds
15723:S 06 May 2024 19:14:21.922 * RDB memory usage when created 2.70 Mb
15723:S 06 May 2024 19:14:21.922 * Done loading RDB, keys loaded: 0, keys expired: 0.
15723:S 06 May 2024 19:14:21.922 * MASTER <-> REPLICA sync: Finished with success
15723:S 06 May 2024 19:14:21.922 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15723:S 06 May 2024 19:14:21.922 * Background append only file rewriting started by pid 16008
16008:C 06 May 2024 19:14:21.924 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16008.aof
16008:C 06 May 2024 19:14:21.925 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15723:S 06 May 2024 19:14:21.951 * Background AOF rewrite terminated with success
15723:S 06 May 2024 19:14:21.951 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16008.aof into appendonly.aof.3.base.rdb
15723:S 06 May 2024 19:14:21.951 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.3.incr.aof
15723:S 06 May 2024 19:14:21.953 * Removing the history file appendonly.aof.2.incr.aof in the background
15723:S 06 May 2024 19:14:21.953 * Removing the history file appendonly.aof.2.base.rdb in the background
15723:S 06 May 2024 19:14:21.954 * Background AOF rewrite finished successfully
15723:S 06 May 2024 19:14:22.257 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is no longer master of shard a1ec0cec552145a6ed1b39a5162d032008e3e8fd; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:22.257 * Node ed209190989ecd1b2fc83edcb92a31a33e7f40c0 () is now part of shard 5ed49dbd94affeba17a99f22ab305f897e30146f
15723:S 06 May 2024 19:14:22.548 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is no longer master of shard 10c2f585fc5fd868287fafbf5212b6227ab7017d; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:22.549 * Node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () is now part of shard bccca0c1e0e1dccd3b58d5709156154f9f4c9957
15723:S 06 May 2024 19:14:23.135 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is no longer master of shard f4049c4ab89f7706b4043fb6168626bdb0c6d5fc; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:23.135 * Node 4565dcea920547db24497a9ea7da3f8ba2d82492 () is now part of shard 1109f6e41ce862f21e8f082deeae031962ef83ac
15723:S 06 May 2024 19:14:23.567 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is no longer master of shard f07b8fb6b6cbfb692ab1b87f30279a0b975efbbe; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:23.567 * Node c5edb8319b761b2e5b47473a36be688c7f503b2f () is now part of shard a3bfec7d6b49cc6990a2b3560914e0bb1a7bda0c
15723:S 06 May 2024 19:14:23.569 * Cluster state changed: ok
15723:S 06 May 2024 19:14:30.034 * FAIL message received from 220970bd9ddf17fd39dbd121039031b6350dc7bd () about 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ()
15723:S 06 May 2024 19:14:30.034 # Cluster state changed: fail
15723:S 06 May 2024 19:14:31.060 * Cluster state changed: ok
15723:S 06 May 2024 19:14:31.224 * Clear FAIL state for node 4ac21db3dc729d28334184ee1a8bd51324f5ca54 ():master without slots is reachable again.
15723:S 06 May 2024 19:14:31.224 * A failover occurred in shard bccca0c1e0e1dccd3b58d5709156154f9f4c9957; node 4ac21db3dc729d28334184ee1a8bd51324f5ca54 () lost 0 slot(s) to node a47fca07157b154ce1988d97b2afcdaa40ce66b2 () with a config epoch of 21
15723:M 06 May 2024 19:14:31.353 * Connection with master lost.
15723:M 06 May 2024 19:14:31.353 * Caching the disconnected master state.
15723:M 06 May 2024 19:14:31.353 * Discarding previously cached master state.
15723:M 06 May 2024 19:14:31.353 * Setting secondary replication ID to b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0, valid up to offset: 2674. New replication ID is 1c1832333f79e42e7f697b3769ae1b27e46a031e
15723:M 06 May 2024 19:14:31.354 * configEpoch set to 0 via CLUSTER RESET HARD
15723:M 06 May 2024 19:14:31.354 * Node hard reset, now I'm 671088db4acca8792f424ee7ea50b8153c7529a0
15723:M 06 May 2024 19:14:31.354 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
15723:M 06 May 2024 19:14:31.354 # Cluster state changed: fail
15723:M 06 May 2024 19:14:31.359 * CONFIG REWRITE executed with success.
15723:S 06 May 2024 19:14:34.859 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15723:S 06 May 2024 19:14:34.859 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:34.860 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:34.860 * Non blocking connect for SYNC fired the event.
15723:S 06 May 2024 19:14:34.860 * Master replied to PING, replication can continue...
15723:S 06 May 2024 19:14:34.860 * Trying a partial resynchronization (request 1c1832333f79e42e7f697b3769ae1b27e46a031e:2674).
15723:S 06 May 2024 19:14:34.860 * Full resync from master: b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0:2673
15723:S 06 May 2024 19:14:34.862 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15723:S 06 May 2024 19:14:34.862 * Discarding previously cached master state.
15723:S 06 May 2024 19:14:34.862 * MASTER <-> REPLICA sync: Flushing old data
15723:S 06 May 2024 19:14:34.862 * MASTER <-> REPLICA sync: Loading DB in memory
15723:S 06 May 2024 19:14:34.864 * Loading RDB produced by valkey version 255.255.255
15723:S 06 May 2024 19:14:34.864 * RDB age 0 seconds
15723:S 06 May 2024 19:14:34.864 * RDB memory usage when created 2.70 Mb
15723:S 06 May 2024 19:14:34.864 * Done loading RDB, keys loaded: 0, keys expired: 0.
15723:S 06 May 2024 19:14:34.864 * MASTER <-> REPLICA sync: Finished with success
15723:S 06 May 2024 19:14:34.865 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15723:S 06 May 2024 19:14:34.865 * Background append only file rewriting started by pid 16117
16117:C 06 May 2024 19:14:34.867 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16117.aof
16117:C 06 May 2024 19:14:34.867 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15723:S 06 May 2024 19:14:34.955 * Background AOF rewrite terminated with success
15723:S 06 May 2024 19:14:34.955 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16117.aof into appendonly.aof.4.base.rdb
15723:S 06 May 2024 19:14:34.955 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.4.incr.aof
15723:S 06 May 2024 19:14:34.957 * Removing the history file appendonly.aof.3.incr.aof in the background
15723:S 06 May 2024 19:14:34.957 * Removing the history file appendonly.aof.3.base.rdb in the background
15723:S 06 May 2024 19:14:34.958 * Background AOF rewrite finished successfully
15723:S 06 May 2024 19:14:35.529 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is no longer master of shard a577fdbb713c832cf9850e989f6b72796108c745; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:35.529 * Node d612f4e24864679d4cb0b4d862c48e2df4373065 () is now part of shard e2220f97589610879561e699a57b8481b7124689
15723:S 06 May 2024 19:14:35.555 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is no longer master of shard 98ec8d4aaf47b8dc3863b2d7c51af4e948cd12a6; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:35.555 * Node 29597550bd8f898394ae88db5a0ba45f1868fd3f () is now part of shard 04e24d93f8b27f67291965144dd39f0ab3b35d25
15723:S 06 May 2024 19:14:35.564 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is no longer master of shard f88e4fd9e9180c926982aaac2f65e8d0105c55fe; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:35.564 * Node 9dad64de16025f02b271510ee2d51a6ca079dc7b () is now part of shard 1c2b75be0f3bbf57778cad018055649104140ab5
15723:S 06 May 2024 19:14:36.581 * Node de3054474d708953456bc221c893ce4305ba5136 () is no longer master of shard 77b4d2bf3a14a66eb7635d415bbf20a86e150b57; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:14:36.581 * Node de3054474d708953456bc221c893ce4305ba5136 () is now part of shard 236cc077a81351c19b896662f24236e227403702
15723:S 06 May 2024 19:14:36.581 * Cluster state changed: ok
15723:S 06 May 2024 19:14:38.504 * Connection with master lost.
15723:S 06 May 2024 19:14:38.504 * Caching the disconnected master state.
15723:S 06 May 2024 19:14:38.504 * Reconnecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:38.504 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:38.505 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:39.503 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:39.503 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:39.503 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:40.514 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:40.514 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:40.514 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:41.524 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:41.524 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:41.524 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:42.534 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:42.534 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:42.534 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:42.796 * FAIL message received from ffdb16089f859482065bad0e5c9a094d823a1410 () about c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ()
15723:S 06 May 2024 19:14:42.796 # Cluster state changed: fail
15723:S 06 May 2024 19:14:42.837 * Start of election delayed for 916 milliseconds (rank #0, offset 4535).
15723:S 06 May 2024 19:14:43.545 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:14:43.545 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:14:43.545 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:14:43.848 * Starting a failover election for epoch 21.
15723:S 06 May 2024 19:14:43.857 * Failover election won: I'm the new master.
15723:S 06 May 2024 19:14:43.857 * configEpoch set to 21 after successful failover
15723:M 06 May 2024 19:14:43.857 * Discarding previously cached master state.
15723:M 06 May 2024 19:14:43.857 * Setting secondary replication ID to b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0, valid up to offset: 4536. New replication ID is c30668eb480917fef3a3a3fbbd24cfb36bbca0ac
15723:M 06 May 2024 19:14:43.857 * Cluster state changed: ok
15723:M 06 May 2024 19:14:44.056 * Clear FAIL state for node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff ():master without slots is reachable again.
15723:M 06 May 2024 19:14:44.056 * A failover occurred in shard 45bde1a3488556534c5594a0f6ace0dba22be32b; node c6b3e5df3a36095ff5d9df268d3186ade1ef31ff () lost 0 slot(s) to node 671088db4acca8792f424ee7ea50b8153c7529a0 () with a config epoch of 21
15723:M 06 May 2024 19:14:44.059 * Replica 127.0.0.1:30004 asks for synchronization
15723:M 06 May 2024 19:14:44.059 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '66e59e5a8446500d3965165ce0d05f4763496bd4', my replication IDs are 'c30668eb480917fef3a3a3fbbd24cfb36bbca0ac' and 'b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0')
15723:M 06 May 2024 19:14:44.059 * Starting BGSAVE for SYNC with target: replicas sockets
15723:M 06 May 2024 19:14:44.060 * Background RDB transfer started by pid 16162
16162:C 06 May 2024 19:14:44.062 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
15723:M 06 May 2024 19:14:44.063 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
15723:M 06 May 2024 19:14:44.065 * Background RDB transfer terminated with success
15723:M 06 May 2024 19:14:44.065 * Streamed RDB transfer with replica 127.0.0.1:30004 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
15723:M 06 May 2024 19:14:44.065 * Synchronization with replica 127.0.0.1:30004 succeeded
15723:M 06 May 2024 19:14:44.255 * Connection with replica 127.0.0.1:30004 lost.
15723:M 06 May 2024 19:14:44.323 * Replica 127.0.0.1:30004 asks for synchronization
15723:M 06 May 2024 19:14:44.323 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'b33a8ee97e5c4da016d9a9cbeeebba4d90d5a20d', my replication IDs are 'c30668eb480917fef3a3a3fbbd24cfb36bbca0ac' and 'b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0')
15723:M 06 May 2024 19:14:44.323 * Starting BGSAVE for SYNC with target: replicas sockets
15723:M 06 May 2024 19:14:44.324 * Background RDB transfer started by pid 16177
16177:C 06 May 2024 19:14:44.326 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
15723:M 06 May 2024 19:14:44.326 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
15723:M 06 May 2024 19:14:44.329 * Background RDB transfer terminated with success
15723:M 06 May 2024 19:14:44.329 * Streamed RDB transfer with replica 127.0.0.1:30004 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
15723:M 06 May 2024 19:14:44.329 * Synchronization with replica 127.0.0.1:30004 succeeded
15723:M 06 May 2024 19:14:49.730 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
15723:M 06 May 2024 19:14:49.730 # Cluster state changed: fail
15723:M 06 May 2024 19:14:50.825 * Failover auth granted to 9dad64de16025f02b271510ee2d51a6ca079dc7b () for epoch 22
15723:M 06 May 2024 19:14:50.839 * Cluster state changed: ok
15723:M 06 May 2024 19:14:51.016 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
15723:M 06 May 2024 19:14:52.517 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 22
15723:M 06 May 2024 19:14:55.630 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
15723:M 06 May 2024 19:14:55.630 # Cluster state changed: fail
15723:M 06 May 2024 19:14:56.724 * Failover auth granted to d612f4e24864679d4cb0b4d862c48e2df4373065 () for epoch 23
15723:M 06 May 2024 19:14:57.021 * Cluster state changed: ok
15723:M 06 May 2024 19:14:57.345 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
15723:M 06 May 2024 19:14:57.345 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 23
15723:M 06 May 2024 19:15:04.031 * FAIL message received from 84a39200cacde99de881c75065e278d774a677f8 () about d612f4e24864679d4cb0b4d862c48e2df4373065 ()
15723:M 06 May 2024 19:15:04.031 # Cluster state changed: fail
15723:M 06 May 2024 19:15:04.841 * Failover auth granted to c26ea12fd513a731ced8b60c63390b5dbe84235d () for epoch 24
15723:M 06 May 2024 19:15:04.883 * Cluster state changed: ok
15723:M 06 May 2024 19:15:05.049 * Clear FAIL state for node d612f4e24864679d4cb0b4d862c48e2df4373065 ():master without slots is reachable again.
15723:M 06 May 2024 19:15:05.049 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node d612f4e24864679d4cb0b4d862c48e2df4373065 () lost 0 slot(s) to node c26ea12fd513a731ced8b60c63390b5dbe84235d () with a config epoch of 24
15723:M 06 May 2024 19:15:10.095 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about c26ea12fd513a731ced8b60c63390b5dbe84235d ()
15723:M 06 May 2024 19:15:10.098 # Cluster state changed: fail
15723:M 06 May 2024 19:15:10.916 * Failover auth granted to d612f4e24864679d4cb0b4d862c48e2df4373065 () for epoch 25
15723:M 06 May 2024 19:15:10.964 * Cluster state changed: ok
15723:M 06 May 2024 19:15:11.115 * Clear FAIL state for node c26ea12fd513a731ced8b60c63390b5dbe84235d ():master without slots is reachable again.
15723:M 06 May 2024 19:15:11.115 * A failover occurred in shard e2220f97589610879561e699a57b8481b7124689; node c26ea12fd513a731ced8b60c63390b5dbe84235d () lost 0 slot(s) to node d612f4e24864679d4cb0b4d862c48e2df4373065 () with a config epoch of 25
15723:M 06 May 2024 19:15:15.638 * FAIL message received from b741288807d59266e5b075e999bfd7863cf32021 () about 9dad64de16025f02b271510ee2d51a6ca079dc7b ()
15723:M 06 May 2024 19:15:15.638 # Cluster state changed: fail
15723:M 06 May 2024 19:15:16.548 * Failover auth granted to d979b3705974c02442b170e8dacc0ba5e04d56bb () for epoch 26
15723:M 06 May 2024 19:15:16.591 * Cluster state changed: ok
15723:M 06 May 2024 19:15:16.848 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
15723:M 06 May 2024 19:15:16.849 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 26
15723:M 06 May 2024 19:15:21.824 * FAIL message received from 9dad64de16025f02b271510ee2d51a6ca079dc7b () about d979b3705974c02442b170e8dacc0ba5e04d56bb ()
15723:M 06 May 2024 19:15:21.824 # Cluster state changed: fail
15723:M 06 May 2024 19:15:22.736 * Failover auth granted to 9dad64de16025f02b271510ee2d51a6ca079dc7b () for epoch 27
15723:M 06 May 2024 19:15:22.783 * Cluster state changed: ok
15723:M 06 May 2024 19:15:22.997 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
15723:M 06 May 2024 19:15:22.997 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 27
15723:M 06 May 2024 19:15:27.755 * FAIL message received from 29597550bd8f898394ae88db5a0ba45f1868fd3f () about 9dad64de16025f02b271510ee2d51a6ca079dc7b ()
15723:M 06 May 2024 19:15:27.755 # Cluster state changed: fail
15723:M 06 May 2024 19:15:28.687 * Failover auth granted to d979b3705974c02442b170e8dacc0ba5e04d56bb () for epoch 28
15723:M 06 May 2024 19:15:28.729 * Cluster state changed: ok
15723:M 06 May 2024 19:15:28.944 * Clear FAIL state for node 9dad64de16025f02b271510ee2d51a6ca079dc7b ():master without slots is reachable again.
15723:M 06 May 2024 19:15:28.944 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node 9dad64de16025f02b271510ee2d51a6ca079dc7b () lost 0 slot(s) to node d979b3705974c02442b170e8dacc0ba5e04d56bb () with a config epoch of 28
15723:M 06 May 2024 19:15:33.573 * Marking node d979b3705974c02442b170e8dacc0ba5e04d56bb () as failing (quorum reached).
15723:M 06 May 2024 19:15:33.573 # Cluster state changed: fail
15723:M 06 May 2024 19:15:34.326 * Failover auth granted to 9dad64de16025f02b271510ee2d51a6ca079dc7b () for epoch 29
15723:M 06 May 2024 19:15:34.369 * Cluster state changed: ok
15723:M 06 May 2024 19:15:34.586 * Clear FAIL state for node d979b3705974c02442b170e8dacc0ba5e04d56bb ():master without slots is reachable again.
15723:M 06 May 2024 19:15:34.586 * A failover occurred in shard 1c2b75be0f3bbf57778cad018055649104140ab5; node d979b3705974c02442b170e8dacc0ba5e04d56bb () lost 0 slot(s) to node 9dad64de16025f02b271510ee2d51a6ca079dc7b () with a config epoch of 29
15723:M 06 May 2024 19:15:34.685 * Connection with replica 127.0.0.1:30004 lost.
15723:M 06 May 2024 19:15:34.796 * Replica 127.0.0.1:30004 asks for synchronization
15723:M 06 May 2024 19:15:34.796 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '62c88fd4d2be0d2445eccdb0ba5fdf7a18b96e10', my replication IDs are 'c30668eb480917fef3a3a3fbbd24cfb36bbca0ac' and 'b3b2987a84a8edc7fb1e9f2b6b422efd45694ee0')
15723:M 06 May 2024 19:15:34.796 * Starting BGSAVE for SYNC with target: replicas sockets
15723:M 06 May 2024 19:15:34.797 * Background RDB transfer started by pid 16843
16843:C 06 May 2024 19:15:34.805 * Fork CoW for RDB: current 1 MB, peak 1 MB, average 0 MB
15723:M 06 May 2024 19:15:34.805 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
15723:M 06 May 2024 19:15:34.808 * Background RDB transfer terminated with success
15723:M 06 May 2024 19:15:34.808 * Streamed RDB transfer with replica 127.0.0.1:30004 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
15723:M 06 May 2024 19:15:34.808 * Synchronization with replica 127.0.0.1:30004 succeeded
15723:M 06 May 2024 19:15:34.902 * Connection with replica 127.0.0.1:30004 lost.
15723:M 06 May 2024 19:15:34.960 * configEpoch set to 0 via CLUSTER RESET HARD
15723:M 06 May 2024 19:15:34.960 * Node hard reset, now I'm a0009d5f604d884e784a3056b063090422c8ce55
15723:M 06 May 2024 19:15:34.960 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
15723:M 06 May 2024 19:15:34.960 # Cluster state changed: fail
15723:M 06 May 2024 19:15:34.966 * CONFIG REWRITE executed with success.
15723:M 06 May 2024 19:15:40.387 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is no longer master of shard c39f6f20eed586245932eadb49d37ef3444eeba3; removed all 0 slot(s) it used to own
15723:M 06 May 2024 19:15:40.420 * Node f618c2d793fec3c1be72eac543ed42af49103087 () is now part of shard 3a32b81103a1e70ff832bacb10b5a5a912e9e2dc
15723:S 06 May 2024 19:15:41.016 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
15723:S 06 May 2024 19:15:41.016 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:15:41.016 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:15:41.016 * Cluster state changed: ok
15723:S 06 May 2024 19:15:41.017 * Non blocking connect for SYNC fired the event.
15723:S 06 May 2024 19:15:41.017 * Master replied to PING, replication can continue...
15723:S 06 May 2024 19:15:41.018 * Trying a partial resynchronization (request c30668eb480917fef3a3a3fbbd24cfb36bbca0ac:62019).
15723:S 06 May 2024 19:15:41.018 * Full resync from master: 305882e7deb3a07b2db5ba093d5268d9149ddb97:61977
15723:S 06 May 2024 19:15:41.020 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15723:S 06 May 2024 19:15:41.020 * Discarding previously cached master state.
15723:S 06 May 2024 19:15:41.020 * MASTER <-> REPLICA sync: Flushing old data
15723:S 06 May 2024 19:15:41.020 * MASTER <-> REPLICA sync: Loading DB in memory
15723:S 06 May 2024 19:15:41.033 * Loading RDB produced by valkey version 255.255.255
15723:S 06 May 2024 19:15:41.033 * RDB age 0 seconds
15723:S 06 May 2024 19:15:41.033 * RDB memory usage when created 2.61 Mb
15723:S 06 May 2024 19:15:41.033 * Done loading RDB, keys loaded: 0, keys expired: 0.
15723:S 06 May 2024 19:15:41.033 * MASTER <-> REPLICA sync: Finished with success
15723:S 06 May 2024 19:15:41.033 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15723:S 06 May 2024 19:15:41.034 * Background append only file rewriting started by pid 16875
16875:C 06 May 2024 19:15:41.052 * Successfully created the temporary AOF base file temp-rewriteaof-bg-16875.aof
16875:C 06 May 2024 19:15:41.052 * Fork CoW for AOF rewrite: current 0 MB, peak 0 MB, average 0 MB
15723:S 06 May 2024 19:15:41.080 * Background AOF rewrite terminated with success
15723:S 06 May 2024 19:15:41.080 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-16875.aof into appendonly.aof.5.base.rdb
15723:S 06 May 2024 19:15:41.080 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.5.incr.aof
15723:S 06 May 2024 19:15:41.270 * Removing the history file appendonly.aof.4.incr.aof in the background
15723:S 06 May 2024 19:15:41.286 * Removing the history file appendonly.aof.4.base.rdb in the background
15723:S 06 May 2024 19:15:41.478 * Background AOF rewrite finished successfully
15723:S 06 May 2024 19:15:41.902 * Node 6a35a86928be660bf08d53b5985287118d92066b () is no longer master of shard 17431b2ec492ed011920dbf31b9d73f688b42f6f; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:15:41.902 * Node 6a35a86928be660bf08d53b5985287118d92066b () is now part of shard 78600edd67e7ea47e3a029230067904131c90109
15723:S 06 May 2024 19:15:42.385 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is no longer master of shard f65a03c57d7accc16ebbe0644b4f594992bf8dce; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:15:42.385 * Node 5269a12afdf4274ccd7df7a3ac397f3fcc91a41b () is now part of shard 897f6391c215d2fd35c543f86aae669878fdc874
15723:S 06 May 2024 19:15:42.414 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is no longer master of shard 3af8d21a8c53a9b6624c360029651d1f88745f55; removed all 0 slot(s) it used to own
15723:S 06 May 2024 19:15:42.414 * Node fbfcf26d16f4bc8614e1fa7be3bc2e62ddd94665 () is now part of shard 6b4d9b2c1f3548ebff40277e67444291a3fa23f5
15723:S 06 May 2024 19:17:20.010 * Connection with master lost.
15723:S 06 May 2024 19:17:20.010 * Caching the disconnected master state.
15723:S 06 May 2024 19:17:20.010 * Reconnecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:17:20.010 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:17:20.010 # Error condition on socket for SYNC: Connection refused
15723:S 06 May 2024 19:17:20.299 * Connecting to MASTER 127.0.0.1:30004
15723:S 06 May 2024 19:17:20.300 * MASTER <-> REPLICA sync started
15723:S 06 May 2024 19:17:20.300 * Non blocking connect for SYNC fired the event.
15723:S 06 May 2024 19:17:20.300 * Master replied to PING, replication can continue...
15723:S 06 May 2024 19:17:20.300 * Trying a partial resynchronization (request 305882e7deb3a07b2db5ba093d5268d9149ddb97:1767938).
15723:S 06 May 2024 19:17:20.300 * Full resync from master: 822d0ea1a4e4f7c85e90476b999e0b0f116cbba1:0
15723:S 06 May 2024 19:17:20.305 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
15723:S 06 May 2024 19:17:20.330 * Discarding previously cached master state.
15723:S 06 May 2024 19:17:20.330 * MASTER <-> REPLICA sync: Flushing old data
15723:S 06 May 2024 19:17:20.333 * MASTER <-> REPLICA sync: Loading DB in memory
15723:S 06 May 2024 19:17:20.335 * Loading RDB produced by valkey version 255.255.255
15723:S 06 May 2024 19:17:20.335 * RDB age 0 seconds
15723:S 06 May 2024 19:17:20.335 * RDB memory usage when created 3.56 Mb
15723:S 06 May 2024 19:17:20.351 * Done loading RDB, keys loaded: 10519, keys expired: 0.
15723:S 06 May 2024 19:17:20.351 * MASTER <-> REPLICA sync: Finished with success
15723:S 06 May 2024 19:17:20.351 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
15723:S 06 May 2024 19:17:20.352 * Background append only file rewriting started by pid 17278
17278:C 06 May 2024 19:17:20.380 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17278.aof
17278:C 06 May 2024 19:17:20.381 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 0 MB
15723:S 06 May 2024 19:17:20.400 * Background AOF rewrite terminated with success
15723:S 06 May 2024 19:17:20.400 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17278.aof into appendonly.aof.6.base.rdb
15723:S 06 May 2024 19:17:20.400 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.6.incr.aof
15723:S 06 May 2024 19:17:20.402 * Removing the history file appendonly.aof.5.incr.aof in the background
15723:S 06 May 2024 19:17:20.402 * Removing the history file appendonly.aof.5.base.rdb in the background
15723:S 06 May 2024 19:17:20.403 * Background AOF rewrite finished successfully
15723:signal-handler (1715023041) Received SIGTERM scheduling shutdown...
15723:S 06 May 2024 19:17:21.117 * User requested shutdown...
15723:S 06 May 2024 19:17:21.118 # Valkey is now ready to exit, bye bye...
17331:C 06 May 2024 19:17:21.143 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
17331:C 06 May 2024 19:17:21.143 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
17331:C 06 May 2024 19:17:21.144 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=17331, just started
17331:C 06 May 2024 19:17:21.144 * Configuration loaded
17331:M 06 May 2024 19:17:21.144 * monotonic clock: POSIX clock_gettime
17331:M 06 May 2024 19:17:21.145 * Running mode=cluster, port=30009.
17331:M 06 May 2024 19:17:21.151 * Node configuration loaded, I'm a0009d5f604d884e784a3056b063090422c8ce55
17331:M 06 May 2024 19:17:21.152 * Server initialized
17331:M 06 May 2024 19:17:21.152 * Reading RDB base file on AOF loading...
17331:M 06 May 2024 19:17:21.152 * Loading RDB produced by valkey version 255.255.255
17331:M 06 May 2024 19:17:21.152 * RDB age 1 seconds
17331:M 06 May 2024 19:17:21.152 * RDB memory usage when created 4.02 Mb
17331:M 06 May 2024 19:17:21.152 * RDB is base AOF
17331:M 06 May 2024 19:17:21.169 * Done loading RDB, keys loaded: 10519, keys expired: 0.
17331:M 06 May 2024 19:17:21.169 * DB loaded from base file appendonly.aof.6.base.rdb: 0.017 seconds
17331:M 06 May 2024 19:17:21.169 * DB loaded from append only file: 0.018 seconds
17331:M 06 May 2024 19:17:21.169 * Opening AOF incr file appendonly.aof.6.incr.aof on server start
17331:M 06 May 2024 19:17:21.169 * Ready to accept connections tcp
17331:S 06 May 2024 19:17:21.172 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17331:S 06 May 2024 19:17:21.172 * Connecting to MASTER 127.0.0.1:30004
17331:S 06 May 2024 19:17:21.172 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:17:21.172 * Cluster state changed: ok
17331:S 06 May 2024 19:17:21.173 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:17:21.176 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:17:21.179 * Trying a partial resynchronization (request f515a20c7beed4a07e5cb0c819fb8223e30c9cdc:1).
17331:S 06 May 2024 19:17:21.179 * Full resync from master: 822d0ea1a4e4f7c85e90476b999e0b0f116cbba1:14
17331:S 06 May 2024 19:17:21.184 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17331:S 06 May 2024 19:17:21.220 * Discarding previously cached master state.
17331:S 06 May 2024 19:17:21.220 * MASTER <-> REPLICA sync: Flushing old data
17331:S 06 May 2024 19:17:21.222 * MASTER <-> REPLICA sync: Loading DB in memory
17331:S 06 May 2024 19:17:21.224 * Loading RDB produced by valkey version 255.255.255
17331:S 06 May 2024 19:17:21.224 * RDB age 0 seconds
17331:S 06 May 2024 19:17:21.224 * RDB memory usage when created 3.60 Mb
17331:S 06 May 2024 19:17:21.238 * Done loading RDB, keys loaded: 10519, keys expired: 0.
17331:S 06 May 2024 19:17:21.238 * MASTER <-> REPLICA sync: Finished with success
17331:S 06 May 2024 19:17:21.238 * Creating AOF incr file temp-appendonly.aof.incr on background rewrite
17331:S 06 May 2024 19:17:21.238 * Background append only file rewriting started by pid 17341
17341:C 06 May 2024 19:17:21.272 * Successfully created the temporary AOF base file temp-rewriteaof-bg-17341.aof
17341:C 06 May 2024 19:17:21.272 * Fork CoW for AOF rewrite: current 1 MB, peak 1 MB, average 1 MB
17331:S 06 May 2024 19:17:21.273 * Background AOF rewrite terminated with success
17331:S 06 May 2024 19:17:21.273 * Successfully renamed the temporary AOF base file temp-rewriteaof-bg-17341.aof into appendonly.aof.7.base.rdb
17331:S 06 May 2024 19:17:21.273 * Successfully renamed the temporary AOF incr file temp-appendonly.aof.incr into appendonly.aof.7.incr.aof
17331:S 06 May 2024 19:17:21.274 * Removing the history file appendonly.aof.6.incr.aof in the background
17331:S 06 May 2024 19:17:21.274 * Removing the history file appendonly.aof.6.base.rdb in the background
17331:S 06 May 2024 19:17:21.275 * Background AOF rewrite finished successfully
17331:M 06 May 2024 19:17:34.807 * Connection with master lost.
17331:M 06 May 2024 19:17:34.808 * Caching the disconnected master state.
17331:M 06 May 2024 19:17:34.808 * Discarding previously cached master state.
17331:M 06 May 2024 19:17:34.808 * Setting secondary replication ID to 822d0ea1a4e4f7c85e90476b999e0b0f116cbba1, valid up to offset: 70. New replication ID is 93f9634c2dd1d47cb487d49b985e4892932dec78
17331:M 06 May 2024 19:17:34.809 * configEpoch set to 0 via CLUSTER RESET HARD
17331:M 06 May 2024 19:17:34.809 * Node hard reset, now I'm 8d75f2d6f057fa546d38d9b15adacee5fdc6a60d
17331:M 06 May 2024 19:17:34.809 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17331:M 06 May 2024 19:17:34.809 # Cluster state changed: fail
17331:M 06 May 2024 19:17:34.814 * CONFIG REWRITE executed with success.
17331:M 06 May 2024 19:17:38.008 # Missing implement of connection type tls
17331:M 06 May 2024 19:17:42.581 * Cluster state changed: ok
17331:M 06 May 2024 19:17:42.882 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is no longer master of shard f7d317600bff619f321470e98d7e3c9c27a4af6a; removed all 0 slot(s) it used to own
17331:M 06 May 2024 19:17:42.882 * Node 078932a058b3293ad41a7bf812f0c25bcb0049d7 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17331:M 06 May 2024 19:17:43.846 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is no longer master of shard 5acc99edc411149fcdb27d6533feac7207649ef7; removed all 0 slot(s) it used to own
17331:M 06 May 2024 19:17:43.846 * Node e1039ccb24f58e1d87d2782b61596d09ff577abb () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17331:M 06 May 2024 19:17:43.910 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is no longer master of shard 5000b06b9aff419dd88e8f75370c3da32551a1cb; removed all 0 slot(s) it used to own
17331:M 06 May 2024 19:17:43.910 * Node 59cb584e870b5276318f48cc46a9edfb0371113f () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17331:S 06 May 2024 19:17:43.960 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17331:S 06 May 2024 19:17:43.960 * Connecting to MASTER 127.0.0.1:30004
17331:S 06 May 2024 19:17:43.960 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:17:43.961 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:17:43.962 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:17:43.963 * Trying a partial resynchronization (request 93f9634c2dd1d47cb487d49b985e4892932dec78:70).
17331:S 06 May 2024 19:17:43.964 * Full resync from master: 822d0ea1a4e4f7c85e90476b999e0b0f116cbba1:69
17331:S 06 May 2024 19:17:43.965 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17331:S 06 May 2024 19:17:43.965 * Discarding previously cached master state.
17331:S 06 May 2024 19:17:43.965 * MASTER <-> REPLICA sync: Flushing old data
17331:S 06 May 2024 19:17:43.965 * MASTER <-> REPLICA sync: Loading DB in memory
17331:S 06 May 2024 19:17:43.974 * Loading RDB produced by valkey version 255.255.255
17331:S 06 May 2024 19:17:43.974 * RDB age 0 seconds
17331:S 06 May 2024 19:17:43.974 * RDB memory usage when created 2.73 Mb
17331:S 06 May 2024 19:17:43.974 * Done loading RDB, keys loaded: 0, keys expired: 0.
17331:S 06 May 2024 19:17:43.974 * MASTER <-> REPLICA sync: Finished with success
17331:S 06 May 2024 19:17:44.156 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is no longer master of shard 1a16ede24c20c880468822e63f78780773a34e47; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:44.156 * Node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 () is now part of shard 71756dc6ec85961b10f92d7936ea08ecbd62d2cc
17331:S 06 May 2024 19:17:44.508 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is no longer master of shard 873ddfcf8e44ed993d28f2c2cc6dee5b6aa4f588; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:44.508 * Node e84e73c5fbab2e444548d2b4b8c70f6592b04067 () is now part of shard 1050559ab1955a6332e03bbdccee0d3b12d80d4e
17331:S 06 May 2024 19:17:44.657 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is no longer master of shard 6c2b56a67af1fb0be8ff8b1b4eefe129c23debb0; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:44.657 * Node 970366324b8c390c737a72bc0527db1e0be88d05 () is now part of shard 069b0453b631c5c0be393a42d2b474a6b5af3bc3
17331:S 06 May 2024 19:17:45.410 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is no longer master of shard 0855cdcd88c49b00e8d3cb7a4549759c3b4ffc32; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:45.410 * Node a64f8ad23dbc7e19bceb3d6de42d635121321462 () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17331:S 06 May 2024 19:17:45.461 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is no longer master of shard 33c4d632371303bcb565e5c7aba0936e2a76b03a; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:45.461 * Node 4f657f5e5c0b9e1d22cbfd768445a3a17a128dce () is now part of shard 65d3f576730fc68ea1d1bef428c17b8f492f5ab4
17331:S 06 May 2024 19:17:45.562 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is no longer master of shard 54bf45e726f98496ec4871cc44f69eed009b4194; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:17:45.562 * Node c82ade1d42699c8d38b49704a4d508edf28962ad () is now part of shard 50e3f96a8fee19ff4889bc9e3adeceeaaa1dbc24
17331:S 06 May 2024 19:17:49.045 * FAIL message received from 8638bb3919fb522af84a8fe3990ded8caa70968c () about 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ()
17331:S 06 May 2024 19:17:54.691 * Clear FAIL state for node 26801f367db02d74aeeeb4c5ec2877a5f31569d9 ():replica is reachable again.
17331:S 06 May 2024 19:17:58.850 * FAIL message received from c82ade1d42699c8d38b49704a4d508edf28962ad () about c6e768b2f8702ccd365a393b642c8ac83ff22379 ()
17331:S 06 May 2024 19:17:58.850 # Cluster state changed: fail
17331:S 06 May 2024 19:17:59.752 * Cluster state changed: ok
17331:M 06 May 2024 19:17:59.914 * Connection with master lost.
17331:M 06 May 2024 19:17:59.914 * Caching the disconnected master state.
17331:M 06 May 2024 19:17:59.914 * Discarding previously cached master state.
17331:M 06 May 2024 19:17:59.914 * Setting secondary replication ID to 822d0ea1a4e4f7c85e90476b999e0b0f116cbba1, valid up to offset: 1049. New replication ID is a768d8f3ef6ee2e35fc20f55ca111d136e39e698
17331:M 06 May 2024 19:17:59.916 * configEpoch set to 0 via CLUSTER RESET HARD
17331:M 06 May 2024 19:17:59.916 * Node hard reset, now I'm ee4e8801b63592ad76065cf689613b8590e8adbd
17331:M 06 May 2024 19:17:59.916 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17331:M 06 May 2024 19:17:59.916 # Cluster state changed: fail
17331:M 06 May 2024 19:17:59.920 * CONFIG REWRITE executed with success.
17331:S 06 May 2024 19:18:02.797 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17331:S 06 May 2024 19:18:02.797 * Connecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:02.798 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:02.798 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:18:02.798 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:18:02.798 * Trying a partial resynchronization (request a768d8f3ef6ee2e35fc20f55ca111d136e39e698:1049).
17331:S 06 May 2024 19:18:02.799 * Full resync from master: 8fa9911ae05d0a1fbbaa8d27c7cfba10046179f9:1150
17331:S 06 May 2024 19:18:02.800 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17331:S 06 May 2024 19:18:02.800 * Discarding previously cached master state.
17331:S 06 May 2024 19:18:02.800 * MASTER <-> REPLICA sync: Flushing old data
17331:S 06 May 2024 19:18:02.800 * MASTER <-> REPLICA sync: Loading DB in memory
17331:S 06 May 2024 19:18:02.802 * Loading RDB produced by valkey version 255.255.255
17331:S 06 May 2024 19:18:02.802 * RDB age 0 seconds
17331:S 06 May 2024 19:18:02.802 * RDB memory usage when created 2.69 Mb
17331:S 06 May 2024 19:18:02.802 * Done loading RDB, keys loaded: 0, keys expired: 0.
17331:S 06 May 2024 19:18:02.802 * MASTER <-> REPLICA sync: Finished with success
17331:S 06 May 2024 19:18:03.081 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is no longer master of shard d0d012e1f78210fb0613fb1538f24f7814fe81bd; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.081 * Node 9735b7886a7f7567bec72eec88999171c7b1481d () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17331:S 06 May 2024 19:18:03.541 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is no longer master of shard 4a54cdb367579758a4e3f891240c194e22265b3d; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.541 * Node 9e37c267ff4ecec857b256ac46b5e11b3201d2bf () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17331:S 06 May 2024 19:18:03.542 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is no longer master of shard ab9f767e76ee34d7511533a455ad90ed1b50d27b; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.542 * Node 204fe20f285eb3c7bcdd744c0500d6789f98b722 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17331:S 06 May 2024 19:18:03.548 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is no longer master of shard dae9d79e83baa0dbce3720b336c773fde1c02492; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.548 * Node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17331:S 06 May 2024 19:18:03.579 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is no longer master of shard cb9315006ffe129274468e5ed2feb38a7f33269f; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.579 * Node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17331:S 06 May 2024 19:18:03.581 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is no longer master of shard 75182d21b15b8439f1522e589f314577961349dd; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.581 * Node 26996c81b1b4b014258b798c0b9cab6593139f89 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17331:S 06 May 2024 19:18:03.583 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is no longer master of shard 66a60939691fd1e6ee1492ac1dbc0dece4a724e9; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.583 * Node f01b0161e695176f83ca792891b1e43f77a88883 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17331:S 06 May 2024 19:18:03.586 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is no longer master of shard 1f9cf276999006c5e2fcd79b4eb3d5840c05a58b; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.586 * Node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17331:S 06 May 2024 19:18:03.589 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is no longer master of shard 7d57fa4efb72d70389b55e5fb06b1ed20ee0b8c3; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.589 * Node 3a8410fdf29d80b74cbb245767c5e44636bc3277 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17331:S 06 May 2024 19:18:03.589 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is no longer master of shard 30c8fb7d5fcde244056aac469ab0a18d2dfa4e6a; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.589 * Node 05434d3c3db7978583050249cc56e331c3083c27 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17331:S 06 May 2024 19:18:03.589 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is no longer master of shard fe0f6b7f23d33e5bedb35ba3947a32d59a5d868f; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:03.589 * Node 918741e37ed669f9cb19eed7a95073e1eae2a8f1 () is now part of shard af57b2c095a58c35952f5be607254c877d1c65e6
17331:S 06 May 2024 19:18:03.760 * Cluster state changed: ok
17331:S 06 May 2024 19:18:04.502 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is no longer master of shard 1bf2cd731a3dbe35a6cec5639318bdb95d99061d; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:04.502 * Node 1e008697fe34fc59625174f184ad2adc37042797 () is now part of shard b1941e40ca7c82b122690b81cfbab0265cfd6255
17331:S 06 May 2024 19:18:04.551 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is no longer master of shard dfcb8ddc797b36f3027193f68a243825e66ecb76; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:04.551 * Node e53ff829cf90e7caea1e3e82bd279765e0d896ce () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17331:S 06 May 2024 19:18:04.592 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is no longer master of shard 71ddfeaa46e5002962c1f850ecca2bc4f0152083; removed all 0 slot(s) it used to own
17331:S 06 May 2024 19:18:04.592 * Node f0648fda209089feb04595850ff99f51f0fb2ba5 () is now part of shard e57e3844f79664a605c7f2e2b61ad0702feaf18d
17331:S 06 May 2024 19:18:07.066 * Connection with master lost.
17331:S 06 May 2024 19:18:07.066 * Caching the disconnected master state.
17331:S 06 May 2024 19:18:07.066 * Reconnecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:07.066 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:07.066 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:07.812 * Connecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:07.813 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:07.813 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:08.822 * Connecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:08.822 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:08.822 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:09.831 * Connecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:09.831 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:09.831 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:10.840 * Connecting to MASTER 127.0.0.1:30000
17331:S 06 May 2024 19:18:10.840 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:10.840 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:11.041 * FAIL message received from 5097e3267f5590392864f4a471c331b04aff1382 () about 450684f318abb9002cec036680f3cb52f5f919b0 ()
17331:S 06 May 2024 19:18:11.041 * Start of election delayed for 890 milliseconds (rank #0, offset 1150).
17331:S 06 May 2024 19:18:11.041 # Cluster state changed: fail
17331:S 06 May 2024 19:18:11.736 * Configuration change detected. Reconfiguring myself as a replica of 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ()
17331:S 06 May 2024 19:18:11.736 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:11.737 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:11.737 * Cluster state changed: ok
17331:S 06 May 2024 19:18:11.740 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:18:11.740 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:18:11.740 * Trying a partial resynchronization (request 8fa9911ae05d0a1fbbaa8d27c7cfba10046179f9:1151).
17331:S 06 May 2024 19:18:11.741 * Successful partial resynchronization with master.
17331:S 06 May 2024 19:18:11.741 * Master replication ID changed to f4193ba52f301b9158bc3de64e6b1033a780a819
17331:S 06 May 2024 19:18:11.741 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17331:S 06 May 2024 19:18:12.839 * Connection with master lost.
17331:S 06 May 2024 19:18:12.839 * Caching the disconnected master state.
17331:S 06 May 2024 19:18:12.839 * Reconnecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:12.839 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:12.839 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:12.855 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:12.855 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:12.855 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:13.862 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:13.863 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:13.863 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:14.872 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:14.872 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:14.873 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:15.884 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:15.884 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:15.884 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:16.542 * FAIL message received from 3a8410fdf29d80b74cbb245767c5e44636bc3277 () about 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ()
17331:S 06 May 2024 19:18:16.542 # Cluster state changed: fail
17331:S 06 May 2024 19:18:16.591 * Starting a failover election for epoch 22.
17331:S 06 May 2024 19:18:16.894 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:16.894 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:16.894 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:17.904 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:17.904 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:17.904 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:18.913 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:18.913 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:18.914 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:19.923 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:19.923 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:19.923 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:20.932 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:20.932 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:20.933 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:21.943 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:21.943 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:21.943 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:22.953 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:22.953 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:22.953 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:23.963 * Connecting to MASTER 127.0.0.1:30015
17331:S 06 May 2024 19:18:23.964 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:23.964 * Start of election delayed for 801 milliseconds (rank #0, offset 1187).
17331:S 06 May 2024 19:18:23.964 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:24.570 * Currently unable to failover: Waiting the delay before I can start a new failover.
17331:S 06 May 2024 19:18:24.723 * Configuration change detected. Reconfiguring myself as a replica of b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17331:S 06 May 2024 19:18:24.723 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:24.724 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:24.724 * Cluster state changed: ok
17331:S 06 May 2024 19:18:24.729 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:18:24.729 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:18:24.729 * Trying a partial resynchronization (request f4193ba52f301b9158bc3de64e6b1033a780a819:1188).
17331:S 06 May 2024 19:18:24.730 * Successful partial resynchronization with master.
17331:S 06 May 2024 19:18:24.730 * Master replication ID changed to 451ef21071b0f1b37b526687c34c3e3dfb766090
17331:S 06 May 2024 19:18:24.730 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17331:S 06 May 2024 19:18:25.788 * Connection with master lost.
17331:S 06 May 2024 19:18:25.788 * Caching the disconnected master state.
17331:S 06 May 2024 19:18:25.788 * Reconnecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:25.788 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:25.788 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:25.982 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:25.982 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:25.982 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:26.994 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:26.995 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:26.995 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:28.006 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:28.006 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:28.006 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:29.017 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:29.017 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:29.017 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:30.028 * Connecting to MASTER 127.0.0.1:30003
17331:S 06 May 2024 19:18:30.028 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:30.028 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:30.545 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about b4c96fe54bd211d5373161c89d6e0f09e6bae07a ()
17331:S 06 May 2024 19:18:30.545 # Cluster state changed: fail
17331:S 06 May 2024 19:18:30.604 * Configuration change detected. Reconfiguring myself as a replica of 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17331:S 06 May 2024 19:18:30.604 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:30.604 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:30.604 * Cluster state changed: ok
17331:S 06 May 2024 19:18:30.608 * Non blocking connect for SYNC fired the event.
17331:S 06 May 2024 19:18:30.608 * Master replied to PING, replication can continue...
17331:S 06 May 2024 19:18:30.608 * Trying a partial resynchronization (request 451ef21071b0f1b37b526687c34c3e3dfb766090:1225).
17331:S 06 May 2024 19:18:30.608 * Successful partial resynchronization with master.
17331:S 06 May 2024 19:18:30.608 * Master replication ID changed to 5b2f1e898e0091b20e1daf794a4836868aeed39e
17331:S 06 May 2024 19:18:30.608 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.
17331:S 06 May 2024 19:18:31.068 * Connection with master lost.
17331:S 06 May 2024 19:18:31.068 * Caching the disconnected master state.
17331:S 06 May 2024 19:18:31.068 * Reconnecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:31.068 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:31.068 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:32.049 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:32.049 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:32.050 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:33.060 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:33.060 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:33.061 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:34.071 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:34.071 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:34.071 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:35.083 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:35.083 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:35.083 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:35.526 * FAIL message received from d85a0c4eb3ba227c220265c8cb0c1be8004d558d () about 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ()
17331:S 06 May 2024 19:18:35.526 # Cluster state changed: fail
17331:S 06 May 2024 19:18:36.095 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:36.095 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:36.095 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:36.805 * Start of election delayed for 751 milliseconds (rank #0, offset 1261).
17331:S 06 May 2024 19:18:37.108 * Connecting to MASTER 127.0.0.1:30012
17331:S 06 May 2024 19:18:37.109 * MASTER <-> REPLICA sync started
17331:S 06 May 2024 19:18:37.109 # Error condition on socket for SYNC: Connection refused
17331:S 06 May 2024 19:18:37.615 * Starting a failover election for epoch 27.
17331:S 06 May 2024 19:18:37.625 * Failover election won: I'm the new master.
17331:S 06 May 2024 19:18:37.625 * configEpoch set to 27 after successful failover
17331:M 06 May 2024 19:18:37.625 * Discarding previously cached master state.
17331:M 06 May 2024 19:18:37.625 * Setting secondary replication ID to 5b2f1e898e0091b20e1daf794a4836868aeed39e, valid up to offset: 1262. New replication ID is bf02ab8253e6525e24e77c84857f67b67703f9ae
17331:M 06 May 2024 19:18:37.626 * Cluster state changed: ok
17331:M 06 May 2024 19:18:37.634 * Replica 127.0.0.1:30006 asks for synchronization
17331:M 06 May 2024 19:18:37.634 * Partial resynchronization request from 127.0.0.1:30006 accepted. Sending 0 bytes of backlog starting from offset 1262.
17331:signal-handler (1715023117) Received SIGTERM scheduling shutdown...
17331:M 06 May 2024 19:18:37.716 * User requested shutdown...
17331:M 06 May 2024 19:18:37.716 * Waiting for replicas before shutting down.
17331:M 06 May 2024 19:18:38.119 * 1 of 1 replicas are in sync when shutting down.
17331:M 06 May 2024 19:18:38.120 # Valkey is now ready to exit, bye bye...
17817:C 06 May 2024 19:18:42.345 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
17817:C 06 May 2024 19:18:42.345 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
17817:C 06 May 2024 19:18:42.345 * Valkey version=255.255.255, bits=64, commit=93f8a19b, modified=0, pid=17817, just started
17817:C 06 May 2024 19:18:42.345 * Configuration loaded
17817:M 06 May 2024 19:18:42.346 * monotonic clock: POSIX clock_gettime
17817:M 06 May 2024 19:18:42.346 * Running mode=cluster, port=30009.
17817:M 06 May 2024 19:18:42.357 * Node configuration loaded, I'm ee4e8801b63592ad76065cf689613b8590e8adbd
17817:M 06 May 2024 19:18:42.358 * Server initialized
17817:M 06 May 2024 19:18:42.358 * Loading RDB produced by valkey version 255.255.255
17817:M 06 May 2024 19:18:42.358 * RDB age 40 seconds
17817:M 06 May 2024 19:18:42.358 * RDB memory usage when created 2.69 Mb
17817:M 06 May 2024 19:18:42.358 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:M 06 May 2024 19:18:42.358 * DB loaded from disk: 0.000 seconds
17817:M 06 May 2024 19:18:42.358 * Ready to accept connections tcp
17817:M 06 May 2024 19:18:42.362 * Configuration change detected. Reconfiguring myself as a replica of 1e008697fe34fc59625174f184ad2adc37042797 ()
17817:S 06 May 2024 19:18:42.362 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:18:42.362 * Connecting to MASTER 127.0.0.1:30006
17817:S 06 May 2024 19:18:42.362 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:18:42.362 * Cluster state changed: ok
17817:S 06 May 2024 19:18:42.365 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:18:42.365 * Clear FAIL state for node 450684f318abb9002cec036680f3cb52f5f919b0 ():master without slots is reachable again.
17817:S 06 May 2024 19:18:42.365 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 450684f318abb9002cec036680f3cb52f5f919b0 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17817:S 06 May 2024 19:18:42.365 * Clear FAIL state for node b4c96fe54bd211d5373161c89d6e0f09e6bae07a ():master without slots is reachable again.
17817:S 06 May 2024 19:18:42.365 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node b4c96fe54bd211d5373161c89d6e0f09e6bae07a () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17817:S 06 May 2024 19:18:42.368 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:18:42.368 * Trying a partial resynchronization (request fa8cd5db3c2709e8a693584efa1d4e8ce236a8b3:1151).
17817:S 06 May 2024 19:18:42.369 * Full resync from master: cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e:1298
17817:S 06 May 2024 19:18:42.372 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:18:42.372 * Discarding previously cached master state.
17817:S 06 May 2024 19:18:42.372 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:18:42.373 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:18:42.374 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:18:42.374 * RDB age 0 seconds
17817:S 06 May 2024 19:18:42.374 * RDB memory usage when created 2.83 Mb
17817:S 06 May 2024 19:18:42.374 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:18:42.374 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:18:42.461 * Clear FAIL state for node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd ():master without slots is reachable again.
17817:S 06 May 2024 19:18:42.461 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9f07e3ca03ad9d75250fe84b63f941d8609c07fd () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17817:S 06 May 2024 19:18:42.565 * Clear FAIL state for node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 ():master without slots is reachable again.
17817:S 06 May 2024 19:18:42.565 * A failover occurred in shard b1941e40ca7c82b122690b81cfbab0265cfd6255; node 9e51f1caf8c08f552dc747bebbfbb287ce3d5b74 () lost 0 slot(s) to node 1e008697fe34fc59625174f184ad2adc37042797 () with a config epoch of 28
17817:M 06 May 2024 19:18:42.649 * Connection with master lost.
17817:M 06 May 2024 19:18:42.649 * Caching the disconnected master state.
17817:M 06 May 2024 19:18:42.649 * Discarding previously cached master state.
17817:M 06 May 2024 19:18:42.649 * Setting secondary replication ID to cb3a9996647dc0fdc72ce3f0f0ddcbdde140ec4e, valid up to offset: 1340. New replication ID is 1738464d990d5ee9511c08b23685891849d4f3fb
17817:M 06 May 2024 19:18:42.651 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:18:42.651 * Node hard reset, now I'm 0f2a180bc3dd1ca08cf7a4547df0ff1a25981a88
17817:M 06 May 2024 19:18:42.651 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:18:42.651 # Cluster state changed: fail
17817:M 06 May 2024 19:18:42.661 * CONFIG REWRITE executed with success.
17817:M 06 May 2024 19:18:44.730 # Missing implement of connection type tls
17817:S 06 May 2024 19:18:45.330 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:18:45.330 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:18:45.331 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:18:45.331 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:18:45.331 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:18:45.332 * Trying a partial resynchronization (request 1738464d990d5ee9511c08b23685891849d4f3fb:1340).
17817:S 06 May 2024 19:18:45.332 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:1637
17817:S 06 May 2024 19:18:45.333 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:18:45.333 * Discarding previously cached master state.
17817:S 06 May 2024 19:18:45.333 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:18:45.334 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:18:45.335 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:18:45.335 * RDB age 0 seconds
17817:S 06 May 2024 19:18:45.335 * RDB memory usage when created 2.80 Mb
17817:S 06 May 2024 19:18:45.335 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:18:45.335 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:18:45.600 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is no longer master of shard 8dd948e148227a2c888397f600e5d52a2509a683; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:18:45.601 * Node 73160c92fac075f0525b46dc984b9d0fc00a7319 () is now part of shard 769aeb08b175de4efdfee71d709927c7f9afc5bf
17817:S 06 May 2024 19:18:46.205 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is no longer master of shard 841113874b49ed5f9d1e126879f92ea4392dcdab; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:18:46.205 * Node bcfc65dd66e5ac03397a54470d6555c63eea6616 () is now part of shard 215dc02bf2afb70a46c0e0788801a0f308924862
17817:S 06 May 2024 19:18:46.246 * Cluster state changed: ok
17817:S 06 May 2024 19:18:46.509 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is no longer master of shard 7eb196d163c5bf7eb03371da41c37de6ab3a27c7; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:18:46.509 * Node 0e7185e5d8d3d3003e3f961ad0a21384374239b2 () is now part of shard d05b2d9c8fad619a6a58ab01b0e5ade057de5ca1
17817:S 06 May 2024 19:18:46.582 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is no longer master of shard 91245c2cd07d4494975195dd1dfe064ec2d4cbfe; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:18:46.582 * Node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 () is now part of shard 5c56bf8f5102b508a2405dedd16899f245264a91
17817:S 06 May 2024 19:18:52.514 * FAIL message received from 6e0ef4878fe1c8835271b94d91093f69119e2c5f () about 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ()
17817:S 06 May 2024 19:19:08.676 * Clear FAIL state for node 4ef320c993d1f723c6e799fc4d9d0d0db57e09d9 ():replica is reachable again.
17817:S 06 May 2024 19:19:12.529 * FAIL message received from e4844e444bd7db045069d93fe2f71d3259852adf () about 03a61a2c512c0aab55d8de4c03711ba741fddb86 ()
17817:S 06 May 2024 19:19:12.529 # Cluster state changed: fail
17817:S 06 May 2024 19:19:18.872 * Clear FAIL state for node 03a61a2c512c0aab55d8de4c03711ba741fddb86 (): is reachable again and nobody is serving its slots after some time.
17817:S 06 May 2024 19:19:18.872 * Cluster state changed: ok
17817:M 06 May 2024 19:19:18.995 * Connection with master lost.
17817:M 06 May 2024 19:19:18.995 * Caching the disconnected master state.
17817:M 06 May 2024 19:19:18.995 * Discarding previously cached master state.
17817:M 06 May 2024 19:19:18.995 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 1721. New replication ID is 82f93098e288c867ccd2850649c6f4bc872b6082
17817:M 06 May 2024 19:19:18.996 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:19:18.996 * Node hard reset, now I'm 91aab1feff1314a17b72183eb4e8308f320601b3
17817:M 06 May 2024 19:19:18.996 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:19:18.996 # Cluster state changed: fail
17817:M 06 May 2024 19:19:19.000 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:19:21.934 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:19:21.934 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:19:21.934 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:19:21.935 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:19:21.935 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:19:21.935 * Trying a partial resynchronization (request 82f93098e288c867ccd2850649c6f4bc872b6082:1721).
17817:S 06 May 2024 19:19:21.936 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:1720
17817:S 06 May 2024 19:19:21.938 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:19:21.938 * Discarding previously cached master state.
17817:S 06 May 2024 19:19:21.938 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:19:21.938 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:19:21.940 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:19:21.940 * RDB age 0 seconds
17817:S 06 May 2024 19:19:21.940 * RDB memory usage when created 2.80 Mb
17817:S 06 May 2024 19:19:21.940 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:19:21.940 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:19:21.993 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is no longer master of shard 5d8efc15227cc965bd521169b3cd81eb9295ec19; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:21.993 * Node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17817:S 06 May 2024 19:19:22.234 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is no longer master of shard 01f47c3a12aa4f053a807eeb5eb545c5e07db794; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:22.234 * Node dfea84e0be1bcd49482af5c67f555de10fdf29b3 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17817:S 06 May 2024 19:19:22.598 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is no longer master of shard 7c13d5b404a532519713ecb0009d48d7b7148c0e; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:22.598 * Node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 () is now part of shard 9f9dea19e17027f3d40367b3d13d624654fd82d8
17817:S 06 May 2024 19:19:22.599 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is no longer master of shard b3f6dd9ab5620f4d8bda693091f3d8d36ad0e7d3; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:22.599 * Node 6691bd2bc72c367620d9f464e8e3094b802ad6fd () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17817:S 06 May 2024 19:19:23.039 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is no longer master of shard ab8e805388a16f08bc5fa7bfd304cf9b2f43c122; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:23.039 * Node d61b725f6dd02d855fdc907190bd06a397b457f8 () is now part of shard 64f4303b3e226ea46acf2328b8cbffb940c502e2
17817:S 06 May 2024 19:19:23.043 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is no longer master of shard 3e228277d31da320b8d2c0ab0d21aa067371dad3; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:23.044 * Node 169ce63f84ab73647e330599b7668e357009f9c7 () is now part of shard f4d0734f796edaeddf2c5d09cbe7bf8e0ddb8692
17817:S 06 May 2024 19:19:23.045 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is no longer master of shard aae63656731167d9c699175203047b6e26efb4be; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:23.045 * Node f483107001db7f57a8bd7a64ac4a49a80b9c200d () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17817:S 06 May 2024 19:19:23.051 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is no longer master of shard 1c166555732b739d6e4c1ab4dd8d165e832b3954; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:23.051 * Node c05d247c09a02616a6098580fdea8f4ce435b8bc () is now part of shard da4f83fcced31239e8986679bcae57566a875c12
17817:S 06 May 2024 19:19:23.544 * Cluster state changed: ok
17817:S 06 May 2024 19:19:23.547 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is no longer master of shard 614a1b42f4baa25c7f6d5a11b30ac1fdea247594; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:23.547 * Node a62662d22e020ab95e9e5259a64ee8b40ba0a365 () is now part of shard ad3b8637be272f5e4b5bea22fac4efd021dc27f8
17817:S 06 May 2024 19:19:29.019 * FAIL message received from 3d1a6ede1fb4785f3437ca2ee7d37934e6eb5120 () about dfea84e0be1bcd49482af5c67f555de10fdf29b3 ()
17817:S 06 May 2024 19:19:29.350 * FAIL message received from 5a72c35e8f819673e4c536c527b3bc7968d5372b () about 169ce63f84ab73647e330599b7668e357009f9c7 ()
17817:S 06 May 2024 19:19:29.538 * FAIL message received from 067f10da6671d30499813707fa18067ea3206b17 () about 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 ()
17817:S 06 May 2024 19:19:29.610 * FAIL message received from 43d342695b2be171fe42d4b24f7f98ee415c3435 () about 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 ()
17817:S 06 May 2024 19:19:35.136 * Migrating to orphaned master 3d1a6ede1fb4785f3437ca2ee7d37934e6eb5120
17817:M 06 May 2024 19:19:35.136 * Connection with master lost.
17817:M 06 May 2024 19:19:35.136 * Caching the disconnected master state.
17817:S 06 May 2024 19:19:35.136 * Connecting to MASTER 127.0.0.1:30001
17817:S 06 May 2024 19:19:35.136 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:19:35.138 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:19:35.138 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:19:35.138 * Trying a partial resynchronization (request d073cf9b323d6bce9c3e21330836b7574b5e4ed2:1749).
17817:S 06 May 2024 19:19:35.140 * Full resync from master: 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4:1748
17817:S 06 May 2024 19:19:35.140 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:19:35.140 * Discarding previously cached master state.
17817:S 06 May 2024 19:19:35.140 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:19:35.140 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:19:35.162 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:19:35.162 * RDB age 0 seconds
17817:S 06 May 2024 19:19:35.162 * RDB memory usage when created 2.75 Mb
17817:S 06 May 2024 19:19:35.162 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:19:35.163 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:19:35.237 * Clear FAIL state for node 169ce63f84ab73647e330599b7668e357009f9c7 ():replica is reachable again.
17817:S 06 May 2024 19:19:35.338 * Clear FAIL state for node 9e613cb2c4712a0062c1b26d3da1ea7442fbf737 ():replica is reachable again.
17817:S 06 May 2024 19:19:35.440 * Clear FAIL state for node dfea84e0be1bcd49482af5c67f555de10fdf29b3 ():replica is reachable again.
17817:S 06 May 2024 19:19:35.440 * Clear FAIL state for node 6a162c47ba07cc9853a11bf0ed7c45443e6e18d8 ():replica is reachable again.
17817:M 06 May 2024 19:19:35.575 * Connection with master lost.
17817:M 06 May 2024 19:19:35.576 * Caching the disconnected master state.
17817:M 06 May 2024 19:19:35.576 * Discarding previously cached master state.
17817:M 06 May 2024 19:19:35.576 * Setting secondary replication ID to 4c29d593488fe1b3f89d03c63c6585c7e5b12bd4, valid up to offset: 1790. New replication ID is 1cd243a214f563129b15b1b81ce5d74bb7ed6372
17817:M 06 May 2024 19:19:35.577 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:19:35.577 * Node hard reset, now I'm 80ff208fc4ecb27a6794991c3774578fada90ceb
17817:M 06 May 2024 19:19:35.577 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:19:35.577 # Cluster state changed: fail
17817:M 06 May 2024 19:19:35.582 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:19:41.483 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:19:41.484 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:19:41.484 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:19:41.484 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:19:41.484 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:19:41.484 * Trying a partial resynchronization (request 1cd243a214f563129b15b1b81ce5d74bb7ed6372:1790).
17817:S 06 May 2024 19:19:41.485 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:1789
17817:S 06 May 2024 19:19:41.486 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:19:41.486 * Discarding previously cached master state.
17817:S 06 May 2024 19:19:41.486 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:19:41.486 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:19:41.488 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:19:41.488 * RDB age 0 seconds
17817:S 06 May 2024 19:19:41.488 * RDB memory usage when created 2.80 Mb
17817:S 06 May 2024 19:19:41.488 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:19:41.488 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:19:42.641 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is no longer master of shard 1faef2c01b90ce80c9287617b3462fcd0eb7c5f5; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.669 * Node adccbec24d60f54090e36a0e6a0703bacb3c890e () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
17817:S 06 May 2024 19:19:42.669 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is no longer master of shard 5a313ed6e1afc160d88e2ec76f16f8322d6a6195; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.669 * Node fca9dbeac23069ec15538914520ce7ba33fb43c5 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
17817:S 06 May 2024 19:19:42.669 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is no longer master of shard 5a93a431ee3c1974750eaeb45212552089e1d70f; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.669 * Node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
17817:S 06 May 2024 19:19:42.669 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is no longer master of shard c249ff0fc7be1576faf8724b7784974260ba49cf; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.669 * Node 801403f53dd98a8a4d2f3d43e78a2214c99c863e () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
17817:S 06 May 2024 19:19:42.690 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is no longer master of shard be264f012de4ca3b640590c83301730d9edc2c7c; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.690 * Node 3c93b8fad484f01b1a2d5e7e5c55a29a75a8d9df () is now part of shard a77eea2c5e3a654920895d37c1cbd2dbc750a2fe
17817:S 06 May 2024 19:19:42.690 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is no longer master of shard 2cb31efff9b31077914e5d8eee67fad987f900ec; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.690 * Node b6d1f331303ea88d453961a6bc247c12cf3f1746 () is now part of shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6
17817:S 06 May 2024 19:19:42.698 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is no longer master of shard a85808b5f3b195646b5e9c75d437940e5910c539; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.698 * Node 208cc47c85e2f37e4bf67550a7534f2dac3c588d () is now part of shard e7d3efbc6079822fdca6c15be9f28019ee3f5618
17817:S 06 May 2024 19:19:42.698 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is no longer master of shard 63920ee806c2e289c54a7d03a407de389e09cdfa; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:42.698 * Node 794c5f0e6a1ac7ff0525a9b6f4e492eac0fbed21 () is now part of shard a102d983c1fa32aabf51cc8c6703313fd3153eb5
17817:S 06 May 2024 19:19:42.706 * Cluster state changed: ok
17817:S 06 May 2024 19:19:44.279 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is no longer master of shard c707b138b75efd32e96dd0d45165ae682cc6e5bb; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:44.279 * Node 0b8db5c36a489b4933345bc1515de904fb6a786f () is now part of shard 5d62c1956a9584bbd08a7a4e543b407d4b8313e1
17817:S 06 May 2024 19:19:49.144 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ()
17817:S 06 May 2024 19:19:49.144 # Cluster state changed: fail
17817:S 06 May 2024 19:19:49.189 * FAIL message received from 9f1e4387e843af5d17bd76f660aaeba8f408efe3 () about b6d1f331303ea88d453961a6bc247c12cf3f1746 ()
17817:S 06 May 2024 19:19:50.286 * Cluster state changed: ok
17817:S 06 May 2024 19:19:55.465 * Clear FAIL state for node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 ():master without slots is reachable again.
17817:S 06 May 2024 19:19:55.465 * A failover occurred in shard a1b26e71b34081a514fd6e0b60f6df57b5624ab6; node 2abfa8eefbebcdd70c094869ef85501903c7d0c4 () lost 0 slot(s) to node 47c7f0d7e3094aae7d589ff660cee63f462cf92c () with a config epoch of 21
17817:S 06 May 2024 19:19:55.566 * Clear FAIL state for node b6d1f331303ea88d453961a6bc247c12cf3f1746 ():replica is reachable again.
17817:M 06 May 2024 19:19:55.649 * Connection with master lost.
17817:M 06 May 2024 19:19:55.649 * Caching the disconnected master state.
17817:M 06 May 2024 19:19:55.650 * Discarding previously cached master state.
17817:M 06 May 2024 19:19:55.650 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 3012. New replication ID is 29c35134f92b263c1627bef20d3aff4e05120863
17817:M 06 May 2024 19:19:55.651 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:19:55.652 * Node hard reset, now I'm 163d3de19db2cb163010c38f385297acaacd0fe6
17817:M 06 May 2024 19:19:55.652 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:19:55.652 # Cluster state changed: fail
17817:M 06 May 2024 19:19:55.656 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:19:58.972 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:19:58.972 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:19:58.972 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:19:58.973 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:19:58.973 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:19:58.973 * Trying a partial resynchronization (request 29c35134f92b263c1627bef20d3aff4e05120863:3012).
17817:S 06 May 2024 19:19:58.974 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:3011
17817:S 06 May 2024 19:19:58.975 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:19:58.975 * Discarding previously cached master state.
17817:S 06 May 2024 19:19:58.975 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:19:58.975 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:19:58.977 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:19:58.977 * RDB age 0 seconds
17817:S 06 May 2024 19:19:58.977 * RDB memory usage when created 2.83 Mb
17817:S 06 May 2024 19:19:58.977 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:19:58.977 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:19:59.512 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is no longer master of shard 5bedd120fdbe94c16f2c067685528596a15fe6e3; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:59.512 * Node 334beb692e0626a4ea1257acac41aaf9962330a7 () is now part of shard e44c3a3bb3cc89ed450e11c453728db45301a89d
17817:S 06 May 2024 19:19:59.515 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is no longer master of shard 5679c354a9e2cf8cb62b26598f6d9e6a0f79b90d; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:59.515 * Node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () is now part of shard 5a15e11d32c6794f2f89faac54fc339efde5e362
17817:S 06 May 2024 19:19:59.603 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is no longer master of shard 78683b2cc868ee4a3e989125c612bea0bc5c90c0; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:19:59.603 * Node a62a8167d5508534ea9bf51fc947454349f1d3bb () is now part of shard b30cbe386d5d2fec228eb45db97ba04991f5020e
17817:S 06 May 2024 19:20:00.513 * Cluster state changed: ok
17817:S 06 May 2024 19:20:00.516 * Node caf83072d9c11f919729af96326335b051c7af49 () is no longer master of shard 6dd4af3291b64805920b88cdb4d7d83d44852f08; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:00.516 * Node caf83072d9c11f919729af96326335b051c7af49 () is now part of shard ada3b4025b51be85bbb1fa1df0e1b44871badaa9
17817:S 06 May 2024 19:20:07.038 * FAIL message received from b2794df9d2dedc8cc3d17b75bd5d3e5c8ca3f180 () about 87c9e3fe2e97200a971dc5aee1822009f9498072 ()
17817:S 06 May 2024 19:20:07.038 # Cluster state changed: fail
17817:S 06 May 2024 19:20:07.919 * Cluster state changed: ok
17817:S 06 May 2024 19:20:12.095 * FAIL message received from a470ecfb84686f1c17778838dfcbeb2ec78c9395 () about ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 ()
17817:S 06 May 2024 19:20:12.095 # Cluster state changed: fail
17817:S 06 May 2024 19:20:12.193 * Clear FAIL state for node 87c9e3fe2e97200a971dc5aee1822009f9498072 ():master without slots is reachable again.
17817:S 06 May 2024 19:20:12.193 * A failover occurred in shard 5a15e11d32c6794f2f89faac54fc339efde5e362; node 87c9e3fe2e97200a971dc5aee1822009f9498072 () lost 0 slot(s) to node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 () with a config epoch of 21
17817:S 06 May 2024 19:20:18.138 * Clear FAIL state for node ecbcb5be44c9e81ac340cc03f4e84abaa8d98e19 (): is reachable again and nobody is serving its slots after some time.
17817:S 06 May 2024 19:20:18.138 * Cluster state changed: ok
17817:M 06 May 2024 19:20:21.035 * Connection with master lost.
17817:M 06 May 2024 19:20:21.035 * Caching the disconnected master state.
17817:M 06 May 2024 19:20:21.036 * Discarding previously cached master state.
17817:M 06 May 2024 19:20:21.036 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 5277. New replication ID is e524920d2ba3831aef56b97e6143c3291ddfe0e0
17817:M 06 May 2024 19:20:21.037 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:20:21.037 * Node hard reset, now I'm 568aac2645b6f747cc54704b8839b3eba6ea8563
17817:M 06 May 2024 19:20:21.037 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:20:21.037 # Cluster state changed: fail
17817:M 06 May 2024 19:20:21.607 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:20:25.017 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:20:25.017 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:20:25.017 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:20:25.018 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:20:25.018 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:20:25.018 * Trying a partial resynchronization (request e524920d2ba3831aef56b97e6143c3291ddfe0e0:5277).
17817:S 06 May 2024 19:20:25.018 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:5276
17817:S 06 May 2024 19:20:25.020 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:20:25.020 * Discarding previously cached master state.
17817:S 06 May 2024 19:20:25.020 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:20:25.020 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:20:25.029 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:20:25.029 * RDB age 0 seconds
17817:S 06 May 2024 19:20:25.029 * RDB memory usage when created 2.83 Mb
17817:S 06 May 2024 19:20:25.029 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:20:25.029 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:20:25.557 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is no longer master of shard e8e4bf95f861f348154c047f60ca28434502f73e; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:25.557 * Node 4c26a1fcc6d2fe9debc60d491053b0cf6618a4ed () is now part of shard a0b63e81b62eaf4c5711335c6f1846909bf7fd69
17817:S 06 May 2024 19:20:25.557 * Cluster state changed: ok
17817:S 06 May 2024 19:20:25.645 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is no longer master of shard 5736e9db4e80aceb59df82a4860201928e6e2dfe; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:25.645 * Node 4c6bf93f4560c44206e9ae153747809786d36116 () is now part of shard 19776fe3f0224c8eb88fdc51c9073d888852d019
17817:S 06 May 2024 19:20:26.021 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is no longer master of shard c5c35fe41b55a11da192d43a8da61be171aa83e6; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:26.021 * Node 7d23a031d90aa6bc6893630f40ee5cf640088fe7 () is now part of shard e04c9009be37f9eec889c59340dc040d0fa0aeb8
17817:S 06 May 2024 19:20:26.302 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is no longer master of shard d22c30d988eed1903566ab8b5147735674e58aca; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:26.302 * Node f2264c981ff99322862f3de220948842b1f6ba1f () is now part of shard 3671e746eaf2973108e1ea8bdd09970af685d6ad
17817:M 06 May 2024 19:20:28.687 * Connection with master lost.
17817:M 06 May 2024 19:20:28.687 * Caching the disconnected master state.
17817:M 06 May 2024 19:20:28.687 * Discarding previously cached master state.
17817:M 06 May 2024 19:20:28.687 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 5332. New replication ID is 7278361af4620f7e1a8f7532d79e8813fab678e9
17817:M 06 May 2024 19:20:28.688 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:20:28.688 * Node hard reset, now I'm 825390a470534406b83be059e6fecaf93dbe83a3
17817:M 06 May 2024 19:20:28.688 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:20:28.688 # Cluster state changed: fail
17817:M 06 May 2024 19:20:28.692 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:20:32.025 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:20:32.025 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:20:32.025 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:20:32.026 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:20:32.026 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:20:32.026 * Trying a partial resynchronization (request 7278361af4620f7e1a8f7532d79e8813fab678e9:5332).
17817:S 06 May 2024 19:20:32.026 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:5331
17817:S 06 May 2024 19:20:32.028 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:20:32.028 * Discarding previously cached master state.
17817:S 06 May 2024 19:20:32.028 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:20:32.028 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:20:32.029 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:20:32.029 * RDB age 0 seconds
17817:S 06 May 2024 19:20:32.029 * RDB memory usage when created 2.87 Mb
17817:S 06 May 2024 19:20:32.029 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:20:32.029 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:20:32.148 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is no longer master of shard 7ac3ac70c33592cdd414687cbff2db517d34143f; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:32.148 * Node 3500eeb0c27fbcd680bc59f0bd19aca81ac51372 () is now part of shard aa921447f701c6e1037283664b481acd5dcc18fa
17817:S 06 May 2024 19:20:32.169 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is no longer master of shard c0ff426f83007655b38a9ba2767c988c6843ca09; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:32.169 * Node 7af9ba57b6fbc0d8a7f97e8a5c86bb8b04b48199 () is now part of shard c149d2cb057be6b4c53e6cda4f8361117987c75b
17817:S 06 May 2024 19:20:32.605 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is no longer master of shard 5c7f555424b6dd7244521040fed352e8a690f783; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:32.605 * Node 8f5c6ef31ffc0d64c65cd285f1e1d3dfd9744bc8 () is now part of shard 82ad93ba4e7c80d6359231435757456cc87347ed
17817:S 06 May 2024 19:20:33.109 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is no longer master of shard 10725e1ebc3e8158a7f12329adb9ba56ee854bc1; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:33.109 * Node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () is now part of shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba
17817:S 06 May 2024 19:20:33.510 * Cluster state changed: ok
17817:S 06 May 2024 19:20:37.536 * A failover occurred in shard bbfd84d9a82d8e08bb771735fe383b7ddafd08ba; node 0e59ecb0630140741e54370064819bde1c9ca23d () lost 0 slot(s) to node 50f8af8bddb3cb25a8322a76a04954c53eeb77b9 () with a config epoch of 21
17817:M 06 May 2024 19:20:40.524 * Connection with master lost.
17817:M 06 May 2024 19:20:40.524 * Caching the disconnected master state.
17817:M 06 May 2024 19:20:40.524 * Discarding previously cached master state.
17817:M 06 May 2024 19:20:40.524 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 152045. New replication ID is facd6fa9be54af882d081e7709d12eaaae83bbd0
17817:M 06 May 2024 19:20:40.525 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:20:40.525 * Node hard reset, now I'm 8162a5e33aefd5da3d8e53c51436fb8e5f04157d
17817:M 06 May 2024 19:20:40.525 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:20:40.525 # Cluster state changed: fail
17817:M 06 May 2024 19:20:40.530 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:20:45.014 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:20:45.014 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:20:45.014 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:20:45.014 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:20:45.015 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:20:45.015 * Trying a partial resynchronization (request facd6fa9be54af882d081e7709d12eaaae83bbd0:152045).
17817:S 06 May 2024 19:20:45.015 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:152044
17817:S 06 May 2024 19:20:45.016 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:20:45.017 * Discarding previously cached master state.
17817:S 06 May 2024 19:20:45.017 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:20:45.017 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:20:45.018 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:20:45.018 * RDB age 0 seconds
17817:S 06 May 2024 19:20:45.018 * RDB memory usage when created 3.06 Mb
17817:S 06 May 2024 19:20:45.018 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:20:45.019 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:20:45.092 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is no longer master of shard f9fc60420f4df6321dff2cb65cfd5443410eaf16; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:45.092 * Node 03f7c6a10e67bff0c5fac874576c356de6d20fb3 () is now part of shard 2f2e8949560f25624e4d18237b260e2412773d08
17817:S 06 May 2024 19:20:45.597 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is no longer master of shard 5894da5030fb6ea0d4890f9fcaefbcb689467382; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:45.597 * Node e0fb6aab67e48702bb3257b35176c6bec33ac399 () is now part of shard 942404fb5086368ee22ef226fc325d99eead816a
17817:S 06 May 2024 19:20:45.598 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is no longer master of shard 065f0cbdfb7ce180a059fcb58c1396f6f36e7c7e; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:45.598 * Node 8053da283ed5221065f944f98f73e25c61a010dd () is now part of shard d13e21ba301f8eb2ca5630faf519e74520eb0320
17817:S 06 May 2024 19:20:45.605 * Node 968e407a2757f6786a27813986652641dfa8c53e () is no longer master of shard c23ee8d4ddce6cd592f3f5d11f12e6f0e7d8357b; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:20:45.605 * Node 968e407a2757f6786a27813986652641dfa8c53e () is now part of shard 3dd139d2458ce1ba40162b75a210452f2564f13f
17817:S 06 May 2024 19:20:46.522 * Cluster state changed: ok
17817:S 06 May 2024 19:20:51.737 * FAIL message received from cf6fdb89f625f08d607b33125f0227fce622927f () about 51d1f09230533baaac63ae94738bf70b5db09342 ()
17817:S 06 May 2024 19:20:51.737 # Cluster state changed: fail
17817:S 06 May 2024 19:20:52.697 * Cluster state changed: ok
17817:S 06 May 2024 19:20:58.465 * Clear FAIL state for node 51d1f09230533baaac63ae94738bf70b5db09342 ():master without slots is reachable again.
17817:S 06 May 2024 19:20:58.465 * A failover occurred in shard 942404fb5086368ee22ef226fc325d99eead816a; node 51d1f09230533baaac63ae94738bf70b5db09342 () lost 0 slot(s) to node e0fb6aab67e48702bb3257b35176c6bec33ac399 () with a config epoch of 21
17817:M 06 May 2024 19:20:59.114 * Connection with master lost.
17817:M 06 May 2024 19:20:59.114 * Caching the disconnected master state.
17817:M 06 May 2024 19:20:59.114 * Discarding previously cached master state.
17817:M 06 May 2024 19:20:59.114 * Setting secondary replication ID to d073cf9b323d6bce9c3e21330836b7574b5e4ed2, valid up to offset: 153267. New replication ID is e1cc66400c01ec9ef1e6b005da9877206c241dc0
17817:M 06 May 2024 19:20:59.116 * configEpoch set to 0 via CLUSTER RESET HARD
17817:M 06 May 2024 19:20:59.116 * Node hard reset, now I'm c30b7ede25963c304ed3b564e73b8fe0144a0723
17817:M 06 May 2024 19:20:59.116 * configEpoch set to 10 via CLUSTER SET-CONFIG-EPOCH
17817:M 06 May 2024 19:20:59.116 # Cluster state changed: fail
17817:M 06 May 2024 19:20:59.120 * CONFIG REWRITE executed with success.
17817:S 06 May 2024 19:21:02.936 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.
17817:S 06 May 2024 19:21:02.936 * Connecting to MASTER 127.0.0.1:30004
17817:S 06 May 2024 19:21:02.937 * MASTER <-> REPLICA sync started
17817:S 06 May 2024 19:21:02.937 * Non blocking connect for SYNC fired the event.
17817:S 06 May 2024 19:21:02.938 * Master replied to PING, replication can continue...
17817:S 06 May 2024 19:21:02.938 * Trying a partial resynchronization (request e1cc66400c01ec9ef1e6b005da9877206c241dc0:153267).
17817:S 06 May 2024 19:21:02.939 * Full resync from master: d073cf9b323d6bce9c3e21330836b7574b5e4ed2:153266
17817:S 06 May 2024 19:21:02.940 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to disk
17817:S 06 May 2024 19:21:02.940 * Discarding previously cached master state.
17817:S 06 May 2024 19:21:02.940 * MASTER <-> REPLICA sync: Flushing old data
17817:S 06 May 2024 19:21:02.940 * MASTER <-> REPLICA sync: Loading DB in memory
17817:S 06 May 2024 19:21:02.943 * Loading RDB produced by valkey version 255.255.255
17817:S 06 May 2024 19:21:02.943 * RDB age 0 seconds
17817:S 06 May 2024 19:21:02.943 * RDB memory usage when created 3.06 Mb
17817:S 06 May 2024 19:21:02.943 * Done loading RDB, keys loaded: 0, keys expired: 0.
17817:S 06 May 2024 19:21:02.943 * MASTER <-> REPLICA sync: Finished with success
17817:S 06 May 2024 19:21:03.082 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is no longer master of shard 54e8a9afa1bd7f67c860e6b09192d24f3b4b5af0; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:21:03.082 * Node 9b69ab04795d6bc643cc24ff909006021bb42465 () is now part of shard fdf0c30757c93c36c4d6e2340444130d7ab6fd1c
17817:S 06 May 2024 19:21:03.514 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is no longer master of shard 48c7ccc13115e6584257112f348f460ed6b3d90b; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:21:03.514 * Node 3213008a052e3d1f53d4bd09bc5dafcc1093df23 () is now part of shard 4897fecbf064c3a5fe304e8cc5bfc121e96f4d0d
17817:S 06 May 2024 19:21:03.522 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is no longer master of shard c9e0b1451dee8dfc4e893df748587b969a26f679; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:21:03.522 * Node 089e7fb433a0847f9e708db936533110ce0cbea5 () is now part of shard 75be0656223117757620482d8547e5fe2e896de1
17817:S 06 May 2024 19:21:03.547 * Cluster state changed: ok
17817:S 06 May 2024 19:21:04.029 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is no longer master of shard c9cdbc7bf1fd6b9537f6151b0e7952ae57507332; removed all 0 slot(s) it used to own
17817:S 06 May 2024 19:21:04.029 * Node ea188db865fa34fa05775d699ccded36d0a815f0 () is now part of shard 1e32518e495bab53ce49d6b0b26890950d0e6932
17817:S 06 May 2024 19:21:10.563 * FAIL message received from 4816b0fa4e909697f2f79ac7605b852ec0d22927 () about f1ab9aca89352244cecd21bb2bebff73910a1237 ()
17817:signal-handler (1715023274) Received SIGINT scheduling shutdown...
17817:S 06 May 2024 19:21:14.590 * User requested shutdown...
17817:S 06 May 2024 19:21:14.591 # Valkey is now ready to exit, bye bye...
